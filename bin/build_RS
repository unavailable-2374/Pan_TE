#!/usr/bin/env perl
use strict;
use warnings;
use Cwd qw(getcwd);
use FindBin;
use lib $FindBin::RealBin;
use File::Path qw(make_path);
use File::Spec;
use Getopt::Long;
use Pod::Usage;
use POSIX qw(:sys_wait_h ceil floor);
use File::Basename;
use Log::Log4perl qw(:easy);
use Parallel::ForkManager;

# Configure logging
Log::Log4perl->easy_init({
    level   => $INFO,
    layout  => '[%d] %p %m%n',
    file    => '>>' . getcwd() . '/build_rs.log'
});
my $logger = Log::Log4perl->get_logger();

our $VERSION = '1.0.0';

# Program Configuration
my %config = (
    tmp_dir => undef,
    genome_file => undef,
    threads => 4,
    sample_size => 'all',
    rmblast_dir => undef,
    work_dir => undef,
    force_overwrite => 0
);

# Parse and validate arguments
get_and_validate_args();

# Set up environment
$logger->info("Setting up environment...");
setup_environment();

# Main execution flow
run_pipeline();

exit(0);

sub get_and_validate_args {
    GetOptions(
        'help|h'        => \my $help,
        'version|v'     => \my $version,
        'tmp=s'         => \$config{tmp_dir},
        'genome=s'      => \$config{genome_file},
        'threads=i'     => \$config{threads},
        'sample=s'      => \$config{sample_size},
        'workdir=s'     => \$config{work_dir},
        'force|f'       => \$config{force_overwrite}
    ) or pod2usage(2);

    pod2usage(1) if $help;
    if ($version) {
        print "build_RS version $VERSION\n";
        exit(0);
    }

    # Try to locate rmblast directory through multiple methods
    $config{rmblast_dir} = $ENV{RMBLAST_DIR};
    unless ($config{rmblast_dir}) {
        eval {
            use RepModelConfig;
            $config{rmblast_dir} = $RepModelConfig::configuration->{'RMBLAST_DIR'}->{'value'};
        };
    }

    # If still not found, try locating rmblastn
    unless ($config{rmblast_dir} && -x "$config{rmblast_dir}/rmblastn") {
        $config{rmblast_dir} = locate_rmblastn();
    }

    my @required = qw(tmp_dir genome_file work_dir); 
    for my $req (@required) {
        unless (defined $config{$req}) {
            $logger->error("Missing required parameter: --$req");
            die "Error: --$req is required\n";
        }
    }

    unless ($config{rmblast_dir}) {
        $logger->error("RMBLAST_DIR environment variable or configuration not found");
        die "Error: RMBLAST_DIR must be set in environment or configuration\n";
    }

    # Log configuration
    $logger->info("Configuration:");
    $logger->info("  Genome file: $config{genome_file}");
    $logger->info("  Threads: $config{threads}");
    $logger->info("  Working directory: $config{work_dir}");
    $logger->info("  RMBlast directory: $config{rmblast_dir}");
    $logger->info("  Force overwrite: " . ($config{force_overwrite} ? "Yes" : "No"));
}

sub locate_rmblastn {
    $logger->info("Attempting to locate rmblastn...");
    
    # Try whereis command
    my $whereis_out = `whereis rmblastn`;
    if ($whereis_out =~ /rmblastn:\s+(\S+)/) {
        my $path = $1;
        if (-x $path) {
            $logger->info("Found rmblastn via whereis: $path");
            return dirname($path);
        }
    }
    
    # Try find command in common locations
    for my $base_dir ("/usr/local", "/usr", $ENV{HOME}, "/opt") {
        my $cmd = "find $base_dir -name rmblastn -type f 2>/dev/null";
        my $find_out = `$cmd`;
        if ($find_out) {
            chomp($find_out);
            my @paths = split(/\n/, $find_out);
            foreach my $path (@paths) {
                if (-x $path) {
                    $logger->info("Found rmblastn via find: $path");
                    return dirname($path);
                }
            }
        }
    }
    
    $logger->error("Could not locate rmblastn executable");
    die "Error: rmblastn not found in system\n";
}

sub setup_environment {
    # Create necessary directories
    for my $dir ($config{tmp_dir}, $config{work_dir}) {
        unless (-d $dir) {
            $logger->info("Creating directory: $dir");
            make_path($dir) or die "Failed to create directory $dir: $!\n";
        }
    }

    # Change to work directory
    chdir $config{work_dir} or die "Cannot change to work directory: $!\n";
    $logger->info("Changed to working directory: " . getcwd());
}

sub should_skip_existing_file {
    my ($filepath, $description) = @_;
    
    # Never skip if force overwrite is enabled
    return 0 if $config{force_overwrite};
    
    # Check if file exists and is valid
    if (-s $filepath) {
        my $header_check = `head -1 "$filepath" 2>/dev/null`;
        if ($header_check && $header_check =~ /^>/) {
            my $size = -s $filepath;
            $logger->info("Found existing $description: $filepath ($size bytes)");
            return 1;
        } else {
            $logger->warn("Existing $description appears invalid, will regenerate: $filepath");
            return 0;
        }
    }
    return 0;
}

sub check_existing_outputs {
    my ($final_consensus, $refiner_consensus, $refiner_masked_genome) = @_;
    
    # Skip check if force overwrite is enabled
    if ($config{force_overwrite}) {
        $logger->info("Force overwrite enabled, will regenerate all outputs");
        return 0;
    }
    
    # Check if all key output files exist and are valid
    my @required_files = (
        {path => $final_consensus, desc => "Final consensus sequences"},
        {path => $refiner_consensus, desc => "Refiner consensus sequences"},
        {path => $refiner_masked_genome, desc => "Refiner masked genome"}
    );
    
    my $all_exist = 1;
    my @missing_files;
    
    for my $file (@required_files) {
        if (-s $file->{path}) {
            # File exists and has content, check if it's a valid FASTA
            my $header_check = `head -1 "$file->{path}" 2>/dev/null`;
            if ($header_check && $header_check =~ /^>/) {
                my $size = -s $file->{path};
                $logger->info("Found existing $file->{desc}: $file->{path} (size: $size bytes)");
            } else {
                $logger->warn("$file->{desc} exists but appears invalid: $file->{path}");
                push @missing_files, $file->{desc};
                $all_exist = 0;
            }
        } else {
            $logger->info("Missing $file->{desc}: $file->{path}");
            push @missing_files, $file->{desc};
            $all_exist = 0;
        }
    }
    
    if ($all_exist) {
        $logger->info("All required output files are present and valid");
        return 1;
    } else {
        $logger->info("Missing or invalid files: " . join(", ", @missing_files));
        $logger->info("Will run pipeline to generate missing outputs");
        return 0;
    }
}

sub run_pipeline {
    $logger->info("Starting RepeatScout pipeline...");

    # Check for existing final outputs first
    my $final_consensus = "$config{work_dir}/consensi.fa";
    my $refiner_output_dir = "$config{work_dir}/refiner_output";
    my $refiner_consensus = "$refiner_output_dir/consensus_masking.fa";
    my $refiner_masked_genome = "$refiner_output_dir/genome_final_masked.fa";
    
    if (check_existing_outputs($final_consensus, $refiner_consensus, $refiner_masked_genome)) {
        $logger->info("All required output files already exist and are valid, pipeline completed");
        return;
    }

    # Get genome size and calculate parameters
    my $genome_size = -s $config{genome_file};
    my $readable_size = sprintf("%.2f", $genome_size/1e9) . " Gb";
    $logger->info("Processing genome of size: $readable_size");

    # Calculate sampling strategy based on genome size thresholds
    my ($sample_strategy, $sample_size, $chunk_size, $num_chunks) = 
        determine_sampling_strategy($genome_size);
    
    $logger->info("Sampling strategy: $sample_strategy");
    if ($sample_strategy eq "sample_parallel") {
        $logger->info("Sample size per chunk: " . sprintf("%.2f", $sample_size/1e6) . " Mb");
        $logger->info("Total sampling: " . sprintf("%.2f", ($sample_size * $num_chunks)/1e6) . " Mb");
    } else {
        $logger->info("Sample size: " . sprintf("%.2f", $sample_size/1e6) . " Mb");
    }
    $logger->info("Chunk size: " . sprintf("%.2f", $chunk_size/1e3) . " Kb");
    $logger->info("Number of parallel chunks: $num_chunks");

    # Calculate lmer size based on actual data being processed, not full genome
    my $effective_size = determine_effective_processing_size($sample_strategy, $sample_size, $num_chunks, $genome_size);
    my $lmer_size = calculate_lmer_size($effective_size);
    $logger->info("Calculated l-mer size: $lmer_size (based on effective processing size: " . 
                  sprintf("%.2f", $effective_size/1e6) . " Mb)");

    # Check for RepeatScout results
    my $rs_consensus = "$config{tmp_dir}/repeats.fa";
    my $filtered_consensus = "$config{tmp_dir}/repeats.filtered.fa";

    if (-s $filtered_consensus) {
        $logger->info("Found existing filtered consensus sequences, skipping to refinement...");
        refine_consensus();
    }
    elsif (-s $rs_consensus) {
        $logger->info("Found existing RepeatScout results, skipping to filtering...");
        filter_repeats();
        refine_consensus();
    }
    else {
        # Run complete pipeline
        eval {
            # Process genome based on sampling strategy
            if ($sample_strategy eq "sample_parallel" && $num_chunks > 1) {
                run_parallel_repeatscout($sample_strategy, $sample_size, $chunk_size, $num_chunks, $lmer_size);
            } else {
                # Single run processing
                process_genome_by_strategy($sample_strategy, $num_chunks, $chunk_size);
                run_build_lmer_table($lmer_size);
                run_repeat_scout($lmer_size);
            }
            filter_repeats();
            refine_consensus();
        };
        if ($@) {
            $logger->error("Pipeline failed: $@");
            die "Pipeline execution failed\n";
        }
    }

    $logger->info("Pipeline completed successfully");
}

sub determine_effective_processing_size {
    my ($strategy, $sample_size, $num_chunks, $genome_size) = @_;
    
    if ($strategy eq "fragment_all") {
        # Processing entire genome
        return $genome_size;
    } elsif ($strategy eq "sample_single") {
        # Processing single sample
        return $sample_size;
    } elsif ($strategy eq "sample_parallel") {
        # Processing multiple samples in parallel - each chunk processed independently
        # Use single chunk size for lmer calculation since each chunk is processed separately
        return $sample_size;
    } else {
        # Fallback to genome size
        return $genome_size;
    }
}

sub calculate_lmer_size {
    my ($data_size) = @_;
    
    # Original formula: ceil(log4(data_size) + 1)
    my $calculated_lmer = ceil(log($data_size)/log(4) + 1);
    
    # Apply bounds based on RepeatScout best practices:
    # - Minimum: 14 (for very small data)
    # - Maximum: 16 (for optimal performance and memory usage)
    # - Sweet spot: 14-16 provides good balance of sensitivity and specificity
    
    if ($calculated_lmer < 14) {
        return 14;  # Minimum for adequate specificity
    } elsif ($calculated_lmer > 16) {
        return 16;  # Maximum for reasonable memory usage and performance
    } else {
        return $calculated_lmer;
    }
}

sub run_build_lmer_table {
    my ($lmer_size) = @_;
    my $freq_file = "$config{tmp_dir}/lmer.freq";

    # Check if lmer frequency table already exists
    if (!$config{force_overwrite} && -s $freq_file) {
        my $size = -s $freq_file;
        $logger->info("Found existing l-mer frequency table: $freq_file ($size bytes)");
        $logger->info("Skipping l-mer table building");
        return;
    }

    $logger->info("Building l-mer frequency table (l=$lmer_size)...");
    
    my $cmd = join(" ",
        "build_lmer_table",
        "-l $lmer_size",
        "-sequence $config{tmp_dir}/tmp.fa",
        "-freq $freq_file",
        "2> $config{tmp_dir}/build_lmer.log"
    );

    run_cmd($cmd);
    $logger->info("L-mer table building completed");
}

sub run_repeat_scout {
    my ($lmer_size) = @_;
    my $freq_file = "$config{tmp_dir}/lmer.freq";
    my $output = "$config{tmp_dir}/repeats.fa";

    # Check if RepeatScout output already exists and is valid
    if (should_skip_existing_file($output, "RepeatScout output")) {
        $logger->info("Skipping RepeatScout");
        return;
    }

    $logger->info("Running RepeatScout...");

    my $cmd = join(" ",
        "RepeatScout",
        "-sequence $config{tmp_dir}/tmp.fa",
        "-output $output",
        "-freq $freq_file",
        "-l $lmer_size",
        "-tandemdist 50",
        "-minthresh 3",
        "2>> $config{tmp_dir}/repeatscout.log"
    );

    run_cmd($cmd);
    $logger->info("RepeatScout processing completed");
}

sub filter_repeats {
    $logger->info("Filtering repeat sequences...");

    my $input = "$config{tmp_dir}/repeats.fa";
    my $output = "$config{tmp_dir}/repeats.filtered.fa";

    # Check if filtered output already exists and is valid
    if (should_skip_existing_file($output, "filtered repeats")) {
        $logger->info("Skipping repeat filtering");
        return;
    }

    $ENV{TRF_COMMAND} = which('trf');
    $ENV{RMBLAST_DIR} = $config{rmblast_dir};

    system("filter-stage-1.prl $input > $output 2> $config{tmp_dir}/filter.log") == 0
    	or die "Failed to run filter-stage-1.prl: $?\n";
    $logger->info("Repeat filtering completed");
}

sub refine_consensus {
    $logger->info("Refining consensus sequences...");

    my $input = "$config{tmp_dir}/repeats.filtered.fa";
    my $output_dir = "$config{work_dir}/refiner_output";
    my $final_output = "$config{work_dir}/consensi.fa";

    # Check if refinement outputs already exist and are valid
    my $refiner_consensus = "$output_dir/consensus_masking.fa";
    my $refiner_masked_genome = "$output_dir/genome_final_masked.fa";
    
    if (!$config{force_overwrite} && 
        should_skip_existing_file($final_output, "final consensus") && 
        should_skip_existing_file($refiner_consensus, "refiner consensus") && 
        should_skip_existing_file($refiner_masked_genome, "masked genome")) {
        $logger->info("All refinement outputs exist and are valid, skipping consensus refinement");
        return;
    }

    # Create required directories
    make_path($config{work_dir}) unless -d $config{work_dir};
    make_path($output_dir) unless -d $output_dir;
    
    # Ensure all input files exist
    die "Input file missing: $input\n" unless -f $input;
    
    # Determine genome input for Refiner based on sampling strategy
    my $refiner_genome = determine_refiner_genome_input();
    
    # Use the new Refiner pipeline
    my $cmd = join(" ",
        "python3", "$FindBin::RealBin/Refiner/main.py",
        '--repeatscout', $input,              # RepeatScout filtered output
        '--genome', $refiner_genome,          # Reference genome for Refiner
        '--output', $output_dir,              # Output directory for Refiner
        '--threads', $config{threads},        # Number of threads
        '--keep-temp',                        # Keep temporary files for debugging
        '2>', "$config{tmp_dir}/refiner.log"  # Log file
    );

    $logger->info("Running advanced TE consensus building with command: $cmd");
    system($cmd) == 0 
        or do {
            $logger->error("Advanced consensus refinement failed with status: $?");
            die "Advanced consensus refinement failed\n";
        };
    
    # Check for the main output file from Refiner and copy it to expected location
    if (-f $refiner_consensus) {
        $logger->info("Copying refined consensus from $refiner_consensus to $final_output");
        system("cp '$refiner_consensus' '$final_output'") == 0
            or do {
                $logger->error("Failed to copy consensus file");
                die "Failed to copy consensus file\n";
            };
    } else {
        $logger->error("Refiner output file not found: $refiner_consensus");
        die "Refiner output file not found\n";
    }
    
    $logger->info("Advanced consensus refinement completed successfully");
}

sub determine_refiner_genome_input {
    my $genome_size = -s $config{genome_file};
    my $gb_1_2 = 1.2 * 1024 * 1024 * 1024;  # 1.2GB
    
    # First try to reuse existing RepeatScout chunk.fa files
    my $reused_genome = reuse_existing_chunks($genome_size < $gb_1_2 ? 1 : 2);
    if ($reused_genome) {
        $logger->info("Successfully reused existing RepeatScout chunks for Refiner input");
        return $reused_genome;
    }
    
    # Fallback to original sampling strategy if chunks not available
    if ($genome_size < $gb_1_2) {
        # Small genome (<1.2Gb): use single sample
        $logger->info("Using single sample for Refiner input (genome < 1.2Gb)");
        return create_refiner_sample(500 * 1024 * 1024, 1);
    } else {
        # Large genome (>=1.2Gb): merge 2 samples
        $logger->info("Using merged samples for Refiner input (genome >= 1.2Gb)");
        return create_refiner_sample(500 * 1024 * 1024, 2);
    }
}

sub create_refiner_sample {
    my ($sample_size, $num_samples) = @_;
    
    my $refiner_genome = "$config{tmp_dir}/refiner_genome.fa";
    
    # Create index if needed
    unless (-f "$config{genome_file}.fai") {
        run_cmd("samtools faidx $config{genome_file}");
    }
    
    if ($num_samples == 1) {
        # Single sample
        my $chunk_size = 40 * 1024;  # 40KB chunks for better representation
        my $num_chunks = int($sample_size / $chunk_size);
        sample_random_chunks($config{genome_file}, $num_chunks, $chunk_size, $refiner_genome);
    } else {
        # Multiple samples - merge them
        my @temp_files;
        my $chunk_size = 40 * 1024;  # 40KB chunks
        my $num_chunks_per_sample = int($sample_size / $chunk_size);
        
        for my $i (1..$num_samples) {
            my $temp_file = "$config{tmp_dir}/refiner_sample_$i.fa";
            sample_random_chunks($config{genome_file}, $num_chunks_per_sample, $chunk_size, $temp_file);
            push @temp_files, $temp_file;
        }
        
        # Merge all samples
        my $cat_cmd = "cat " . join(" ", @temp_files) . " > $refiner_genome";
        run_cmd($cat_cmd);
        
        # Clean up temp files
        unlink @temp_files;
    }
    
    $logger->info("Created Refiner genome input: $refiner_genome");
    return $refiner_genome;
}

sub reuse_existing_chunks {
    my ($needed_chunks) = @_;
    
    my $refiner_genome = "$config{tmp_dir}/refiner_genome.fa";
    
    # Check if RepeatScout chunk.fa files exist and are valid
    my @available_chunks;
    for my $i (1..4) {  # Check up to 4 chunks (max number from parallel strategy)
        my $chunk_file = "$config{tmp_dir}/chunk_$i/chunk.fa";
        if (-s $chunk_file) {
            # Verify it's a valid FASTA file
            my $header_check = `head -1 "$chunk_file" 2>/dev/null`;
            if ($header_check && $header_check =~ /^>/) {
                push @available_chunks, $chunk_file;
                $logger->info("Found valid RepeatScout chunk: $chunk_file");
            }
        }
    }
    
    # Check if we have enough chunks for reuse
    if (scalar(@available_chunks) >= $needed_chunks) {
        $logger->info("Reusing " . scalar(@available_chunks) . " existing RepeatScout chunks (need $needed_chunks)");
        
        # Select the first N chunks we need
        my @chunks_to_use = @available_chunks[0..($needed_chunks-1)];
        
        if ($needed_chunks == 1) {
            # Single chunk: just copy it
            my $cmd = "cp '$chunks_to_use[0]' '$refiner_genome'";
            run_cmd($cmd);
        } else {
            # Multiple chunks: merge them
            my @quoted_files = map { "'$_'" } @chunks_to_use;
            my $cat_cmd = "cat " . join(" ", @quoted_files) . " > '$refiner_genome'";
            system($cat_cmd) == 0 or do {
                $logger->error("Failed to merge RepeatScout chunks for reuse");
                return undef;
            };
        }
        
        # Verify the output file was created successfully
        if (-s $refiner_genome) {
            my $size = -s $refiner_genome;
            $logger->info("Successfully created Refiner genome input from existing chunks: $refiner_genome ($size bytes)");
            return $refiner_genome;
        } else {
            $logger->warn("Failed to create Refiner genome from existing chunks");
            return undef;
        }
    } else {
        $logger->info("Insufficient RepeatScout chunks for reuse (found: " . scalar(@available_chunks) . ", need: $needed_chunks)");
        return undef;
    }
}

sub which {
    my ($program) = @_;
    for my $path (split /:/, $ENV{PATH}) {
        my $file = "$path/$program";
        return $file if -x $file;
    }
    $logger->error("Cannot find $program in PATH");
    die "Cannot find $program in PATH\n";
}

sub determine_sampling_strategy {
    my ($genome_size) = @_;
    
    my $mb_500 = 500 * 1024 * 1024;    # 500MB
    my $gb_1 = 1024 * 1024 * 1024;     # 1GB
    my $gb_3 = 3 * 1024 * 1024 * 1024; # 3GB
    my $gb_5 = 5 * 1024 * 1024 * 1024; # 5GB
    
    if ($genome_size < $mb_500) {
        # Small genome (<500Mb): fragment into 30Kb pieces
        my $strategy = "fragment_all";
        my $chunk_size = 30 * 1024;  # 30KB
        my $num_chunks = 1;  # Single run
        return ($strategy, $genome_size, $chunk_size, $num_chunks);
    }
    elsif ($genome_size >= $mb_500 && $genome_size < $gb_1) {
        # Medium genome (500Mb-1Gb): sample 500Mb in 30Kb chunks
        my $strategy = "sample_single";
        my $sample_size = 500 * 1024 * 1024;  # 500MB
        my $chunk_size = 30 * 1024;  # 30KB
        my $num_chunks = 1;  # Single run
        return ($strategy, $sample_size, $chunk_size, $num_chunks);
    }
    elsif ($genome_size >= $gb_1 && $genome_size < $gb_3) {
        # Large genome (1Gb-3Gb): sample 2×500Mb in 40Kb chunks
        my $strategy = "sample_parallel";
        my $sample_size = 500 * 1024 * 1024;  # 500MB per chunk
        my $chunk_size = 40 * 1024;  # 40KB
        my $num_chunks = 2;  # Two parallel runs
        return ($strategy, $sample_size, $chunk_size, $num_chunks);
    }
    elsif ($genome_size >= $gb_3 && $genome_size < $gb_5) {
        # Very large genome (3Gb-5Gb): sample 3×500Mb in 50Kb chunks
        my $strategy = "sample_parallel";
        my $sample_size = 500 * 1024 * 1024;  # 500MB per chunk
        my $chunk_size = 50 * 1024;  # 50KB
        my $num_chunks = 3;  # Three parallel runs
        return ($strategy, $sample_size, $chunk_size, $num_chunks);
    }
    else {
        # Massive genome (>5Gb): sample 4×500Mb in 50Kb chunks
        my $strategy = "sample_parallel";
        my $sample_size = 500 * 1024 * 1024;  # 500MB per chunk
        my $chunk_size = 50 * 1024;  # 50KB
        my $num_chunks = 4;  # Four parallel runs
        return ($strategy, $sample_size, $chunk_size, $num_chunks);
    }
}

sub process_genome_by_strategy {
    my ($strategy, $num_chunks, $chunk_size) = @_;
    
    $logger->info("Processing genome with strategy: $strategy");
    
    if ($strategy eq "fragment_all") {
        fragment_entire_genome($chunk_size);
    }
    elsif ($strategy eq "sample_single") {
        # Single sampling run
        my $sample_size = 500 * 1024 * 1024;  # 500MB
        my $num_samples = $sample_size / $chunk_size;
        sample_genome_chunks($num_samples, $chunk_size);
    }
    else {
        die "Unknown single sampling strategy: $strategy\n";
    }
}

sub fragment_entire_genome {
    my ($fragment_size) = @_;
    
    $logger->info("Fragmenting entire genome into ${fragment_size}bp pieces...");
    
    # For small genomes, we should just copy the whole genome
    # since fragmenting sequences shorter than fragment_size would result in empty output
    my $genome_size = -s $config{genome_file};
    
    if ($genome_size < $fragment_size) {
        # If genome is smaller than fragment size, just copy it as is
        $logger->info("Genome smaller than fragment size, using entire genome");
        my $cmd = "cp $config{genome_file} $config{tmp_dir}/tmp.fa";
        run_cmd($cmd);
    } else {
        # Use seqkit sliding with step size = 1/10 of window to ensure coverage
        # This creates overlapping fragments for better repeat detection
        my $step_size = int($fragment_size / 10);
        $step_size = $step_size > 0 ? $step_size : 1;
        
        my $cmd = join(" ",
            "seqkit sliding",
            "-s $step_size",           # Step size for overlap
            "-W $fragment_size",       # Window size = fragment size
            "-g",                      # Remove gap characters in sequences
            "$config{genome_file}",
            "> $config{tmp_dir}/tmp.fa"
        );
        
        run_cmd($cmd);
    }
    
    $logger->info("Genome fragmentation completed");
}

sub sample_genome_chunks {
    my ($num_samples, $chunk_size) = @_;
    
    $logger->info("Sampling $num_samples chunks of ${chunk_size}bp from genome...");
    
    # First create an index if it doesn't exist
    unless (-f "$config{genome_file}.fai") {
        run_cmd("samtools faidx $config{genome_file}");
    }
    
    # Use custom sampling function
    sample_random_chunks($config{genome_file}, $num_samples, $chunk_size, "$config{tmp_dir}/tmp.fa");
    
    $logger->info("Random chunk sampling completed");
}

sub run_parallel_repeatscout {
    my ($strategy, $sample_size_per_chunk, $chunk_size, $num_chunks, $lmer_size) = @_;
    
    $logger->info("Running parallel RepeatScout with $num_chunks chunks...");
    
    # Create index if needed
    unless (-f "$config{genome_file}.fai") {
        run_cmd("samtools faidx $config{genome_file}");
    }
    
    # Create parallel work directories
    my @chunk_dirs;
    for my $i (1..$num_chunks) {
        my $chunk_dir = "$config{tmp_dir}/chunk_$i";
        make_path($chunk_dir) unless -d $chunk_dir;
        push @chunk_dirs, $chunk_dir;
    }
    
    # Generate sampling for each chunk (skip if existing and valid)
    my $num_samples_per_chunk = int($sample_size_per_chunk / $chunk_size);
    for my $i (1..$num_chunks) {
        my $chunk_dir = $chunk_dirs[$i-1];
        my $chunk_fa = "$chunk_dir/chunk.fa";
        my $filtered_file = "$chunk_dir/repeats.filtered.fa";
        
        # Skip chunk generation if we already have a valid filtered result
        if (!$config{force_overwrite} && -s $filtered_file) {
            my $header_check = `head -1 "$filtered_file" 2>/dev/null`;
            if ($header_check && $header_check =~ /^>/) {
                my $size = -s $filtered_file;
                $logger->info("Chunk $i: Found existing filtered results ($size bytes), skipping chunk generation");
                next;
            } else {
                $logger->warn("Chunk $i: Existing filtered file appears invalid, will regenerate");
            }
        }
        
        # Skip chunk generation if we already have a valid chunk.fa
        if (!$config{force_overwrite} && -s $chunk_fa) {
            my $header_check = `head -1 "$chunk_fa" 2>/dev/null`;
            if ($header_check && $header_check =~ /^>/) {
                my $size = -s $chunk_fa;
                $logger->info("Chunk $i: Found existing chunk file ($size bytes), skipping generation");
                next;
            } else {
                $logger->warn("Chunk $i: Existing chunk file appears invalid, will regenerate");
            }
        }
        
        $logger->info("Generating chunk $i: $num_samples_per_chunk samples of ${chunk_size}bp");
        sample_random_chunks($config{genome_file}, $num_samples_per_chunk, $chunk_size, $chunk_fa);
    }
    
    # Run RepeatScout in parallel using fork
    use Parallel::ForkManager;
    my $pm = Parallel::ForkManager->new($num_chunks);
    
    my @results;
    $pm->run_on_finish(sub {
        my ($pid, $exit_code, $ident, $exit_signal, $core_dump, $data_ref) = @_;
        if ($data_ref) {
            push @results, $data_ref;
        }
    });
    
    # Fork and run RepeatScout for each chunk
    for my $i (1..$num_chunks) {
        my $chunk_dir = $chunk_dirs[$i-1];
        my $filtered_file = "$chunk_dir/repeats.filtered.fa";
        
        # Check if this chunk already has valid results before forking
        if (!$config{force_overwrite} && -s $filtered_file) {
            my $header_check = `head -1 "$filtered_file" 2>/dev/null`;
            if ($header_check && $header_check =~ /^>/) {
                my $size = -s $filtered_file;
                $logger->info("Chunk $i: Found existing filtered results ($size bytes), skipping processing");
                next;
            } else {
                $logger->warn("Chunk $i: Existing filtered file appears invalid, will regenerate");
            }
        }
        
        $pm->start and next;  # Fork
        
        # Child process
        eval {
            my $chunk_fa = "$chunk_dir/chunk.fa";
            my $freq_file = "$chunk_dir/lmer.freq";
            my $repeats_file = "$chunk_dir/repeats.fa";
            my $filtered_file = "$chunk_dir/repeats.filtered.fa";
            
            # Check if chunk.fa exists (should have been checked above, but safety check)
            die "Chunk file missing: $chunk_fa\n" unless -s $chunk_fa;
            
            # Build lmer table for this chunk (skip if exists and valid)
            if (!$config{force_overwrite} && -s $freq_file) {
                $logger->info("Found existing lmer table for chunk $i, skipping build_lmer_table");
            } else {
                $logger->info("Building lmer table for chunk $i...");
                my $cmd = "build_lmer_table -l $lmer_size -sequence $chunk_fa -freq $freq_file 2> $chunk_dir/build_lmer.log";
                system($cmd) == 0 or die "Failed to build lmer table for chunk $i: $?\n";
            }
            
            # Run RepeatScout for this chunk (skip if exists and valid)
            if (!$config{force_overwrite} && -s $repeats_file) {
                my $header_check = `head -1 "$repeats_file" 2>/dev/null`;
                if ($header_check && $header_check =~ /^>/) {
                    $logger->info("Found existing RepeatScout results for chunk $i, skipping RepeatScout");
                } else {
                    $logger->warn("Existing RepeatScout file appears invalid for chunk $i, will regenerate");
                    goto run_repeatscout;
                }
            } else {
                run_repeatscout:
                $logger->info("Running RepeatScout for chunk $i...");
                my $cmd = "RepeatScout -sequence $chunk_fa -output $repeats_file -freq $freq_file -l $lmer_size -tandemdist 50 -minthresh 3 2> $chunk_dir/repeatscout.log";
                system($cmd) == 0 or die "Failed to run RepeatScout for chunk $i: $?\n";
            }
            
            # Filter results for this chunk (skip if exists and valid)
            if (!$config{force_overwrite} && -s $filtered_file) {
                my $header_check = `head -1 "$filtered_file" 2>/dev/null`;
                if ($header_check && $header_check =~ /^>/) {
                    $logger->info("Found existing filtered results for chunk $i, skipping filtering");
                } else {
                    $logger->warn("Existing filtered file appears invalid for chunk $i, will regenerate");
                    goto run_filter;
                }
            } else {
                run_filter:
                $logger->info("Filtering repeats for chunk $i...");
                $ENV{TRF_COMMAND} = which('trf');
                $ENV{RMBLAST_DIR} = $config{rmblast_dir};
                system("filter-stage-1.prl $repeats_file > $filtered_file 2> $chunk_dir/filter.log") == 0
                    or die "Failed to filter repeats for chunk $i: $?\n";
            }
            
            $logger->info("Chunk $i completed successfully");
        };
        
        if ($@) {
            $logger->error("Chunk $i failed: $@");
            $pm->finish(1);  # Exit child with error
        } else {
            $pm->finish(0);  # Exit child successfully
        }
    }
    
    $pm->wait_all_children;
    
    # Merge results from all chunks
    $logger->info("Merging results from $num_chunks chunks...");
    merge_parallel_results(\@chunk_dirs);
    
    $logger->info("Parallel RepeatScout processing completed");
}

sub merge_parallel_results {
    my ($chunk_dirs) = @_;
    
    my $merged_file = "$config{tmp_dir}/repeats.filtered.fa";
    
    # Collect all filtered results
    my @filtered_files;
    for my $chunk_dir (@$chunk_dirs) {
        my $filtered_file = "$chunk_dir/repeats.filtered.fa";
        if (-s $filtered_file) {
            # Verify file contains sequences by checking for FASTA headers
            my $header_check = `head -1 "$filtered_file" 2>/dev/null`;
            if ($header_check && $header_check =~ /^>/) {
                push @filtered_files, $filtered_file;
                $logger->info("Found valid filtered results in $chunk_dir");
            } else {
                $logger->warn("Filtered file in $chunk_dir exists but appears empty or invalid");
            }
        } else {
            $logger->warn("No filtered results found in $chunk_dir");
        }
    }
    
    die "No filtered results found from any chunk\n" unless @filtered_files;
    
    # Direct concatenation with proper file quoting
    $logger->info("Concatenating results from " . scalar(@filtered_files) . " chunks...");
    
    # Quote all filenames to handle spaces and special characters
    my @quoted_files = map { "'$_'" } @filtered_files;
    my $cat_cmd = "cat " . join(" ", @quoted_files) . " > '$merged_file'";
    
    # Use system call instead of run_cmd to avoid string escaping issues
    $logger->info("Executing merge command: $cat_cmd");
    my $result = system($cat_cmd);
    if ($result != 0) {
        $logger->error("Failed to merge chunk results: exit code " . ($result >> 8));
        die "Failed to merge chunk results\n";
    }
    
    # Verify the merged file was created successfully
    if (-s $merged_file) {
        my $merged_size = -s $merged_file;
        $logger->info("Results merging completed successfully - merged file size: $merged_size bytes");
    } else {
        $logger->error("Merged file is empty or was not created");
        die "Merged file is empty or was not created\n";
    }
}


sub sample_random_chunks {
    my ($genome_file, $num_samples, $chunk_size, $output_file) = @_;
    
    # Read genome index to get sequence information
    my @sequences;
    open(my $fai_fh, '<', "$genome_file.fai") or die "Cannot read fai file: $!\n";
    while (my $line = <$fai_fh>) {
        chomp $line;
        my ($seq_name, $length) = split(/\t/, $line);
        # Include all sequences, even those shorter than chunk size
        push @sequences, {name => $seq_name, length => $length};
    }
    close($fai_fh);
    
    die "No sequences long enough for sampling\n" unless @sequences;
    
    # Calculate total available length
    my $total_length = 0;
    for my $seq (@sequences) {
        if ($seq->{length} >= $chunk_size) {
            $total_length += $seq->{length} - $chunk_size + 1;  # Available positions for sampling
        } else {
            $total_length += 1;  # Can use the whole sequence as one sample
        }
    }
    
    $logger->info("Total sequences: " . scalar(@sequences));
    $logger->info("Total available positions: $total_length");
    
    # Generate random sampling positions
    my @samples;
    my $attempts = 0;
    my $max_attempts = $num_samples * 10;
    
    while (@samples < $num_samples && $attempts < $max_attempts) {
        $attempts++;
        
        # Randomly select a sequence weighted by length
        my $rand_pos = int(rand($total_length));
        my $cumulative = 0;
        my $selected_seq;
        
        for my $seq (@sequences) {
            my $available_positions;
            if ($seq->{length} >= $chunk_size) {
                $available_positions = $seq->{length} - $chunk_size + 1;
            } else {
                $available_positions = 1;  # Can only take the whole sequence
            }
            $cumulative += $available_positions;
            if ($rand_pos < $cumulative) {
                $selected_seq = $seq;
                last;
            }
        }
        
        next unless $selected_seq;
        
        # Handle position selection based on sequence length
        my ($start_pos, $end_pos);
        if ($selected_seq->{length} >= $chunk_size) {
            # Random position within selected sequence
            my $max_start = $selected_seq->{length} - $chunk_size;
            $start_pos = int(rand($max_start)) + 1;  # 1-based coordinates
            $end_pos = $start_pos + $chunk_size - 1;
        } else {
            # For sequences shorter than chunk size, take the whole sequence
            $start_pos = 1;
            $end_pos = $selected_seq->{length};
        }
        
        # Create sample entry
        my $sample_id = sprintf("sample_%04d_%s_%d_%d", 
                               scalar(@samples), $selected_seq->{name}, $start_pos, $end_pos);
        
        push @samples, {
            id => $sample_id,
            seq => $selected_seq->{name},
            start => $start_pos,
            end => $end_pos
        };
        
        if (@samples % 500 == 0) {
            $logger->info("Generated " . scalar(@samples) . " / $num_samples samples");
        }
    }
    
    $logger->info("Generated " . scalar(@samples) . " samples total");
    
    # Extract sequences using samtools faidx
    open(my $out_fh, '>', $output_file) or die "Cannot create output file: $!\n";
    
    for my $sample (@samples) {
        my $region = "$sample->{seq}:$sample->{start}-$sample->{end}";
        my $seq_data = `samtools faidx $genome_file $region`;
        
        if ($seq_data) {
            # Replace header with our sample ID
            $seq_data =~ s/^>[^\n]+/>$sample->{id}/;
            print $out_fh $seq_data;
        }
    }
    
    close($out_fh);
    $logger->info("Sequence extraction completed");
}

sub run_cmd {
    my $cmd = shift;
    $cmd =~ s/[`;\$]/\\$&/g;
    $logger->info("Executing command: $cmd");
    
    my $output = `$cmd 2>&1`;
    my $exit_code = $? >> 8;
    
    if ($exit_code != 0) {
        $logger->error("Command failed with exit code: $exit_code");
        $logger->error("Command output:");
        $logger->error($output);
        die "Command execution failed\n";
    }
    
    $logger->debug("Command completed successfully");
    return $output;
}

