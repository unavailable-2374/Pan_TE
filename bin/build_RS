#!/usr/bin/env perl

use strict;
use warnings;
use Cwd qw(getcwd);
use FindBin;
use lib $FindBin::RealBin;
use lib qw(/home/shuoc/tool/Pan_TE/bin 
           /home/shuoc/tool/Pan_TE/share 
           /home/shuoc/tool/miniconda3/envs/PGTA/share/RepeatMasker 
           /home/shuoc/tool/miniconda3/envs/PGTA/lib/perl5/site_perl);

use File::Path qw(make_path);
use File::Spec;
use Getopt::Long;
use Pod::Usage;
use POSIX qw(:sys_wait_h ceil floor);
use File::Basename;
use Log::Log4perl qw(:easy);
use Parallel::ForkManager;
use List::Util qw(min max);

# Configure logging
Log::Log4perl->easy_init({
    level   => $INFO,
    layout  => '[%d] %p %m%n',
    file    => '>>' . getcwd() . '/build_rs.log'
});
my $logger = Log::Log4perl->get_logger();

our $VERSION = '2.0.0';

# Program Configuration
my %config = (
    tmp_dir => undef,
    genome_file => undef,
    threads => 4,
    sample_size => 'auto',
    rmblast_dir => undef,
    work_dir => undef,
    force_overwrite => 0
);

# Parse and validate arguments
get_and_validate_args();

# Set up environment
$logger->info("Setting up environment...");
setup_environment();

# Main execution flow
run_pipeline();

exit(0);

sub get_and_validate_args {
    GetOptions(
        'help|h'        => \my $help,
        'version|v'     => \my $version,
        'tmp=s'         => \$config{tmp_dir},
        'genome=s'      => \$config{genome_file},
        'threads=i'     => \$config{threads},
        'sample=s'      => \$config{sample_size},
        'workdir=s'     => \$config{work_dir},
        'force|f'       => \$config{force_overwrite}
    ) or pod2usage(2);

    pod2usage(1) if $help;
    if ($version) {
        print "build_RS version $VERSION\n";
        exit(0);
    }

    # Try to locate rmblast directory through multiple methods
    $config{rmblast_dir} = $ENV{RMBLAST_DIR};
    unless ($config{rmblast_dir}) {
        eval {
            use RepModelConfig;
            $config{rmblast_dir} = $RepModelConfig::configuration->{'RMBLAST_DIR'}->{'value'};
        };
    }

    # If still not found, try locating rmblastn
    unless ($config{rmblast_dir} && -x "$config{rmblast_dir}/rmblastn") {
        $config{rmblast_dir} = locate_rmblastn();
    }

    my @required = qw(tmp_dir genome_file work_dir); 
    for my $req (@required) {
        unless (defined $config{$req}) {
            $logger->error("Missing required parameter: --$req");
            die "Error: --$req is required\n";
        }
    }

    unless ($config{rmblast_dir}) {
        $logger->error("RMBLAST_DIR environment variable or configuration not found");
        die "Error: RMBLAST_DIR must be set in environment or configuration\n";
    }

    # Log configuration
    $logger->info("Configuration:");
    $logger->info("  Genome file: $config{genome_file}");
    $logger->info("  Threads: $config{threads}");
    $logger->info("  Working directory: $config{work_dir}");
    $logger->info("  RMBlast directory: $config{rmblast_dir}");
}

sub locate_rmblastn {
    $logger->info("Attempting to locate rmblastn...");
    
    # Try whereis command
    my $whereis_out = `whereis rmblastn`;
    if ($whereis_out =~ /rmblastn:\s+(\S+)/) {
        my $path = $1;
        if (-x $path) {
            $logger->info("Found rmblastn via whereis: $path");
            return dirname($path);
        }
    }
    
    # Try find command in common locations
    for my $base_dir ("/usr/local", "/usr", $ENV{HOME}, "/opt") {
        my $cmd = "find $base_dir -name rmblastn -type f 2>/dev/null";
        my $find_out = `$cmd`;
        if ($find_out) {
            chomp($find_out);
            my @paths = split(/\n/, $find_out);
            foreach my $path (@paths) {
                if (-x $path) {
                    $logger->info("Found rmblastn via find: $path");
                    return dirname($path);
                }
            }
        }
    }
    
    $logger->error("Could not locate rmblastn executable");
    die "Error: rmblastn not found in system\n";
}

sub setup_environment {
    # Create necessary directories
    for my $dir ($config{tmp_dir}, $config{work_dir}) {
        unless (-d $dir) {
            $logger->info("Creating directory: $dir");
            make_path($dir) or die "Failed to create directory $dir: $!\n";
        }
    }

    # Change to work directory
    chdir $config{work_dir} or die "Cannot change to work directory: $!\n";
    $logger->info("Changed to working directory: " . getcwd());
}

sub run_pipeline {
    $logger->info("Starting multi-parameter RepeatScout pipeline...");

    # Get genome size and determine strategy
    my $genome_size = -s $config{genome_file};
    my $readable_size = sprintf("%.2f", $genome_size/1e9) . " GB";
    $logger->info("Processing genome of size: $readable_size");

    # Determine multi-parameter strategy based on genome size
    my ($strategy, $data_size, $lmer_sizes_ref, $copy_thresholds_ref, $num_samples) = determine_strategy($genome_size);
    
    $logger->info("Strategy: $strategy");
    $logger->info("Data size per sample: " . sprintf("%.2f", $data_size/1e6) . " MB");
    $logger->info("Number of samples: $num_samples");
    $logger->info("L-mer sizes: " . join(", ", @$lmer_sizes_ref));
    $logger->info("Copy thresholds: " . join(", ", @$copy_thresholds_ref));
    
    my $total_jobs = $num_samples * scalar(@$lmer_sizes_ref);
    $logger->info("Total RepeatScout jobs: $total_jobs");

    # Check for existing RepeatScout checkpoint
    my $rs_output = "$config{tmp_dir}/repeats.fa";
    my $repeatscout_checkpoint = "repeatscout_complete";
    
    if (check_checkpoint($repeatscout_checkpoint) && -s $rs_output && !$config{force_overwrite}) {
        $logger->info("Found existing RepeatScout checkpoint and results, skipping to refinement...");
    } elsif (-s $rs_output && !$config{force_overwrite}) {
        $logger->info("Found existing RepeatScout results without checkpoint, creating checkpoint...");
        create_checkpoint($repeatscout_checkpoint);
    } else {
        # Run multi-parameter RepeatScout
        run_multi_parameter_repeatscout($strategy, $data_size, $lmer_sizes_ref, $copy_thresholds_ref, $num_samples);
        
        # Create checkpoint after successful RepeatScout completion
        if (-s $rs_output) {
            create_checkpoint($repeatscout_checkpoint);
            $logger->info("RepeatScout completion checkpoint created successfully");
        } else {
            $logger->error("RepeatScout did not produce output file: $rs_output");
            die "RepeatScout pipeline failed\n";
        }
    }
    
    # Run Refiner pipeline
    refine_consensus();
    
    # Final cleanup of tmp directory - remove all HTML and remaining part files
    cleanup_tmp_directory();

    $logger->info("Pipeline completed successfully");
}

sub determine_strategy {
    my ($genome_size) = @_;
    
    my $mb_200 = 200 * 1024 * 1024;    # 200MB
    my $mb_800 = 400 * 1024 * 1024;   # 400MB  
    my $gb_1 = 1 * 1024 * 1024 * 1024; # 1GB
    my $gb_2 = 2 * 1024 * 1024 * 1024; # 2GB
    my $gb_3 = 3 * 1024 * 1024 * 1024; # 3GB
    my $gb_5 = 5 * 1024 * 1024 * 1024; # 5GB
    
    my (@lmer_sizes, @copy_thresholds, $strategy, $data_size, $num_samples);
    
    if ($genome_size < $mb_200) {
        # Small genomes <200MB: use full genome
        @lmer_sizes = (14, 16);
        @copy_thresholds = (5, 3);
        $strategy = "small_genome_parallel";
        $data_size = $genome_size;
        $num_samples = 1;
    }
    elsif ($genome_size >= $mb_200 && $genome_size < $gb_1) {
        # Medium genomes 200MB-1GB
        @lmer_sizes = (14, 16, 18);
        @copy_thresholds = (5, 5, 3);
        if ($genome_size < $mb_800) {
            $strategy = "medium_genome_full";
            $data_size = $genome_size;
            $num_samples = 1;
        } else {
            $strategy = "medium_genome_sampled";
            $data_size = $mb_800;  # Sample 800MB
            $num_samples = 1;
        }
    }
    elsif ($genome_size >= $gb_1 && $genome_size < $gb_2) {
        # Large genomes 1-2GB: single 800MB sample
        @lmer_sizes = (14, 16, 18);
        @copy_thresholds = (5, 5, 3);
        $strategy = "large_genome_sampled";
        $data_size = $mb_800;  # Sample 800MB
        $num_samples = 1;
    }
    elsif ($genome_size >= $gb_2 && $genome_size < $gb_3) {
        # Very large genomes 2-3GB: 2 samples of 800MB each (6 total jobs: 2 samples × 3 lmers)
        @lmer_sizes = (14, 16, 18);
        @copy_thresholds = (5, 5, 3);
        $strategy = "very_large_genome_multi_sampled";
        $data_size = $mb_800;  # Each sample 800MB
        $num_samples = 2;
    }
    elsif ($genome_size >= $gb_3 && $genome_size < $gb_5) {
        # Huge genomes 3-5GB: 3 samples of 800MB each (9 total jobs: 3 samples × 3 lmers)
        @lmer_sizes = (14, 16, 18);
        @copy_thresholds = (5, 5, 3);
        $strategy = "huge_genome_multi_sampled";
        $data_size = $mb_800;  # Each sample 800MB
        $num_samples = 3;
    }
    else {
        # Massive genomes >=5GB: 4 samples of 800MB each (12 total jobs: 4 samples × 3 lmers)
        @lmer_sizes = (14, 16, 18);
        @copy_thresholds = (5, 5, 3);
        $strategy = "massive_genome_multi_sampled";
        $data_size = $mb_800;  # Each sample 800MB
        $num_samples = 4;
    }
    
    return ($strategy, $data_size, \@lmer_sizes, \@copy_thresholds, $num_samples);
}

sub run_multi_parameter_repeatscout {
    my ($strategy, $data_size, $lmer_sizes_ref, $copy_thresholds_ref, $num_samples) = @_;
    
    my @lmer_sizes = @$lmer_sizes_ref;
    my @copy_thresholds = @$copy_thresholds_ref;
    
    $logger->info("Running multi-parameter RepeatScout analysis...");
    
    # Check for dust_trf_masking checkpoint first
    my $masking_checkpoint = "dust_trf_masking";
    my @working_genomes;
    
    if (check_checkpoint($masking_checkpoint) && !$config{force_overwrite}) {
        $logger->info("Found existing dust and TRF masking checkpoint, looking for masked files...");
        
        # Try to find existing masked files based on strategy
        if ($strategy =~ /multi_sampled/) {
            # Look for multiple masked sample files
            for my $sample_id (1..$num_samples) {
                my $masked_file = "$config{tmp_dir}/genome_sample_${sample_id}_masked.fa";
                if (-s $masked_file) {
                    push @working_genomes, $masked_file;
                    $logger->info("Found existing masked sample $sample_id: $masked_file");
                }
            }
        } elsif ($strategy =~ /sampled/) {
            # Look for single masked sample file
            my $masked_file = "$config{tmp_dir}/genome_sample_masked.fa";
            if (-s $masked_file) {
                push @working_genomes, $masked_file;
                $logger->info("Found existing masked sample: $masked_file");
            }
        } else {
            # Look for full genome masked file
            my $masked_file = "$config{tmp_dir}/tmp_masked.fa";
            if (-s $masked_file) {
                push @working_genomes, $masked_file;
                $logger->info("Found existing masked genome: $masked_file");
            }
        }
    }
    
    # If we found all expected masked files, use them; otherwise create new ones
    if (@working_genomes == $num_samples || (@working_genomes == 1 && $num_samples == 1)) {
        $logger->info("Using existing masked files from checkpoint");
    } else {
        $logger->info("Creating new genome samples with masking...");
        @working_genomes = ();  # Clear any partial results
        
        # Prepare working genomes (multiple samples for large genomes)
        if ($strategy =~ /multi_sampled/) {
            # Create multiple non-overlapping samples for large genomes
            $logger->info("Creating $num_samples non-overlapping samples of " . sprintf("%.1f MB each", $data_size/1e6));
            @working_genomes = create_non_overlapping_genome_samples($data_size, $num_samples);
        } elsif ($strategy =~ /sampled/) {
            # Single sample for medium genomes
            my $working_genome = create_genome_sample($data_size);
            push @working_genomes, $working_genome;
        } else {
            # Full genome for small genomes
            my $working_genome = "$config{tmp_dir}/tmp.fa";
            run_cmd("cp $config{genome_file} $working_genome");
            
            # Apply masking to full genome
            my @masked_files = apply_soft_masking_to_samples($working_genome);
            @working_genomes = @masked_files;
        }
    }
    
    # Run parallel RepeatScout with different sample×lmer combinations
    my $pm = Parallel::ForkManager->new($config{threads});
    my @result_files;
    
    my $job_id = 0;
    for my $sample_idx (0..$#working_genomes) {
        my $sample_genome = $working_genomes[$sample_idx];
        
        for my $lmer_idx (0..$#lmer_sizes) {
            $pm->start and next;
            
            my $lmer = $lmer_sizes[$lmer_idx];
            my $threshold = $copy_thresholds[$lmer_idx];
            
            # Include sample ID in job naming for multi-sample strategies
            # run_single_repeatscout_job now includes filtering
            my $result_file;
            if ($strategy =~ /multi_sampled/) {
                $result_file = run_single_repeatscout_job($sample_genome, $lmer, $threshold, $job_id, $sample_idx + 1);
            } else {
                $result_file = run_single_repeatscout_job($sample_genome, $lmer, $threshold, $job_id);
            }
            
            $pm->finish(0, { result_file => $result_file });
            $job_id++;
        }
    }
    
    # Collect results from parallel processes
    $pm->run_on_finish(sub {
        my ($pid, $exit_code, $ident, $exit_signal, $core_dump, $data) = @_;
        if ($data && $data->{result_file} && -s $data->{result_file}) {
            push @result_files, $data->{result_file};
        }
    });
    
    $pm->wait_all_children;
    
    # Results are already filtered, just merge them
    merge_repeatscout_results(\@result_files, "$config{tmp_dir}/repeats.fa");
    
    $logger->info("Multi-parameter RepeatScout analysis completed ($num_samples samples × " . scalar(@lmer_sizes) . " lmers = " . scalar(@result_files) . " jobs)");
    $logger->info("All RepeatScout jobs completed with immediate filtering, results merged and ready for Refiner pipeline");
}

sub create_genome_sample {
    my ($target_size) = @_;
    
    $logger->info("Creating genome sample of " . sprintf("%.1f MB", $target_size/1e6) . " using BED-based sampling");
    
    # Create initial empty mask BED file
    my $mask_bed = "$config{tmp_dir}/sample_mask.bed";
    open(my $mask_fh, '>', $mask_bed) or die "Cannot create mask BED file: $!";
    close($mask_fh);
    
    # Generate sample using BED-based approach
    $logger->info("Generating BED samples for target size: " . sprintf("%.1f MB", $target_size/1e6));
    my @sample_beds = generate_progressive_bed_samples($target_size, 1, $mask_bed);
    
    unless (@sample_beds) {
        $logger->error("No BED files were generated");
        die "BED file generation failed\n";
    }
    
    my $sample_bed = $sample_beds[0];
    $logger->info("Generated BED file: $sample_bed");
    
    # Check if BED file was actually created
    unless (-s $sample_bed) {
        $logger->error("BED file is empty or missing: $sample_bed");
        die "BED file creation failed\n";
    }
    
    # Extract sequences using bedtools
    my $sample_file = "$config{tmp_dir}/genome_sample.fa";
    my $extraction_success = extract_sequences_from_bed($sample_bed, $sample_file);
    
    unless ($extraction_success) {
        $logger->error("Failed to extract sequences from BED file: $sample_bed");
        die "Sequence extraction failed for single sample\n";
    }
    
    # Apply soft masking
    my @masked_files = apply_soft_masking_to_samples($sample_file);
    my $masked_sample = $masked_files[0];
    
    unless (-s $masked_sample) {
        $logger->error("Masking failed - no output file created");
        die "Sample masking failed\n";
    }
    
    my $actual_size = -s $masked_sample;
    $logger->info("BED-based sample created and masked: " . sprintf("%.1f MB", $actual_size/1e6));
    $logger->info("Returning masked file for RepeatScout: $masked_sample");
    
    return $masked_sample;
}

sub create_non_overlapping_genome_samples {
    my ($target_size, $num_samples) = @_;
    
    $logger->info("Creating $num_samples non-overlapping samples of " . sprintf("%.1f MB each", $target_size/1e6) . " using progressive BED-based sampling");
    
    # Create initial empty mask BED file
    my $mask_bed = "$config{tmp_dir}/progressive_mask.bed";
    open(my $mask_fh, '>', $mask_bed) or die "Cannot create mask BED file: $!";
    close($mask_fh);
    
    # Generate progressive samples using BED-based approach
    $logger->info("Generating progressive BED samples for $num_samples samples, target size: " . sprintf("%.1f MB each", $target_size/1e6));
    my @sample_beds = generate_progressive_bed_samples($target_size, $num_samples, $mask_bed);
    
    unless (@sample_beds) {
        $logger->error("No BED files were generated for any samples");
        die "BED file generation failed for all samples\n";
    }
    
    $logger->info("Generated " . scalar(@sample_beds) . " BED files");
    
    my @sample_files;
    
    # Extract sequences for each sample
    for my $sample_id (1..$num_samples) {
        my $sample_bed = $sample_beds[$sample_id - 1];
        unless ($sample_bed) {
            $logger->warn("No BED file for sample $sample_id, skipping");
            next;
        }
        
        $logger->info("Processing sample $sample_id with BED file: $sample_bed");
        
        # Check BED file before extraction
        unless (-s $sample_bed) {
            $logger->error("Sample $sample_id BED file is empty or missing: $sample_bed");
            next;
        }
        
        my $sample_file = "$config{tmp_dir}/genome_sample_${sample_id}.fa";
        my $extraction_success = extract_sequences_from_bed($sample_bed, $sample_file);
        
        if ($extraction_success) {
            $logger->info("Sample $sample_id extraction successful: " . sprintf("%.1f MB", (-s $sample_file)/1e6));
            push @sample_files, $sample_file;
        } else {
            $logger->error("Failed to extract sequences for sample $sample_id from BED: $sample_bed");
        }
    }
    
    if (@sample_files == 0) {
        $logger->error("No sample files were successfully created");
        die "All sample extractions failed\n";
    }
    
    $logger->info("Successfully created " . scalar(@sample_files) . " sample files out of $num_samples requested");
    
    # Apply soft masking to all samples in parallel
    $logger->info("Applying soft masking to all " . scalar(@sample_files) . " samples...");
    my @masked_files = apply_soft_masking_to_samples(@sample_files);
    
    # Filter out failed masking results
    @masked_files = grep { -s $_ } @masked_files;
    
    if (@masked_files == 0) {
        $logger->error("No masked files were successfully created");
        die "All sample masking failed\n";
    }
    
    # Log final results
    for my $i (0..$#masked_files) {
        my $actual_size = -s $masked_files[$i];
        $logger->info("Progressive BED-based sample " . ($i + 1) . " created and masked: " . sprintf("%.1f MB", $actual_size/1e6));
        $logger->info("Returning masked file " . ($i + 1) . " for RepeatScout: $masked_files[$i]");
    }
    
    return @masked_files;
}

sub generate_progressive_bed_samples {
    my ($target_size, $num_samples, $mask_bed) = @_;
    
    # Create genome index if needed
    unless (-f "$config{genome_file}.fai") {
        run_cmd("samtools faidx $config{genome_file}");
    }
    
    my $window_size = 500000;  # 500kb windows
    my $overlap_size = 0;  # No overlap between windows
    my $effective_window = $window_size;  # Full 500kb chunks
    my $num_windows_per_sample = int($target_size / $effective_window);
    
    $logger->info("Generating $num_samples progressive BED samples with ${num_windows_per_sample} windows each");
    
    my @sample_beds;
    
    for my $sample_id (1..$num_samples) {
        $logger->info("Generating BED file for sample $sample_id...");
        
        my $sample_bed = "$config{tmp_dir}/sample_${sample_id}.bed";
        $logger->info("Sample $sample_id BED file path: $sample_bed");
        
        generate_random_bed_regions($sample_bed, $num_windows_per_sample, $window_size, $mask_bed);
        
        # Check if BED file was created
        if (-f $sample_bed) {
            my $bed_size = count_bed_regions($sample_bed);
            $logger->info("Sample $sample_id BED file created with $bed_size regions");
            
            if ($bed_size > 0) {
                # Update the mask with current sample regions
                update_mask_bed($mask_bed, $sample_bed);
                push @sample_beds, $sample_bed;
            } else {
                $logger->warn("Sample $sample_id BED file is empty, skipping");
            }
        } else {
            $logger->error("Failed to create BED file for sample $sample_id: $sample_bed");
        }
    }
    
    return @sample_beds;
}

sub generate_random_bed_regions {
    my ($output_bed, $num_windows, $window_size, $mask_bed) = @_;
    
    $logger->info("Generating $num_windows random BED regions of size $window_size");
    
    # Get genome chromosome sizes
    my %chr_sizes = read_genome_sizes("$config{genome_file}.fai");
    my @chromosomes = keys %chr_sizes;
    
    if (@chromosomes == 0) {
        $logger->error("No chromosomes found in genome index: $config{genome_file}.fai");
        return;
    }
    
    $logger->info("Found " . scalar(@chromosomes) . " chromosomes");
    
    # Calculate total genome size for proportional sampling
    my $total_genome_size = 0;
    $total_genome_size += $chr_sizes{$_} for @chromosomes;
    
    $logger->info("Total genome size: " . sprintf("%.1f MB", $total_genome_size/1e6));
    
    # Filter chromosomes that are large enough
    my @valid_chromosomes = grep { $chr_sizes{$_} >= $window_size } @chromosomes;
    
    if (@valid_chromosomes == 0) {
        $logger->error("No chromosomes are large enough for window size $window_size");
        return;
    }
    
    $logger->info("Valid chromosomes for sampling: " . scalar(@valid_chromosomes));
    
    open(my $bed_fh, '>', $output_bed) or die "Cannot create BED file $output_bed: $!";
    
    my $attempts = 0;
    my $max_attempts = $num_windows * 10;  # Allow up to 10x attempts to find non-masked regions
    my $regions_generated = 0;
    
    while ($regions_generated < $num_windows && $attempts < $max_attempts) {
        $attempts++;
        
        # Choose random chromosome from valid ones
        my $chr = $valid_chromosomes[int(rand(@valid_chromosomes))];
        my $chr_size = $chr_sizes{$chr};
        
        # Generate random position with proper boundary checks
        my $max_start = $chr_size - $window_size;
        my $start = int(rand($max_start + 1));  # +1 to include max_start
        my $end = $start + $window_size;
        
        # 确保重叠区域不会导致边界问题
        if ($end > $chr_size) {
            $end = $chr_size;
            $start = max(0, $end - $window_size);
        }
        
        # Double check boundaries (should not be needed but safety first)
        if ($start < 0) {
            $start = 0;
            $end = $window_size;
        }
        if ($end > $chr_size) {
            $end = $chr_size;
            $start = $chr_size - $window_size;
        }
        
        # Final validation
        if ($start < 0 || $end > $chr_size || $start >= $end) {
            next;  # Skip this iteration if coordinates are invalid
        }
        
        # Check if this region overlaps with mask
        unless (region_overlaps_mask($chr, $start, $end, $mask_bed)) {
            print $bed_fh "$chr\t$start\t$end\n";
            $regions_generated++;
            
            # Update mask without overlap since overlap_size is 0
            # Simply add the current region to the mask
            my $temp_region = "$config{tmp_dir}/temp_region_$$.bed";
            open(my $temp_fh, '>', $temp_region) or next;
            print $temp_fh "$chr\t$start\t$end\n";
            close($temp_fh);
            update_mask_bed($mask_bed, $temp_region);
            unlink($temp_region);
            
            # Log progress every 100 regions
            if ($regions_generated % 100 == 0) {
                $logger->info("Generated $regions_generated/$num_windows regions");
            }
        }
    }
    
    close($bed_fh);
    
    if ($regions_generated < $num_windows) {
        $logger->warn("Only generated $regions_generated regions out of $num_windows requested (after $attempts attempts)");
    } else {
        $logger->info("Successfully generated $regions_generated BED regions");
    }
}

sub read_genome_sizes {
    my ($fai_file) = @_;
    my %sizes;
    
    open(my $fai_fh, '<', $fai_file) or die "Cannot open FAI file $fai_file: $!";
    while (my $line = <$fai_fh>) {
        chomp $line;
        my ($chr, $size) = split(/\t/, $line);
        $sizes{$chr} = $size;
    }
    close($fai_fh);
    
    return %sizes;
}

sub choose_weighted_chromosome {
    my ($chr_sizes_ref, $total_size) = @_;
    my %chr_sizes = %$chr_sizes_ref;
    
    my $rand_pos = rand($total_size);
    my $cumulative = 0;
    
    for my $chr (sort keys %chr_sizes) {
        $cumulative += $chr_sizes{$chr};
        if ($rand_pos <= $cumulative) {
            return $chr;
        }
    }
    
    # Fallback to first chromosome if something goes wrong
    return (sort keys %chr_sizes)[0];
}

sub region_overlaps_mask {
    my ($chr, $start, $end, $mask_bed) = @_;
    
    # If mask is empty, no overlap
    return 0 unless -s $mask_bed;
    
    # Create temporary BED file for this region
    my $temp_region_bed = "$config{tmp_dir}/temp_region_$$.bed";
    open(my $temp_fh, '>', $temp_region_bed) or die "Cannot create temp BED: $!";
    print $temp_fh "$chr\t$start\t$end\n";
    close($temp_fh);
    
    # Use bedtools intersect to check overlap
    my $cmd = "bedtools intersect -a $temp_region_bed -b $mask_bed -u 2>/dev/null";
    my $result = `$cmd`;
    
    # Clean up temp region file
    unlink($temp_region_bed);
    
    # Return true if there's any overlap
    return length($result) > 0;
}

sub update_mask_bed {
    my ($mask_bed, $new_regions_bed) = @_;
    
    # If mask is empty, just copy new regions
    unless (-s $mask_bed) {
        run_cmd("cp $new_regions_bed $mask_bed");
        return;
    }
    
    # Merge existing mask with new regions (DISABLED - keeping temp files)
    my $temp_merged = "$config{tmp_dir}/temp_merged_mask_$$.bed";
    run_cmd("cat $mask_bed $new_regions_bed | bedtools sort | bedtools merge > $temp_merged");
    # run_cmd("mv $temp_merged $mask_bed");  # DISABLED - keeping temp files
    $logger->info("Would have moved $temp_merged to $mask_bed, but keeping both files");
}

# Function removed - no longer needed since we don't use overlapping regions

sub count_bed_regions {
    my ($bed_file) = @_;
    return 0 unless -s $bed_file;
    
    my $count = `wc -l < $bed_file`;
    chomp($count);
    return $count;
}

sub validate_bed_file {
    my ($bed_file) = @_;
    
    return 0 unless -s $bed_file;
    
    my $valid_lines = 0;
    my $invalid_lines = 0;
    my $temp_validated = "$bed_file.validated";
    
    open(my $in_fh, '<', $bed_file) or die "Cannot open BED file $bed_file: $!";
    open(my $out_fh, '>', $temp_validated) or die "Cannot create validated BED file: $!";
    
    while (my $line = <$in_fh>) {
        chomp $line;
        next if $line =~ /^\s*$/;  # Skip empty lines
        
        my @fields = split(/\t/, $line);
        if (@fields >= 3) {
            my ($chr, $start, $end) = @fields[0..2];
            
            # Validate coordinates
            if ($start =~ /^\d+$/ && $end =~ /^\d+$/ && $start >= 0 && $end > $start) {
                print $out_fh "$chr\t$start\t$end\n";
                $valid_lines++;
            } else {
                $invalid_lines++;
                $logger->warn("Invalid BED line: $line (start=$start, end=$end)");
            }
        } else {
            $invalid_lines++;
            $logger->warn("Malformed BED line: $line");
        }
    }
    
    close($in_fh);
    close($out_fh);
    
    if ($invalid_lines > 0) {
        $logger->warn("Found $invalid_lines invalid lines in BED file $bed_file, fixed to $temp_validated");
        # rename($temp_validated, $bed_file) or die "Cannot replace BED file: $!";  # DISABLED - keeping temp files
        $logger->info("Would have replaced BED file with validated version, but keeping both files");
    } else {
        # unlink($temp_validated);  # DISABLED - keeping temp files
    }
    
    return $valid_lines;
}

sub extract_sequences_from_bed {
    my ($bed_file, $output_fasta) = @_;
    
    
    # Check if BED file exists
    unless (-s $bed_file) {
        $logger->error("BED file does not exist or is empty: $bed_file");
        return 0;
    }
    
    # Check if genome file exists and is accessible
    unless (-s $config{genome_file}) {
        $logger->error("Genome file does not exist or is empty: $config{genome_file}");
        return 0;
    }
    
    # Check if genome index exists
    unless (-f "$config{genome_file}.fai") {
        $logger->warn("Genome index missing, creating: $config{genome_file}.fai");
        my $index_cmd = "samtools faidx '$config{genome_file}'";
        eval {
            run_cmd($index_cmd);
        };
        if ($@) {
            $logger->error("Failed to create genome index: $@");
            return 0;
        }
    }
    
    # Test genome file accessibility by reading the first sequence header
    my $first_header = `head -1 '$config{genome_file}'`;
    chomp($first_header);
    unless ($first_header =~ /^>/) {
        $logger->error("Genome file does not appear to be in FASTA format: $first_header");
        return 0;
    }
    
    # Validate and clean the BED file first
    my $valid_regions = validate_bed_file($bed_file);
    if ($valid_regions == 0) {
        $logger->error("No valid regions found in BED file: $bed_file");
        return 0;
    }
    
    
    # Use bedtools getfasta to extract sequences
    my $cmd = "bedtools getfasta -fi '$config{genome_file}' -bed '$bed_file' -fo '$output_fasta'";
    
    
    
    # Ensure output file doesn't exist before running bedtools
    unlink($output_fasta) if -f $output_fasta;
    
    # Check if bedtools is available
    my $bedtools_check = `which bedtools 2>/dev/null`;
    if (!$bedtools_check) {
        $logger->error("bedtools command not found in PATH");
        return 0;
    }
    
    eval {
        run_cmd($cmd);
    };
    
    if ($@) {
        $logger->error("bedtools getfasta failed: $@");
        $logger->error("Command that failed: $cmd");
        
        # If there's a corrupted output file, remove it
        if (-f $output_fasta) {
            $logger->warn("Removing potentially corrupted output file: $output_fasta");
            unlink($output_fasta);
        }
        return 0;
    }
    
    # Check if output file was created successfully
    unless (-s $output_fasta) {
        $logger->error("No sequences extracted - output file is empty: $output_fasta");
        return 0;
    }
    
    # Validate that the output file is actually in FASTA format
    my $first_line = `head -1 '$output_fasta'`;
    chomp($first_line);
    unless ($first_line =~ /^>/) {
        $logger->error("Output file is not in FASTA format. First line: $first_line");
        $logger->error("This suggests bedtools getfasta failed silently. Attempting alternative extraction...");
        
        # Try alternative extraction method
        my $alt_success = extract_sequences_alternative($bed_file, $output_fasta);
        return $alt_success;
    }
    
    
    return 1;  # Success
}

sub extract_sequences_alternative {
    my ($bed_file, $output_fasta) = @_;
    
    
    # Remove potentially corrupted output file
    unlink($output_fasta) if -f $output_fasta;
    
    # Use samtools faidx for extraction
    open(my $bed_fh, '<', $bed_file) or die "Cannot open BED file: $!";
    open(my $out_fh, '>', $output_fasta) or die "Cannot create output FASTA: $!";
    
    my $seq_count = 0;
    while (my $line = <$bed_fh>) {
        chomp $line;
        next if $line =~ /^\s*$/;
        
        my ($chr, $start, $end) = split(/\t/, $line);
        next unless defined $chr && defined $start && defined $end;
        
        # Convert to 1-based coordinates for samtools
        my $region = "${chr}:" . ($start + 1) . "-${end}";
        
        # Extract sequence using samtools
        my $faidx_cmd = "samtools faidx '$config{genome_file}' '$region'";
        my $seq_output = `$faidx_cmd 2>/dev/null`;
        
        if ($seq_output && $seq_output =~ /^>/) {
            # Rename header to include region info
            $seq_output =~ s/^>[^\n]+/>${chr}_${start}_${end}/;
            print $out_fh $seq_output;
            $seq_count++;
        } else {
            $logger->warn("Failed to extract sequence for region: $region");
        }
    }
    
    close($bed_fh);
    close($out_fh);
    
    return ($seq_count > 0) ? 1 : 0;
}

sub apply_soft_masking_to_samples {
    my (@sample_files) = @_;
    
    $logger->info("Applying masking to " . scalar(@sample_files) . " samples");
    
    # Calculate optimal splitting strategy
    my ($dust_threads_per_sample, $total_dust_jobs) = calculate_dust_parallelization(scalar(@sample_files));
    
    
    my @all_masked_files;
    
    # Use parallel processing for samples
    my $pm = Parallel::ForkManager->new(scalar(@sample_files));
    
    # Collect results from parallel processes
    $pm->run_on_finish(sub {
        my ($pid, $exit_code, $ident, $exit_signal, $core_dump, $data_structure_reference) = @_;
        if ($data_structure_reference && $data_structure_reference->{masked_file}) {
            push @all_masked_files, $data_structure_reference->{masked_file};
        }
    });
    
    # Process each sample in parallel
    for my $sample_file (@sample_files) {
        $pm->start and next;
        
        # Child process
        # Validate sample file format before processing
        unless (validate_fasta_format($sample_file)) {
            $logger->error("Skipping invalid sample file: $sample_file");
            $pm->finish(1);
        }
        
        my $masked_file = apply_soft_masking_with_splitting($sample_file, $dust_threads_per_sample);
        if ($masked_file && -s $masked_file) {
            $pm->finish(0, { masked_file => $masked_file });
        } else {
            $logger->error("Masking failed for sample: $sample_file");
            $pm->finish(1);
        }
    }
    
    $pm->wait_all_children;
    
    $logger->info("Masking completed");
    
    # Create checkpoint after successful masking (checkpoint name is fixed)
    if (@all_masked_files == @sample_files) {
        create_checkpoint("dust_trf_masking");
        $logger->info("Dust and TRF masking checkpoint created successfully");
    } else {
        $logger->warn("Not all samples were masked successfully, skipping checkpoint creation");
    }
    
    return @all_masked_files;
}

sub calculate_dust_parallelization {
    my ($num_samples) = @_;
    
    my $total_threads = $config{threads};
    
    # Strategy: maximize dust parallelization while keeping reasonable sample processing
    if ($num_samples == 1) {
        # Single sample: use all threads for dust splitting
        return ($total_threads, $total_threads);
    } elsif ($num_samples <= 4) {
        # Few samples: allocate threads evenly, minimum 2 per sample
        my $threads_per_sample = max(2, int($total_threads / $num_samples));
        return ($threads_per_sample, $num_samples * $threads_per_sample);
    } else {
        # Many samples: limit to reasonable split sizes
        my $threads_per_sample = max(2, min(8, int($total_threads / 4)));
        return ($threads_per_sample, $num_samples * $threads_per_sample);
    }
}

sub apply_soft_masking_with_splitting {
    my ($sample_file, $dust_threads) = @_;
    
    # Check if input file exists
    unless (-s $sample_file) {
        $logger->error("Sample file does not exist or is empty: $sample_file");
        return undef;
    }
    
    my $sample_basename = basename($sample_file, '.fa');
    my $working_dir = dirname($sample_file);
    
    
    # Step 1: Apply dust masking directly to the whole sample
    my $dust_masked = "$working_dir/${sample_basename}_dust.fa";
    my $dust_success = apply_dust_masking($sample_file, $dust_masked);
    
    unless ($dust_success && -s $dust_masked) {
        $logger->error("Dust masking failed for: $sample_basename");
        return undef;
    }
    
    # Step 2: Apply TRF masking with parallel processing
    my $final_masked = "$working_dir/${sample_basename}_masked.fa";
    my $trf_success = apply_trf_masking_parallel_threaded($dust_masked, $final_masked, $dust_threads);
    
    unless ($trf_success && -s $final_masked) {
        $logger->error("TRF masking failed for: $sample_basename");
        return undef;
    }
    
    # Clean up temporary files after TRF completion
    cleanup_trf_files($working_dir);
    
    # Clean up the intermediate dust file
    if (-f $dust_masked) {
        unlink($dust_masked);
        $logger->debug("Removed intermediate dust file: $dust_masked");
    }
    
    # Log masking statistics
    my $original_size = -s $sample_file;
    my $masked_size = -s $final_masked;
    $logger->info("Sample $sample_basename masked: " . 
                 sprintf("%.1f MB -> %.1f MB", $original_size/1e6, $masked_size/1e6));
    
    return $final_masked;
}

sub split_sample_for_dust {
    my ($sample_file, $num_splits) = @_;
    
    return ($sample_file) if $num_splits <= 1;
    
    # Check if input file exists and has content
    unless (-s $sample_file) {
        $logger->error("Cannot split sample file - file is empty or missing: $sample_file");
        return ();
    }
    
    my $sample_basename = basename($sample_file, '.fa');
    my $working_dir = dirname($sample_file);
    
    $logger->info("Splitting sample $sample_basename into $num_splits parts for parallel dust processing");
    
    # Count sequences first
    my $seq_count = `grep -c '^>' $sample_file`;
    chomp($seq_count);
    
    if ($seq_count <= $num_splits) {
        $logger->info("Sample has only $seq_count sequences, using sequence-based splitting");
        return split_by_sequences($sample_file, $num_splits);
    } else {
        $logger->info("Using size-based splitting for $seq_count sequences");
        return split_by_size($sample_file, $num_splits);
    }
}

sub split_by_sequences {
    my ($sample_file, $num_splits) = @_;
    
    return ($sample_file) unless -s $sample_file;
    
    my $sample_basename = basename($sample_file, '.fa');
    my $working_dir = dirname($sample_file);
    
    $logger->info("Splitting $sample_basename by sequences ($num_splits parts)");
    
    # Use seqkit to split by sequence count
    my $cmd = "seqkit split --quiet -p $num_splits '$sample_file' -O '$working_dir'";
    
    eval {
        run_cmd($cmd);
    };
    
    if ($@) {
        $logger->warn("seqkit sequence splitting failed, using fallback");
        return split_by_simple_method($sample_file, $num_splits);
    }
    
    # Find the created split files
    my @split_files = glob("$working_dir/${sample_basename}.fa.split/*.fa");
    
    if (!@split_files) {
        $logger->warn("No split files found, using fallback");
        return split_by_simple_method($sample_file, $num_splits);
    }
    
    # Rename files to a predictable pattern (DISABLED - keeping original names)
    my @renamed_files;
    for my $i (0..$#split_files) {
        my $new_name = "$working_dir/${sample_basename}_split_$i.fa";
        # run_cmd("mv '$split_files[$i]' '$new_name'");  # DISABLED - keeping temp files
        $logger->info("Would have renamed $split_files[$i] to $new_name, but keeping original name");
        push @renamed_files, $split_files[$i];  # Use original filename
    }
    
    # Clean up seqkit split directory safely
    my $split_dir = "$working_dir/${sample_basename}.fa.split";
    if (-d $split_dir) {
        $logger->info("Cleaning up seqkit split directory: $split_dir");
        run_cmd("rm -rf '$split_dir'");
    }
    
    $logger->info("Split into " . scalar(@renamed_files) . " sequence-based parts");
    return @renamed_files;
}

sub split_by_size {
    my ($sample_file, $num_splits) = @_;
    
    return ($sample_file) unless -s $sample_file;
    
    my $sample_basename = basename($sample_file, '.fa');
    my $working_dir = dirname($sample_file);
    
    # Get file size and calculate split size
    my $total_size = -s $sample_file;
    my $target_size = int($total_size / $num_splits);
    
    return ($sample_file) if $target_size <= 0;
    
    $logger->info("Splitting $sample_basename by size ($num_splits parts, ${target_size}B each)");
    
    # Use seqkit to split by size
    my $cmd = "seqkit split --quiet -s $target_size '$sample_file' -O '$working_dir'";
    
    eval {
        run_cmd($cmd);
    };
    
    if ($@) {
        $logger->warn("seqkit size splitting failed, using fallback");
        return split_by_simple_method($sample_file, $num_splits);
    }
    
    # Find the created split files
    my @split_files = glob("$working_dir/${sample_basename}.fa.split/*.fa");
    
    if (!@split_files) {
        $logger->warn("No split files found, using fallback");
        return split_by_simple_method($sample_file, $num_splits);
    }
    
    # Rename files to a predictable pattern (DISABLED - keeping original names)
    my @renamed_files;
    for my $i (0..$#split_files) {
        my $new_name = "$working_dir/${sample_basename}_split_$i.fa";
        # run_cmd("mv '$split_files[$i]' '$new_name'");  # DISABLED - keeping temp files
        $logger->info("Would have renamed $split_files[$i] to $new_name, but keeping original name");
        push @renamed_files, $split_files[$i];  # Use original filename
    }
    
    # Clean up seqkit split directory safely
    my $split_dir = "$working_dir/${sample_basename}.fa.split";
    if (-d $split_dir) {
        $logger->info("Cleaning up seqkit split directory: $split_dir");
        run_cmd("rm -rf '$split_dir'");
    }
    
    $logger->info("Split into " . scalar(@renamed_files) . " size-based parts");
    return @renamed_files;
}

sub split_by_simple_method {
    my ($sample_file, $num_splits) = @_;
    
    $logger->info("Using simple FASTA-aware splitting fallback method");
    
    my $sample_basename = basename($sample_file, '.fa');
    my $working_dir = dirname($sample_file);
    
    # Count sequences in the file
    my $seq_count = `grep -c '^>' '$sample_file'`;
    chomp($seq_count);
    
    # Check if file exists and has content
    unless (-s $sample_file && $seq_count && $seq_count =~ /^\d+$/ && $seq_count > 0) {
        $logger->error("Sample file is empty or missing sequences: $sample_file (sequences: '$seq_count')");
        return ();
    }
    
    # If too few sequences for splitting, return original file
    if ($seq_count < $num_splits) {
        $logger->info("File has only $seq_count sequences, cannot split into $num_splits parts");
        return ($sample_file);
    }
    
    my $seqs_per_split = int($seq_count / $num_splits) + 1;
    my @split_files;
    
    # Manual FASTA-aware splitting
    $logger->info("Splitting $seq_count sequences into $num_splits parts (~$seqs_per_split sequences each)");
    
    open(my $in_fh, '<', $sample_file) or die "Cannot open $sample_file: $!";
    
    my $current_split = 0;
    my $current_seq_count = 0;
    my $out_fh;
    my $current_file;
    
    while (my $line = <$in_fh>) {
        # Start new split file if needed
        if ($line =~ /^>/) {
            if ($current_seq_count >= $seqs_per_split || !$out_fh) {
                # Close previous file
                close($out_fh) if $out_fh;
                
                # Open new split file
                $current_file = "$working_dir/${sample_basename}_part_" . sprintf("%03d", $current_split) . ".fa";
                open($out_fh, '>', $current_file) or die "Cannot create split file $current_file: $!";
                push @split_files, $current_file;
                
                $current_split++;
                $current_seq_count = 0;
                
                last if $current_split >= $num_splits && scalar(@split_files) >= $num_splits;
            }
            $current_seq_count++;
        }
        
        # Write line to current split file
        print $out_fh $line if $out_fh;
    }
    
    close($out_fh) if $out_fh;
    close($in_fh);
    
    $logger->info("Simple FASTA-aware split created " . scalar(@split_files) . " parts");
    return @split_files;
}

sub apply_dust_masking_parallel {
    my ($split_files_ref) = @_;
    my @split_files = @$split_files_ref;
    
    $logger->info("Applying dust masking to " . scalar(@split_files) . " split parts in parallel");
    
    # Use parallel processing for dust masking
    my $pm = Parallel::ForkManager->new(scalar(@split_files));
    my @dust_masked_files;
    
    # Collect results from parallel processes
    $pm->run_on_finish(sub {
        my ($pid, $exit_code, $ident, $exit_signal, $core_dump, $data) = @_;
        if ($data && $data->{masked_file} && -s $data->{masked_file}) {
            push @dust_masked_files, $data->{masked_file};
        }
    });
    
    for my $split_file (@split_files) {
        $pm->start and next;
        
        my $split_basename = basename($split_file, '.fa');
        my $working_dir = dirname($split_file);
        my $dust_masked = "$working_dir/${split_basename}_dust.fa";
        
        apply_dust_masking($split_file, $dust_masked);
        
        $pm->finish(0, { masked_file => $dust_masked });
    }
    
    $pm->wait_all_children;
    
    $logger->info("Dust masking completed for all split parts");
    return sort @dust_masked_files;  # Sort to maintain order
}

sub apply_trf_masking_parallel {
    my ($dust_files_ref) = @_;
    my @dust_files = @$dust_files_ref;
    
    $logger->info("Applying TRF masking to " . scalar(@dust_files) . " dust-masked parts in parallel");
    
    # Use parallel processing for TRF masking
    my $pm = Parallel::ForkManager->new(scalar(@dust_files));
    my @trf_masked_files;
    
    # Collect results from parallel processes
    $pm->run_on_finish(sub {
        my ($pid, $exit_code, $ident, $exit_signal, $core_dump, $data) = @_;
        if ($data && $data->{masked_file} && -s $data->{masked_file}) {
            push @trf_masked_files, $data->{masked_file};
        }
    });
    
    for my $dust_file (@dust_files) {
        $pm->start and next;
        
        my $dust_basename = basename($dust_file, '.fa');
        # Remove _dust suffix if present
        $dust_basename =~ s/_dust$//;
        my $working_dir = dirname($dust_file);
        my $trf_masked = "$working_dir/${dust_basename}_trf.fa";
        
        apply_trf_masking($dust_file, $trf_masked);
        
        $pm->finish(0, { masked_file => $trf_masked });
    }
    
    $pm->wait_all_children;
    
    $logger->info("TRF masking completed for all dust-masked parts");
    
    # Clean up all intermediate part files after TRF masking
    my $cleanup_cmd = "rm -f $config{tmp_dir}/*part* 2>/dev/null";
    system($cleanup_cmd);
    $logger->info("Cleaned up intermediate part files");
    
    return sort @trf_masked_files;  # Sort to maintain order
}

sub merge_fasta_files {
    my ($files_ref, $output_file) = @_;
    my @files = @$files_ref;
    
    $logger->info("Merging " . scalar(@files) . " dust-masked parts");
    
    # Simple concatenation
    my $cmd = "cat " . join(" ", map { "'$_'" } @files) . " > '$output_file'";
    run_cmd($cmd);
    
    my $merged_size = -s $output_file;
    $logger->info("Merged file size: " . sprintf("%.1f MB", $merged_size/1e6));
}

sub apply_dust_masking {
    my ($input_file, $output_file) = @_;
    
    $logger->info("Applying dust hard masking to: " . basename($input_file));
    
    # Validate input file format first
    unless (validate_fasta_format($input_file)) {
        $logger->error("Input file is not in valid FASTA format: $input_file");
        return 0;
    }
    
    # Use simple dustmasker approach
    
    my $simple_cmd = "dustmasker -in '$input_file' -outfmt fasta -hard_masking > '$output_file'";
    
    eval {
        run_cmd($simple_cmd);
    };
    
    if ($@ || !-s $output_file || !validate_fasta_format($output_file)) {
        $logger->error("dustmasker failed or produced invalid output: $@");
        return 0;
    }
    
    
    # Convert soft masking to hard masking (N characters) if dustmasker used soft masking
    my $conversion_success = convert_soft_to_hard_masking($output_file);
    unless ($conversion_success) {
        $logger->error("Hard masking conversion failed for: $output_file");
        return 0;
    }
    
    return 1;
}

sub validate_fasta_format {
    my ($file) = @_;
    
    unless (-s $file) {
        $logger->error("File does not exist or is empty: $file");
        return 0;
    }
    
    open(my $fh, '<', $file) or do {
        $logger->error("Cannot open file for validation: $file ($!)");
        return 0;
    };
    
    my $first_line = <$fh>;
    close($fh);
    
    unless ($first_line) {
        $logger->error("File is empty: $file");
        return 0;
    }
    
    chomp($first_line);
    unless ($first_line =~ /^>/) {
        $logger->error("File is not in FASTA format - first line: '$first_line' (file: $file)");
        
        # Log first few lines for debugging
        open(my $debug_fh, '<', $file) or return 0;
        my @debug_lines;
        for my $i (0..4) {
            my $line = <$debug_fh>;
            last unless defined $line;
            chomp($line);
            push @debug_lines, $line;
        }
        close($debug_fh);
        
        $logger->error("First 5 lines of invalid file:");
        for my $i (0..$#debug_lines) {
            $logger->error("  Line " . ($i+1) . ": '$debug_lines[$i]'");
        }
        
        return 0;
    }
    
    return 1;
}

sub convert_soft_to_hard_masking {
    my ($file) = @_;
    
    
    # Validate input before conversion
    unless (validate_fasta_format($file)) {
        $logger->error("Input file for hard masking conversion is not valid FASTA: $file");
        return 0;
    }
    
    # Create a temporary file for the conversion
    my $temp_file = "$file.hard_mask_tmp";
    
    open(my $in_fh, '<', $file) or die "Cannot open input file $file: $!";
    open(my $out_fh, '>', $temp_file) or die "Cannot create temp file $temp_file: $!";
    
    my $converted_bases = 0;
    while (my $line = <$in_fh>) {
        if ($line =~ /^>/) {
            # Header line - copy as is
            print $out_fh $line;
        } else {
            # Sequence line - convert lowercase letters to N
            chomp $line;
            my $original_line = $line;
            $line =~ s/[actgn]/N/g;  # Convert lowercase nucleotides to N
            $converted_bases += ($original_line =~ tr/actgn//);
            print $out_fh "$line\n";
        }
    }
    
    close($in_fh);
    close($out_fh);
    
    # Replace original file with hard-masked version
    run_cmd("mv $temp_file $file");
    
    # Validate the final result
    unless (validate_fasta_format($file)) {
        $logger->error("Hard masking conversion corrupted the file: $file");
        return 0;
    }
    
    
    return 1;
}

sub apply_trf_masking {
    my ($input_file, $output_file) = @_;
    
    
    # Validate input file format first
    unless (validate_fasta_format($input_file)) {
        $logger->error("Input file is not in valid FASTA format for TRF: $input_file");
        return 0;
    }
    
    my $working_dir = dirname($input_file);
    my $input_basename = basename($input_file, '.fa');
    
    # TRF parameters: match=2, mismatch=7, delta=7, PM=80, PI=10, minscore=50, maxperiod=500
    # -h option enables hard masking (N characters instead of soft masking)
    my $cmd = "trf $input_file 2 7 7 80 10 50 500 -d -m -h";
    
    my $original_dir = getcwd();
    eval {
        # Change to working directory for TRF (it creates output files in current dir)
        chdir($working_dir) or die "Cannot change to $working_dir: $!";
        
        # Run TRF - capture output but don't fail on non-zero exit code
        my $trf_output = `$cmd 2>&1`;
        my $trf_exit_code = $? >> 8;
        
        
        # Wait a moment for TRF to finish writing files
        sleep(2);
        
        # TRF creates a .mask file - try different possible names
        my $base_without_ext = basename($input_file);
        my @possible_mask_files = (
            "$input_basename.fa.2.7.7.80.10.50.500.mask",
            "${input_basename}_dust.fa.2.7.7.80.10.50.500.mask",
            basename($input_file) . ".2.7.7.80.10.50.500.mask",
            "$base_without_ext.2.7.7.80.10.50.500.mask"
        );
        
        # Also check for any TRF output files in the directory
        my @trf_mask_files = glob("*.2.7.7.80.10.50.500.mask");
        push @possible_mask_files, @trf_mask_files;
        
        my $found_mask = 0;
        for my $trf_mask_file (@possible_mask_files) {
            if (-s $trf_mask_file) {
                # Move the masked file to desired output location
                run_cmd("mv $trf_mask_file $output_file");
                $found_mask = 1;
                last;
            }
        }
        
        if ($found_mask) {
            # Convert soft masking to hard masking if TRF still used soft masking
            convert_soft_to_hard_masking($output_file);
        } else {
            $logger->warn("TRF mask file not found, copying input file");
            run_cmd("cp $input_file $output_file");
        }
    };
    
    # Always return to original directory, even if there was an error
    chdir($original_dir) or $logger->error("Cannot return to original directory: $!");
    
    if ($@ || !-s $output_file) {
        $logger->warn("TRF masking failed: $@");
        $logger->info("Copying input file without TRF masking");
        run_cmd("cp $input_file $output_file");
    }
    
    return 1;
}

sub apply_trf_masking_parallel_threaded {
    my ($input_file, $output_file, $num_threads) = @_;
    
    # Use thread count from dust_threads parameter
    $num_threads = max(1, $num_threads // 4);
    
    my $input_basename = basename($input_file, '.fa');
    my $working_dir = dirname($input_file);
    
    $logger->info("Applying TRF masking with $num_threads parallel threads");
    
    # Step 1: Split the file into parts based on thread count (without breaking sequences)
    my @split_files = split_fasta_for_trf($input_file, $num_threads);
    
    unless (@split_files) {
        $logger->warn("Failed to split file for parallel TRF, falling back to sequential processing");
        return apply_trf_masking($input_file, $output_file);
    }
    
    $logger->info("Split file into " . scalar(@split_files) . " parts for parallel TRF processing");
    
    # Step 2: Run TRF on each part in parallel
    my @trf_masked_files = run_parallel_trf(@split_files);
    
    unless (@trf_masked_files) {
        $logger->error("Parallel TRF processing failed");
        # Clean up split files
        unlink @split_files;
        return 0;
    }
    
    # Step 3: Merge the TRF-masked results
    my $merge_success = merge_trf_results(\@trf_masked_files, $output_file);
    
    # Step 4: Clean up temporary files
    unlink @split_files;
    unlink @trf_masked_files;
    
    return $merge_success;
}

sub split_fasta_for_trf {
    my ($input_file, $num_threads) = @_;
    
    # Don't split if only one thread or file is small
    return ($input_file) if $num_threads <= 1;
    
    my $input_basename = basename($input_file, '.fa');
    my $working_dir = dirname($input_file);
    
    # Count sequences first
    my $seq_count = `grep -c '^>' '$input_file'`;
    chomp $seq_count;
    
    # If fewer sequences than threads, reduce the number of splits
    my $actual_splits = min($num_threads, $seq_count);
    return ($input_file) if $actual_splits <= 1;
    
    $logger->info("Splitting $seq_count sequences into $actual_splits parts for TRF processing");
    
    my @split_files;
    my $seqs_per_split = int($seq_count / $actual_splits);
    $seqs_per_split = 1 if $seqs_per_split < 1;
    
    open(my $input_fh, '<', $input_file) or die "Cannot open $input_file: $!";
    
    my $current_split = 1;
    my $current_seq_count = 0;
    my $current_file = "$working_dir/${input_basename}_trf_part${current_split}.fa";
    my $out_fh;
    
    open($out_fh, '>', $current_file) or die "Cannot create $current_file: $!";
    push @split_files, $current_file;
    
    while (my $line = <$input_fh>) {
        if ($line =~ /^>/) {
            # Start of new sequence
            
            # Check if we need to start a new split file
            if ($current_seq_count >= $seqs_per_split && $current_split < $actual_splits) {
                close($out_fh);
                $current_split++;
                $current_seq_count = 0;
                $current_file = "$working_dir/${input_basename}_trf_part${current_split}.fa";
                open($out_fh, '>', $current_file) or die "Cannot create $current_file: $!";
                push @split_files, $current_file;
            }
            
            $current_seq_count++;
        }
        
        # Write line to current split file
        print $out_fh $line;
    }
    
    close($input_fh);
    close($out_fh) if $out_fh;
    
    $logger->info("Created " . scalar(@split_files) . " split files for TRF processing");
    return @split_files;
}

sub run_parallel_trf {
    my @split_files = @_;
    
    my $pm = Parallel::ForkManager->new(scalar(@split_files));
    my @trf_masked_files;
    
    # Collect results from parallel processes
    $pm->run_on_finish(sub {
        my ($pid, $exit_code, $ident, $exit_signal, $core_dump, $data) = @_;
        if ($data && $data->{masked_file} && -s $data->{masked_file}) {
            push @trf_masked_files, $data->{masked_file};
        }
    });
    
    for my $split_file (@split_files) {
        $pm->start and next;
        
        my $split_basename = basename($split_file, '.fa');
        my $working_dir = dirname($split_file);
        my $trf_masked = "$working_dir/${split_basename}_masked.fa";
        
        my $success = apply_trf_masking($split_file, $trf_masked);
        
        if ($success && -s $trf_masked) {
            $pm->finish(0, { masked_file => $trf_masked });
        } else {
            $logger->error("TRF masking failed for split file: $split_file");
            $pm->finish(1);
        }
    }
    
    $pm->wait_all_children;
    
    $logger->info("Parallel TRF processing completed for " . scalar(@trf_masked_files) . " files");
    return sort @trf_masked_files;  # Sort to maintain order
}

sub merge_trf_results {
    my ($trf_files_ref, $output_file) = @_;
    my @trf_files = @$trf_files_ref;
    
    $logger->info("Merging " . scalar(@trf_files) . " TRF-masked files");
    
    open(my $output_fh, '>', $output_file) or die "Cannot create $output_file: $!";
    
    for my $trf_file (@trf_files) {
        unless (-s $trf_file) {
            $logger->warn("TRF file is empty or missing: $trf_file");
            next;
        }
        
        open(my $input_fh, '<', $trf_file) or die "Cannot open $trf_file: $!";
        while (my $line = <$input_fh>) {
            print $output_fh $line;
        }
        close($input_fh);
    }
    
    close($output_fh);
    
    my $output_size = -s $output_file;
    $logger->info("TRF merge completed: " . format_size($output_size));
    
    return $output_size > 0;
}

sub cleanup_trf_files {
    my ($working_dir) = @_;
    
    # Clean up HTML files generated by TRF
    my @html_files = glob("$working_dir/*.html");
    for my $html_file (@html_files) {
        if (-f $html_file) {
            unlink($html_file);
            $logger->debug("Removed HTML file: $html_file");
        }
    }
    if (@html_files) {
        $logger->info("Removed " . scalar(@html_files) . " HTML files");
    }
    
    # Clean up TRF dat files if present
    my @dat_files = glob("$working_dir/*.dat");
    for my $dat_file (@dat_files) {
        if (-f $dat_file) {
            unlink($dat_file);
            $logger->debug("Removed TRF dat file: $dat_file");
        }
    }
    if (@dat_files) {
        $logger->info("Removed " . scalar(@dat_files) . " TRF dat files");
    }
    
    # Clean up TRF mask files
    my @mask_files = glob("$working_dir/*.2.7.7.80.10.50.500.mask");
    for my $mask_file (@mask_files) {
        if (-f $mask_file) {
            unlink($mask_file);
            $logger->debug("Removed TRF mask file: $mask_file");
        }
    }
    if (@mask_files) {
        $logger->info("Removed " . scalar(@mask_files) . " TRF mask files");
    }
    
    # Clean up temporary TRF split files  
    my @trf_split_files = glob("$working_dir/*_trf_part*.fa");
    for my $split_file (@trf_split_files) {
        if (-f $split_file) {
            unlink($split_file);
            $logger->debug("Removed TRF split file: $split_file");
        }
    }
    if (@trf_split_files) {
        $logger->info("Removed " . scalar(@trf_split_files) . " TRF split files");
    }
}

sub format_size {
    my ($bytes) = @_;
    return "0B" unless $bytes;
    
    my @units = qw(B KB MB GB TB);
    my $unit_index = 0;
    
    while ($bytes >= 1024 && $unit_index < @units - 1) {
        $bytes /= 1024;
        $unit_index++;
    }
    
    return sprintf("%.1f%s", $bytes, $units[$unit_index]);
}

sub run_single_repeatscout_job {
    my ($genome, $lmer, $threshold, $job_id, $sample_id) = @_;
    $sample_id = $sample_id // 1;  # Default to sample 1 if not provided
    
    my $job_prefix;
    if (defined $sample_id && $sample_id > 1) {
        $job_prefix = "$config{tmp_dir}/RS_job${job_id}_s${sample_id}_l${lmer}_t${threshold}";
    } else {
        $job_prefix = "$config{tmp_dir}/RS_job${job_id}_l${lmer}_t${threshold}";
    }
    
    if (defined $sample_id && $sample_id > 1) {
        $logger->info("Job $job_id (Sample $sample_id): Running RepeatScout (l-mer=$lmer, threshold=$threshold)");
    } else {
        $logger->info("Job $job_id: Running RepeatScout (l-mer=$lmer, threshold=$threshold)");
    }
    
    # Build frequency table
    my $freq_file = "${job_prefix}.freq";
    $logger->info("RepeatScout Job $job_id using input file: $genome (size: " . (-s $genome) . " bytes)");
    my $freq_cmd = "build_lmer_table -sequence $genome -freq $freq_file -l $lmer";
    run_cmd($freq_cmd);
    
    # Run RepeatScout
    my $raw_output = "${job_prefix}.raw.fa";
    my $rs_cmd = "RepeatScout -sequence $genome -output $raw_output -freq $freq_file -l $lmer";
    run_cmd($rs_cmd);
    
    # Filter immediately with filter-stage-1.prl if available
    my $final_output = filter_single_repeatscout_result($raw_output, $job_prefix, $job_id, $sample_id);
    
    # Clean up intermediate files (DISABLED - keeping temp files)
    # unlink($freq_file, $raw_output);
    
    if (defined $sample_id && $sample_id > 1) {
        $logger->info("Job $job_id (Sample $sample_id) completed and filtered: $final_output");
    } else {
        $logger->info("Job $job_id completed and filtered: $final_output");
    }
    
    return $final_output;
}

sub merge_repeatscout_results {
    my ($files_ref, $output_file) = @_;
    my @files = @$files_ref;
    
    $logger->info("Merging " . scalar(@files) . " pre-filtered RepeatScout result files");
    
    open(my $out_fh, '>', $output_file) or die "Cannot open $output_file: $!";
    
    my $seq_count = 0;
    for my $i (0..$#files) {
        my $file = $files[$i];
        next unless -s $file;
        
        # Extract job information from filename for better labeling
        my $job_info = "";
        # Updated regex to handle both filtered and unfiltered file patterns
        if ($file =~ /RS_job\d+(_s\d+)?(_l\d+)?(_t\d+)?(_filtered)?(_cleaned)?\.fa$/) {
            my ($sample_part, $lmer_part, $threshold_part) = ($1, $2, $3);
            $sample_part = $sample_part || "";
            $lmer_part = $lmer_part || "";
            $threshold_part = $threshold_part || "";
            $job_info = "${sample_part}${lmer_part}${threshold_part}_set${i}";
        } else {
            $job_info = "_set${i}";
        }
        
        open(my $in_fh, '<', $file) or die "Cannot open $file: $!";
        while (my $line = <$in_fh>) {
            if ($line =~ /^>/) {
                $seq_count++;
                chomp $line;
                $line .= $job_info;  # Add detailed job identifier
                print $out_fh "$line\n";
            } else {
                print $out_fh $line;
            }
        }
        close $in_fh;
    }
    
    close $out_fh;
    
    $logger->info("Merged $seq_count pre-filtered sequences from " . scalar(@files) . " RepeatScout jobs into $output_file");
}

sub filter_single_repeatscout_result {
    my ($raw_output, $job_prefix, $job_id, $sample_id) = @_;
    
    # Check if filter-stage-1.prl exists (try multiple locations)
    my $filter_script;
    my @possible_locations = (
        "$FindBin::RealBin/filter-stage-1.prl",
        "filter-stage-1.prl"  # Let system PATH find it
    );
    
    for my $location (@possible_locations) {
        if ($location eq "filter-stage-1.prl") {
            # Check if it's in PATH
            my $which_result = `which filter-stage-1.prl 2>/dev/null`;
            chomp($which_result);
            if ($which_result && -x $which_result) {
                $filter_script = $location;
                if (defined $sample_id && $sample_id > 1) {
                    $logger->info("Job $job_id (Sample $sample_id): Found filter-stage-1.prl in PATH");
                } else {
                    $logger->info("Job $job_id: Found filter-stage-1.prl in PATH");
                }
                last;
            }
        } elsif (-x $location) {
            $filter_script = $location;
            if (defined $sample_id && $sample_id > 1) {
                $logger->info("Job $job_id (Sample $sample_id): Found filter-stage-1.prl at: $location");
            } else {
                $logger->info("Job $job_id: Found filter-stage-1.prl at: $location");
            }
            last;
        }
    }
    
    unless ($filter_script) {
        if (defined $sample_id && $sample_id > 1) {
            $logger->warn("Job $job_id (Sample $sample_id): filter-stage-1.prl not found, using unfiltered output");
        } else {
            $logger->warn("Job $job_id: filter-stage-1.prl not found, using unfiltered output");
        }
        # Copy raw output as final output if no filter available
        my $final_output = "${job_prefix}.fa";
        run_cmd("cp $raw_output $final_output");
        return $final_output;
    }
    
    # Generate filtered output filename
    my $filtered_file = "${job_prefix}_filtered.fa";
    
    if (defined $sample_id && $sample_id > 1) {
        $logger->info("Job $job_id (Sample $sample_id): Filtering with filter-stage-1.prl");
    } else {
        $logger->info("Job $job_id: Filtering with filter-stage-1.prl");
    }
    
    # Run filter-stage-1.prl
    my $filter_cmd = "$filter_script $raw_output > $filtered_file";
    
    eval {
        run_cmd($filter_cmd);
    };
    
    if ($@ || !-s $filtered_file) {
        if (defined $sample_id && $sample_id > 1) {
            $logger->warn("Job $job_id (Sample $sample_id): Filtering failed, using unfiltered output: $@");
        } else {
            $logger->warn("Job $job_id: Filtering failed, using unfiltered output: $@");
        }
        # Copy raw output as final output if filtering failed
        my $final_output = "${job_prefix}.fa";
        run_cmd("cp $raw_output $final_output");
        return $final_output;
    }
    
    # Clean the filtered file to remove any log contamination
    my $cleaned_file = clean_fasta_file($filtered_file);
    
    my $original_seqs = `grep -c '^>' '$raw_output' 2>/dev/null || echo 0`;
    my $filtered_seqs = `grep -c '^>' '$cleaned_file' 2>/dev/null || echo 0`;
    chomp($original_seqs, $filtered_seqs);
    
    if (defined $sample_id && $sample_id > 1) {
        $logger->info("Job $job_id (Sample $sample_id): Filtered $original_seqs -> $filtered_seqs sequences");
    } else {
        $logger->info("Job $job_id: Filtered $original_seqs -> $filtered_seqs sequences");
    }
    
    return $cleaned_file;
}

sub filter_repeatscout_results {
    my ($result_files_ref) = @_;
    my @result_files = @$result_files_ref;
    
    $logger->info("Filtering " . scalar(@result_files) . " RepeatScout result files with filter-stage-1.prl");
    
    # Check if filter-stage-1.prl exists (try multiple locations)
    my $filter_script;
    my @possible_locations = (
        "$FindBin::RealBin/filter-stage-1.prl",
        "filter-stage-1.prl"  # Let system PATH find it
    );
    
    for my $location (@possible_locations) {
        if ($location eq "filter-stage-1.prl") {
            # Check if it's in PATH
            my $which_result = `which filter-stage-1.prl 2>/dev/null`;
            chomp($which_result);
            if ($which_result && -x $which_result) {
                $filter_script = $location;
                $logger->info("Found filter-stage-1.prl in PATH: $which_result");
                last;
            }
        } elsif (-x $location) {
            $filter_script = $location;
            $logger->info("Found filter-stage-1.prl at: $location");
            last;
        }
    }
    
    unless ($filter_script) {
        $logger->warn("filter-stage-1.prl not found in any location, skipping filtering");
        return @result_files;
    }
    
    my @filtered_files;
    
    for my $result_file (@result_files) {
        next unless -s $result_file;
        
        # Generate filtered output filename
        my $basename = basename($result_file, '.fa');
        my $working_dir = dirname($result_file);
        my $filtered_file = "$working_dir/${basename}_filtered.fa";
        
        $logger->info("Filtering $result_file -> $filtered_file");
        
        # Run filter-stage-1.prl
        my $filter_cmd = "$filter_script $result_file > $filtered_file";
        
        eval {
            run_cmd($filter_cmd);
        };
        
        if ($@ || !-s $filtered_file) {
            $logger->warn("Filtering failed for $result_file: $@");
            $logger->warn("Using unfiltered file instead");
            push @filtered_files, $result_file;
        } else {
            # Clean the filtered file to remove any log contamination
            my $cleaned_file = clean_fasta_file($filtered_file);
            
            my $original_seqs = `grep -c '^>' '$result_file' 2>/dev/null || echo 0`;
            my $filtered_seqs = `grep -c '^>' '$cleaned_file' 2>/dev/null || echo 0`;
            chomp($original_seqs, $filtered_seqs);
            
            $logger->info("Filtered and cleaned $result_file: $original_seqs -> $filtered_seqs sequences");
            push @filtered_files, $cleaned_file;
        }
    }
    
    $logger->info("Filtering completed for all RepeatScout results");
    return @filtered_files;
}

sub refine_consensus {
    $logger->info("Refining consensus sequences with Refiner pipeline...");

    my $input = "$config{tmp_dir}/repeats.fa";
    my $refiner_script = "$FindBin::RealBin/Refiner/main.py";
    my $refiner_output_dir = "$config{work_dir}/refiner_output";

    # Create required directories
    make_path($config{work_dir}) unless -d $config{work_dir};
    make_path($refiner_output_dir) unless -d $refiner_output_dir;
    
    # Ensure all input files exist
    die "Input file missing: $input\n" unless -f $input;
    die "Refiner script missing: $refiner_script\n" unless -x $refiner_script;

    # Determine which genome file to use for Refiner
    my $genome_for_refiner = determine_genome_for_refiner();
    $logger->info("Using genome file for Refiner: $genome_for_refiner");

    # Build command for Refiner/main.py
    my $cmd = join(" ",
        "python3 '$refiner_script'",
        "--repeatscout '$input'",
        "--genome '$genome_for_refiner'", 
        "--output '$refiner_output_dir'",
        "--threads $config{threads}"
    );

    run_cmd($cmd);
    
    # Copy final consensus to expected location
    my $final_consensus = "$refiner_output_dir/consensus_masking.fa";
    my $output = "$config{work_dir}/consensi.fa";
    
    if (-s $final_consensus) {
        run_cmd("cp '$final_consensus' '$output'");
        $logger->info("Final consensus copied to $output");
    } else {
        $logger->warn("Refiner output not found: $final_consensus");
    }
    
    $logger->info("Consensus refinement completed");
}

sub determine_genome_for_refiner {
    # Priority order:
    # 1. Merged samples if multiple exist (up to 2)
    # 2. Single sample if only one exists
    # 3. Full genome masked if available
    # 4. Original genome file as fallback
    
    # Find all available genome samples
    my @sample_files;
    
    # Check for numbered samples (multi-sample case)
    for my $sample_id (1..10) {  # Check up to 10 samples
        my $sample_masked = "$config{tmp_dir}/genome_sample_${sample_id}_masked.fa";
        if (-s $sample_masked) {
            push @sample_files, $sample_masked;
        }
    }
    
    # Check for single sample (single-sample case)
    my $single_sample_masked = "$config{tmp_dir}/genome_sample_masked.fa";
    if (-s $single_sample_masked) {
        push @sample_files, $single_sample_masked;
    }
    
    # Determine what to use based on number of samples found
    if (@sample_files >= 2) {
        # Multiple samples found - merge the first 2
        my $merged_file = "$config{tmp_dir}/genome_merged_for_refiner.fa";
        $logger->info("Found " . scalar(@sample_files) . " genome samples, merging first 2 for Refiner");
        $logger->info("Samples to merge: $sample_files[0], $sample_files[1]");
        
        # Merge the first two samples
        my $merge_cmd = "cat '$sample_files[0]' '$sample_files[1]' > '$merged_file'";
        eval {
            run_cmd($merge_cmd);
        };
        
        if ($@ || !-s $merged_file) {
            $logger->warn("Failed to merge samples, using first sample: $sample_files[0]");
            return $sample_files[0];
        } else {
            my $merged_size = -s $merged_file;
            $logger->info("Successfully merged 2 samples for Refiner: " . sprintf("%.1f MB", $merged_size/1e6));
            return $merged_file;
        }
    } elsif (@sample_files == 1) {
        # Single sample found
        $logger->info("Using single masked sample for Refiner: $sample_files[0]");
        return $sample_files[0];
    }
    
    # Check for full genome masked (small genome case)
    my $full_masked = "$config{tmp_dir}/tmp_masked.fa";
    if (-s $full_masked) {
        $logger->info("Using full masked genome for Refiner (small genome strategy)");
        return $full_masked;
    }
    
    # Fallback to original genome
    $logger->warn("No masked sample found, falling back to original genome for Refiner");
    $logger->warn("This may affect Refiner performance - consider checking sample generation");
    return $config{genome_file};
}

sub cleanup_tmp_directory {
    $logger->info("Performing final cleanup of tmp directory: $config{tmp_dir}");
    
    # Clean up all HTML files in tmp directory
    my @html_files = glob("$config{tmp_dir}/*.html");
    my $html_count = 0;
    for my $html_file (@html_files) {
        if (-f $html_file) {
            unlink($html_file);
            $logger->debug("Removed HTML file: $html_file");
            $html_count++;
        }
    }
    if ($html_count > 0) {
        $logger->info("Removed $html_count HTML files from tmp directory");
    }
    
    # Clean up all part files in tmp directory
    my @part_files = glob("$config{tmp_dir}/*part*.fa");
    my $part_count = 0;
    for my $part_file (@part_files) {
        if (-f $part_file) {
            unlink($part_file);
            $logger->debug("Removed part file: $part_file");
            $part_count++;
        }
    }
    if ($part_count > 0) {
        $logger->info("Removed $part_count part files from tmp directory");
    }
    
    # Clean up filtered files (keep the final ones but remove individual job filtered files)
    my @filtered_files = glob("$config{tmp_dir}/RS_job*_filtered.fa");
    my $filtered_count = 0;
    for my $filtered_file (@filtered_files) {
        if (-f $filtered_file) {
            unlink($filtered_file);
            $logger->debug("Removed filtered file: $filtered_file");
            $filtered_count++;
        }
    }
    if ($filtered_count > 0) {
        $logger->info("Removed $filtered_count filtered intermediate files from tmp directory");
    }
    
    # Clean up all TRF dat files in tmp directory
    my @dat_files = glob("$config{tmp_dir}/*.dat");
    my $dat_count = 0;
    for my $dat_file (@dat_files) {
        if (-f $dat_file) {
            unlink($dat_file);
            $logger->debug("Removed TRF dat file: $dat_file");
            $dat_count++;
        }
    }
    if ($dat_count > 0) {
        $logger->info("Removed $dat_count TRF dat files from tmp directory");
    }
    
    # Clean up TRF intermediate files with specific patterns
    my @trf_intermediates = glob("$config{tmp_dir}/*.2.7.7.80.10.50.500.*");
    my $trf_count = 0;
    my $mask_count = 0;
    for my $trf_file (@trf_intermediates) {
        if (-f $trf_file) {
            # Now also clean up .mask files from TRF/dust processing
            if ($trf_file =~ /\.mask$/) {
                unlink($trf_file);
                $logger->debug("Removed TRF mask file: $trf_file");
                $mask_count++;
            } else {
                unlink($trf_file);
                $logger->debug("Removed TRF intermediate file: $trf_file");
                $trf_count++;
            }
        }
    }
    if ($trf_count > 0) {
        $logger->info("Removed $trf_count TRF intermediate files from tmp directory");
    }
    if ($mask_count > 0) {
        $logger->info("Removed $mask_count TRF mask files from tmp directory");
    }
    
    # Clean up dust-related files
    my @dust_files = glob("$config{tmp_dir}/*_dust.fa");
    my $dust_count = 0;
    for my $dust_file (@dust_files) {
        if (-f $dust_file) {
            unlink($dust_file);
            $logger->debug("Removed dust file: $dust_file");
            $dust_count++;
        }
    }
    if ($dust_count > 0) {
        $logger->info("Removed $dust_count dust intermediate files from tmp directory");
    }
    
    $logger->info("Final cleanup completed. Removed $html_count HTML files, $part_count part files, $dat_count dat files, $trf_count TRF intermediate files, $mask_count mask files, and $dust_count dust files");
}

sub clean_fasta_file {
    my ($input_file) = @_;
    
    my $basename = basename($input_file, '.fa');
    my $working_dir = dirname($input_file);
    my $cleaned_file = "$working_dir/${basename}_cleaned.fa";
    
    $logger->info("Cleaning FASTA file to remove log contamination: $input_file");
    
    open(my $in_fh, '<', $input_file) or die "Cannot open input file $input_file: $!";
    open(my $out_fh, '>', $cleaned_file) or die "Cannot create cleaned file $cleaned_file: $!";
    
    my $in_sequence = 0;
    my $lines_written = 0;
    my $lines_skipped = 0;
    my $headers_extracted = 0;
    
    while (my $line = <$in_fh>) {
        chomp $line;
        
        # Skip empty lines
        next if $line =~ /^\s*$/;
        
        # Check for TRF log patterns - skip these entirely
        if ($line =~ /^(Tandem Repeats Finder|Copyright.*Dr\. Gary Benson|Loading sequence|Allocating Memory|Initializing data structures|Computing TR Model Statistics|Scanning|Freeing Memory|Resolving output)/) {
            $lines_skipped++;
            next;
        }
        
        # Check for lines that contain "Done." followed by sequence ID
        if ($line =~ /^Done\./) {
            $lines_skipped++;
            
            # Extract sequence ID if present
            # Pattern: Done.deleting >R=0: 1 / 1 OR Done.>R=1 (RR=2.  TRF=0.075 DUST=0.220)
            if ($line =~ /Done\.(deleting\s+)?(>R=\d+.*)$/) {
                my $seq_id = $2;
                if ($seq_id) {
                    print $out_fh "$seq_id\n";
                    $lines_written++;
                    $headers_extracted++;
                    $in_sequence = 1;
                    $logger->debug("Extracted sequence ID from contaminated line: $seq_id");
                }
            }
            next;
        }
        
        # Check if this is a proper FASTA header
        if ($line =~ /^>/) {
            print $out_fh "$line\n";
            $lines_written++;
            $in_sequence = 1;
            next;
        }
        
        # Check if this looks like sequence data (DNA/RNA nucleotides)
        if ($line =~ /^[ACGTNRWSMKYHBVDXacgtnrwsmkyhbvdx-]+$/i) {
            if ($in_sequence) {
                print $out_fh "$line\n";
                $lines_written++;
            } else {
                # Orphan sequence without header - skip
                $lines_skipped++;
                $logger->debug("Skipped orphan sequence line without header");
            }
            next;
        }
        
        # Any other line is considered contamination
        $lines_skipped++;
        $logger->debug("Skipped unrecognized line: " . substr($line, 0, 50) . "...");
    }
    
    close($in_fh);
    close($out_fh);
    
    # Check if the cleaned file has content
    unless (-s $cleaned_file) {
        $logger->warn("Cleaned file is empty, using original file");
        run_cmd("cp '$input_file' '$cleaned_file'");
    }
    
    $logger->info("FASTA cleaning completed: wrote $lines_written lines, skipped $lines_skipped lines, extracted $headers_extracted headers from contamination");
    
    return $cleaned_file;
}

sub safe_unlink {
    my @files = @_;
    my $deleted_count = 0;
    
    for my $file (@files) {
        next unless defined $file;
        next unless -f $file;
        
        # Safety check: only delete files in tmp directory or with specific patterns (DISABLED - keeping temp files)
        if ($file =~ m{^$config{tmp_dir}/} || $file =~ /\.(test|temp|tmp|split|dust|part)/) {
            # if (unlink($file)) {  # DISABLED - keeping temp files
            #     $logger->debug("Safely deleted temporary file: $file");
            #     $deleted_count++;
            # } else {
            #     $logger->warn("Failed to delete file: $file ($!)");
            # }
            $logger->debug("Would have safely deleted temporary file: $file");
        } else {
            $logger->warn("Skipping deletion of non-temporary file: $file");
        }
    }
    
    return $deleted_count;
}

sub create_checkpoint {
    my ($checkpoint_name, $checkpoint_file) = @_;
    
    $checkpoint_file = $checkpoint_file || "$config{work_dir}/${checkpoint_name}.ok";
    
    $logger->info("Creating checkpoint: $checkpoint_name");
    
    open(my $fh, '>', $checkpoint_file) or die "Cannot create checkpoint file $checkpoint_file: $!";
    print $fh "Checkpoint created: " . localtime() . "\n";
    print $fh "Process ID: $$\n";
    print $fh "Working directory: " . getcwd() . "\n";
    close($fh);
    
    $logger->info("Checkpoint created successfully: $checkpoint_file");
}

sub check_checkpoint {
    my ($checkpoint_name, $checkpoint_file) = @_;
    
    $checkpoint_file = $checkpoint_file || "$config{work_dir}/${checkpoint_name}.ok";
    
    if (-f $checkpoint_file) {
        $logger->info("Found existing checkpoint: $checkpoint_name");
        return 1;
    }
    
    return 0;
}

sub run_cmd {
    my $cmd = shift;
    $logger->info("Executing command: $cmd");
    
    my $output = `$cmd 2>&1`;
    my $exit_code = $? >> 8;
    
    if ($exit_code != 0) {
        $logger->error("Command failed with exit code: $exit_code");
        $logger->error("Command output:");
        $logger->error($output);
        die "Command execution failed\n";
    }
    
    $logger->debug("Command completed successfully");
    return $output;
}

__END__

=head1 NAME

build_RS - Multi-parameter RepeatScout Pipeline for Pan_TE

=head1 SYNOPSIS

build_RS --genome GENOME.fa --tmp TMP_DIR --workdir WORK_DIR [OPTIONS]

=head1 DESCRIPTION

This script implements a multi-parameter RepeatScout pipeline that runs 
RepeatScout with different l-mer sizes and copy thresholds in parallel,
then merges the results and passes them to the Refiner pipeline for
consensus building.

=head1 REQUIRED ARGUMENTS

  --genome FILE      Input genome in FASTA format
  --tmp DIR          Temporary directory for intermediate files  
  --workdir DIR      Working directory for output files

=head1 OPTIONS

  --threads INT      Number of threads to use (default: 4)
  --force, -f        Force overwrite of existing files
  --help, -h         Show this help message
  --version, -v      Show version information

=head1 EXAMPLES

  # Basic usage
  build_RS --genome genome.fa --tmp /tmp/build_rs --workdir output

  # With custom parameters
  build_RS --genome genome.fa --tmp /tmp --workdir output --threads 16 --force

=head1 AUTHOR

Pan_TE Development Team

=head1 VERSION

2.0.0

=cut
