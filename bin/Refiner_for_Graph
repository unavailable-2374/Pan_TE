#!/usr/bin/env python3
import os
import sys
import time
import random
import subprocess
import shutil
import tempfile
import argparse
import json
import logging
from collections import defaultdict, Counter
from io import StringIO
import re
from dataclasses import dataclass
from typing import List, Dict, Tuple, Set, Optional

import numpy as np
from Bio import SeqIO, AlignIO, Phylo
from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord
from Bio.Align import MultipleSeqAlignment
from scipy.cluster.hierarchy import linkage, fcluster
from scipy.spatial.distance import squareform

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# IUPAC ambiguity codes for DNA
IUPAC_CODES = {
    frozenset(['A']): 'A',
    frozenset(['C']): 'C',
    frozenset(['G']): 'G',
    frozenset(['T']): 'T',
    frozenset(['A', 'G']): 'R',
    frozenset(['C', 'T']): 'Y',
    frozenset(['G', 'C']): 'S',
    frozenset(['A', 'T']): 'W',
    frozenset(['G', 'T']): 'K',
    frozenset(['A', 'C']): 'M',
    frozenset(['C', 'G', 'T']): 'B',
    frozenset(['A', 'G', 'T']): 'D',
    frozenset(['A', 'C', 'T']): 'H',
    frozenset(['A', 'C', 'G']): 'V',
    frozenset(['A', 'C', 'G', 'T']): 'N',
}

# Reverse lookup of IUPAC codes
IUPAC_REVERSE = {
    'A': ['A'],
    'C': ['C'],
    'G': ['G'],
    'T': ['T'],
    'R': ['A', 'G'],
    'Y': ['C', 'T'],
    'S': ['G', 'C'],
    'W': ['A', 'T'],
    'K': ['G', 'T'],
    'M': ['A', 'C'],
    'B': ['C', 'G', 'T'],
    'D': ['A', 'G', 'T'],
    'H': ['A', 'C', 'T'],
    'V': ['A', 'C', 'G'],
    'N': ['A', 'C', 'G', 'T'],
}

@dataclass
class PositionStats:
    """Store statistics for a single position"""
    base_counts: Dict[str, int]  # Count of each base
    entropy: float               # Diversity measure  
    gap_freq: float              # Gap frequency
    consensus: str               # Consensus base
    
    @classmethod
    def from_column(cls, column):
        """Create position statistics from an alignment column"""
        bases = [b.upper() for b in column if b != '-']
        total = len(column)
        base_counts = Counter(bases)
        
        # Calculate gap frequency
        gap_count = column.count('-')
        gap_freq = gap_count / total if total > 0 else 0
        
        # If all gaps, return gap
        if gap_freq == 1.0:
            return cls(base_counts, 0.0, gap_freq, '-')
            
        # Calculate entropy (diversity)
        entropy = 0.0
        if bases:
            probs = [count / len(bases) for count in base_counts.values()]
            for p in probs:
                if p > 0:
                    entropy -= p * np.log2(p)
                    
        # Determine consensus base - using IUPAC representation
        if base_counts:
            # If one base appears with frequency > 70%, use that base
            most_common = base_counts.most_common(1)[0]
            if most_common[1] / len(bases) >= 0.7:
                consensus = most_common[0]
            else:
                # Otherwise, find all bases with frequency >= 20%
                significant_bases = frozenset(
                    base for base, count in base_counts.items() 
                    if count / len(bases) >= 0.2
                )
                if significant_bases:
                    consensus = IUPAC_CODES.get(significant_bases, 'N')
                else:
                    consensus = 'N'
        else:
            consensus = '-'
            
        return cls(base_counts, entropy, gap_freq, consensus)


class RMBlastAlignment:
    def __init__(self, query_id, subject_id, score, query_start, query_end,
                 subject_start, subject_end, alignment, orientation):
        self.query_id = query_id
        self.subject_id = subject_id
        self.score = score
        self.query_start = query_start
        self.query_end = query_end
        self.subject_start = subject_start
        self.subject_end = subject_end
        self.alignment = alignment
        self.orientation = orientation


class EnhancedConsensus:
    """Enhanced consensus sequence with position frequency and variation information"""
    
    def __init__(self, seq_id, consensus_seq, position_stats=None, 
                 entropy=None, conservation_blocks=None, alignment=None):
        self.seq_id = seq_id
        self.consensus_seq = consensus_seq
        self.position_stats = position_stats or []
        self.entropy = entropy or []
        self.conservation_blocks = conservation_blocks or []
        self.alignment = alignment
        
    def to_seqrecord(self):
        """Convert to Bio.SeqRecord object"""
        desc_parts = []
        
        # Add conservation block information
        if self.conservation_blocks:
            block_info = []
            for start, end, cons in self.conservation_blocks:
                block_info.append(f"{start}-{end}:{cons:.2f}")
            desc_parts.append(f"conserved_blocks={','.join(block_info)}")
            
        # Add average entropy
        if self.entropy:
            mean_entropy = sum(self.entropy) / len(self.entropy) if self.entropy else 0
            desc_parts.append(f"mean_entropy={mean_entropy:.4f}")
            
        description = " ".join(desc_parts) if desc_parts else ""
        
        return SeqRecord(
            Seq(self.consensus_seq),
            id=self.seq_id,
            description=description
        )
    
    def to_detailed_seqrecord(self):
        """Convert to detailed SeqRecord with position annotations"""
        record = self.to_seqrecord()
        
        # Add a position annotation line every 20 positions
        annotations = []
        for i in range(0, len(self.position_stats), 20):
            block = self.position_stats[i:i+20]
            annotation_line = []
            for pos in block:
                if pos.entropy > 0.5:  # High variation position
                    annotation_line.append('*')
                else:
                    annotation_line.append(' ')
            annotations.append(''.join(annotation_line))
            
        # Add to record annotations
        record.annotations['site_annotations'] = annotations
        
        return record


class SequenceClusterer:
    def __init__(self, te_builder, distance_threshold=0.7, 
                 max_cluster_size=50, subclustering_threshold=0.4):
        self.te_builder = te_builder
        self.distance_threshold = distance_threshold
        self.max_cluster_size = max_cluster_size
        self.subclustering_threshold = subclustering_threshold

    def calculate_distance_matrix(self, sequences):
        n_seqs = len(sequences)
        unique_id = f"{int(time.time())}_{random.randint(1000, 9999)}"

        if not hasattr(self.te_builder, 'temp_dir'):
            raise ValueError("TEConsensusBuilder temp_dir not initialized")

        temp_name = os.path.join(self.te_builder.temp_dir, f'ref_sequences_{unique_id}.fa')
        with open(temp_name, 'w') as temp_file:
            SeqIO.write(sequences, temp_file, "fasta")

        try:
            alignments = self.te_builder.run_rmblast(temp_name, temp_name)
            seq_lengths = {seq.id: len(seq.seq) for seq in sequences}
            seq_id_to_idx = {seq.id: idx for idx, seq in enumerate(sequences)}

            similarities = np.zeros((n_seqs, n_seqs))
            for aln in alignments:
                if aln.query_id != aln.subject_id:
                    i = seq_id_to_idx[aln.query_id]
                    j = seq_id_to_idx[aln.subject_id]
                    min_len = min(seq_lengths[aln.query_id], seq_lengths[aln.subject_id])
                    norm_score = aln.score / (min_len * 2)
                    similarities[i, j] = max(similarities[i, j], norm_score)
                    similarities[j, i] = similarities[i, j]

            np.fill_diagonal(similarities, 1.0)
            distances = 1.0 - similarities
            distances = np.maximum(distances, 0.0)

            logger.info(f"Distance matrix shape: {distances.shape}")
            logger.info(f"Distance range: [{distances.min()}, {distances.max()}]")

            return distances

        finally:
            try:
                for ext in ['.nin', '.nsq', '.nhr']:
                    db_file = temp_name + ext
                    if os.path.exists(db_file):
                        os.remove(db_file)
            except OSError as e:
                logger.warning(f"Error cleaning up temporary files: {e}")

    def cluster_sequences(self, sequences):
        if len(sequences) == 1:
            return [[sequences[0]]]

        logger.info("Calculating distance matrix...")
        distances = self.calculate_distance_matrix(sequences)
        
        logger.info("Performing hierarchical clustering...")
        try:
            distances = np.maximum(distances, distances.T)
            condensed_distances = squareform(distances)
            linkage_matrix = linkage(condensed_distances, method='average')
            clusters = fcluster(linkage_matrix, t=self.distance_threshold, criterion='distance')

            # Assign sequences to clusters
            cluster_dict = defaultdict(list)
            for seq, cluster_id in zip(sequences, clusters):
                cluster_dict[cluster_id].append(seq)
                
            # Check for large clusters and split
            final_clusters = []
            for cluster_id, cluster_seqs in cluster_dict.items():
                if len(cluster_seqs) > self.max_cluster_size:
                    logger.info(f"Large cluster detected with {len(cluster_seqs)} sequences. Attempting to split.")
                    # Try splitting with stricter threshold
                    subclusters = self.split_large_cluster(cluster_seqs)
                    final_clusters.extend(subclusters)
                else:
                    final_clusters.append(cluster_seqs)

            return final_clusters

        except Exception as e:
            logger.error(f"Clustering error: {str(e)}")
            logger.error(f"Distance matrix stats - min: {distances.min()}, max: {distances.max()}, mean: {distances.mean()}")
            raise
            
    def split_large_cluster(self, sequences):
        """Try to split large clusters using evolutionary tree methods"""
        logger.info(f"Splitting large cluster with {len(sequences)} sequences")
        
        # Method 1: Use stricter hierarchical clustering threshold
        try:
            distances = self.calculate_distance_matrix(sequences)
            condensed_distances = squareform(distances)
            linkage_matrix = linkage(condensed_distances, method='average')
            
            # Try decreasing thresholds until reasonable cluster sizes
            thresholds = [self.subclustering_threshold, 0.3, 0.2, 0.1]
            for threshold in thresholds:
                subclusters = fcluster(linkage_matrix, t=threshold, criterion='distance')
                subcluster_dict = defaultdict(list)
                
                for seq, cluster_id in zip(sequences, subclusters):
                    subcluster_dict[cluster_id].append(seq)
                    
                # Check maximum cluster size
                max_size = max(len(seqs) for seqs in subcluster_dict.values())
                n_clusters = len(subcluster_dict)
                
                logger.info(f"Threshold {threshold}: {n_clusters} clusters, max size {max_size}")
                
                if max_size <= self.max_cluster_size or n_clusters >= 2:
                    logger.info(f"Successfully split into {n_clusters} subclusters")
                    return list(subcluster_dict.values())
        
        except Exception as e:
            logger.warning(f"Failed to split using hierarchical clustering: {e}")
            
        # Method 2: If hierarchical clustering fails, try random sampling
        if len(sequences) > self.max_cluster_size:
            logger.warning(f"Could not split cluster effectively. Using random sampling to create manageable clusters.")
            random.shuffle(sequences)
            return [sequences[i:i+self.max_cluster_size] 
                    for i in range(0, len(sequences), self.max_cluster_size)]
        
        return [sequences]


class TEConsensusBuilder:
    def __init__(self, rmblast_dir, makeblastdb_path, matrix_path,
                 min_score=150, gap_init=20, gap_ext=5, threads=None,
                 entropy_threshold=0.4, min_conserved_block=15,
                 use_iupac=True, keep_variants=True):
        self.rmblast_path = os.path.join(rmblast_dir, "rmblastn")
        self.makeblastdb_path = makeblastdb_path
        self.matrix_path = matrix_path
        self.min_score = min_score
        self.gap_init = gap_init
        self.gap_ext = gap_ext
        self.threads = threads or 1
        self.temp_dir = None
        self.entropy_threshold = entropy_threshold
        self.min_conserved_block = min_conserved_block
        self.use_iupac = use_iupac
        self.keep_variants = keep_variants

        # Check if MAFFT exists and get its path
        self.mafft_path = shutil.which("mafft")
        if not self.mafft_path:
            raise FileNotFoundError("mafft not found in system PATH")
            
        # Check if FastTree can be used
        self.fasttree_path = shutil.which("fasttree")
        if not self.fasttree_path:
            logger.warning("FastTree not found. Phylogenetic methods will be limited.")

    def prepare_blast_db(self, fasta_file):
        cmd = [
            self.makeblastdb_path,
            "-in", fasta_file,
            "-dbtype", "nucl",
            "-parse_seqids"
        ]
        try:
            logger.info(f"Creating BLAST database: {' '.join(cmd)}")
            result = subprocess.run(cmd, check=True, capture_output=True, text=True)
            logger.info("BLAST database creation successful")
            if result.stderr:
                logger.debug(f"makeblastdb stderr: {result.stderr}")
        except subprocess.CalledProcessError as e:
            logger.error(f"Failed to create BLAST database: {e.stderr}")
            raise

    def run_rmblast(self, query_file, subject_file):
        current_dir = os.getcwd()
        query_path = os.path.abspath(query_file)
        subject_path = os.path.abspath(subject_file)
        work_dir = os.path.dirname(subject_path)

        os.chdir(work_dir)
        try:
            self.prepare_blast_db(subject_path)

            if not os.path.exists(query_path):
                logger.error(f"Query file not found: {query_path}")
                raise FileNotFoundError(f"Query file not found: {query_path}")
            if not os.path.exists(subject_path):
                logger.error(f"Subject file not found: {subject_path}")
                raise FileNotFoundError(f"Subject file not found: {subject_path}")

            self.prepare_blast_db(os.path.basename(subject_path))

            db_files = [os.path.basename(subject_path) + ext for ext in ['.nhr', '.nin', '.nsq']]
            for db_file in db_files:
                if not os.path.exists(db_file):
                    raise FileNotFoundError(f"BLAST database file not found: {db_file}")

            cmd = [
                self.rmblast_path,
                "-query", query_path,
                "-db", subject_path,
                "-outfmt", "6 qseqid sseqid score qstart qend sstart send qseq sseq sstrand",
                "-matrix", self.matrix_path,
                "-gapopen", str(self.gap_init),
                "-gapextend", str(self.gap_ext),
                "-dust", "no",
                "-soft_masking", "false",
                "-num_threads", str(self.threads),
                "-complexity_adjust",
                "-evalue", "1e-10",
                "-word_size", "7",
                "-window_size", "40",
                "-xdrop_gap", "50",
                "-xdrop_gap_final", "100"
            ]

            logger.info(f"Running RMBlast command: {' '.join(cmd)}")

            try:
                result = subprocess.run(cmd, check=True, capture_output=True, text=True)
                alignments = []
                for line in result.stdout.split('\n'):
                    if line.strip():
                        fields = line.split('\t')
                        if len(fields) >= 9:
                            alignment = RMBlastAlignment(
                                query_id=fields[0],
                                subject_id=fields[1],
                                score=float(fields[2]),
                                query_start=int(fields[3]),
                                query_end=int(fields[4]),
                                subject_start=int(fields[5]),
                                subject_end=int(fields[6]),
                                alignment=(fields[7], fields[8]),
                                orientation=fields[9] if len(fields) > 9 else 'plus'
                            )
                            if alignment.score >= self.min_score:
                                alignments.append(alignment)

                return alignments

            except subprocess.CalledProcessError as e:
                logger.error(f"RMBlast failed with error: {e.stderr}")
                raise

        finally:
            os.chdir(current_dir)

    def build_multiple_alignment(self, sequences, reference=None, generate_tree=True):
        """
        Use MAFFT to perform global multiple sequence alignment on all sequences in a cluster
        Enhanced version can select a representative sequence as reference and optionally generate a phylogenetic tree
        """
        with tempfile.NamedTemporaryFile(mode="w+", delete=False, suffix=".fa", dir=self.temp_dir) as temp_in:
            # If reference sequence specified, place it first
            if reference is not None:
                SeqIO.write([reference] + [s for s in sequences if s.id != reference.id], temp_in, "fasta")
            else:
                SeqIO.write(sequences, temp_in, "fasta")
            temp_in_name = temp_in.name

        alignment_file = temp_in_name + ".aln"
        tree_file = temp_in_name + ".tree"

        try:
            # Run MAFFT for multiple sequence alignment
            if reference is not None:
                # Use --add option to keep reference sequence fixed
                cmd = [
                    self.mafft_path, 
                    "--retree", "2",
                    "--maxiterate", "1000",
                    "--localpair",
                    "--thread", str(self.threads),
                    temp_in_name
                ]
            else:
                cmd = [self.mafft_path, "--auto", "--thread", str(self.threads), temp_in_name]
                
            logger.info("Running MAFFT for multiple sequence alignment: " + " ".join(cmd))
            result = subprocess.run(cmd, check=True, capture_output=True, text=True)
            
            # Save alignment results
            with open(alignment_file, "w") as f:
                f.write(result.stdout)
                
            # Parse aligned sequences from results
            aligned_output = result.stdout
            alignment = AlignIO.read(StringIO(aligned_output), "fasta")
            
            # Generate phylogenetic tree if requested
            if generate_tree and self.fasttree_path and len(sequences) >= 3:
                try:
                    # Run FastTree to generate phylogenetic tree
                    tree_cmd = [
                        self.fasttree_path,
                        "-nt", # nucleotide sequences
                        "-gtr", # general time reversible model
                        "-gamma", # Gamma distribution model
                    ]
                    
                    logger.info("Generating phylogenetic tree with FastTree")
                    tree_process = subprocess.Popen(
                        tree_cmd,
                        stdin=subprocess.PIPE,
                        stdout=subprocess.PIPE,
                        stderr=subprocess.PIPE,
                        text=True
                    )
                    
                    tree_stdout, tree_stderr = tree_process.communicate(input=aligned_output)
                    
                    if tree_process.returncode == 0:
                        with open(tree_file, "w") as f:
                            f.write(tree_stdout)
                        logger.info(f"Phylogenetic tree saved to {tree_file}")
                    else:
                        logger.warning(f"FastTree error: {tree_stderr}")
                except Exception as e:
                    logger.warning(f"Error generating phylogenetic tree: {e}")
            
            return alignment, alignment_file, tree_file if os.path.exists(tree_file) else None

        except subprocess.CalledProcessError as e:
            logger.error(f"MAFFT failed: {e.stderr}")
            raise

        finally:
            os.remove(temp_in_name)

    def analyze_alignment(self, alignment):
        """Analyze multiple sequence alignment results, calculate position statistics and conserved blocks"""
        if not alignment:
            return [], [], []
            
        # Calculate statistics for each position
        position_stats = []
        for i in range(alignment.get_alignment_length()):
            column = alignment[:, i]
            stats = PositionStats.from_column(column)
            position_stats.append(stats)
            
        # Calculate position entropy
        entropy = [stats.entropy for stats in position_stats]
        
        # Identify conserved blocks
        conservation_blocks = []
        current_block = None
        
        for i, stats in enumerate(position_stats):
            if stats.entropy < self.entropy_threshold and stats.gap_freq < 0.2:
                # Conserved position
                if current_block is None:
                    # Start new block
                    current_block = [i, i, 1.0 - stats.entropy]
                else:
                    # Continue existing block
                    current_block[1] = i
                    current_block[2] = (current_block[2] * (i - current_block[0]) + (1.0 - stats.entropy)) / ((i - current_block[0]) + 1)
            else:
                # Non-conserved position
                if current_block is not None:
                    # If block is long enough, save it
                    if current_block[1] - current_block[0] + 1 >= self.min_conserved_block:
                        conservation_blocks.append(tuple(current_block))
                    current_block = None
                    
        # Check the last block
        if current_block is not None and current_block[1] - current_block[0] + 1 >= self.min_conserved_block:
            conservation_blocks.append(tuple(current_block))
            
        return position_stats, entropy, conservation_blocks

    def build_enhanced_consensus(self, alignment, seq_id, reference_idx=None):
        """Build enhanced consensus sequence with position frequency and variation information"""
        if not alignment:
            return EnhancedConsensus(seq_id, "")
            
        # Analyze alignment
        position_stats, entropy, conservation_blocks = self.analyze_alignment(alignment)
        
        # Build consensus sequence
        consensus_seq = []
        
        for i, stats in enumerate(position_stats):
            if stats.gap_freq > 0.5:  # Majority gaps
                consensus_seq.append("")  # Don't add any character
            else:
                if self.use_iupac:
                    consensus_seq.append(stats.consensus)
                else:
                    if stats.base_counts:
                        most_common = stats.base_counts.most_common(1)[0][0]
                        consensus_seq.append(most_common)
                    else:
                        consensus_seq.append("N")
        
        consensus_seq = "".join(consensus_seq)
        
        # Create enhanced consensus object
        enhanced_consensus = EnhancedConsensus(
            seq_id=seq_id,
            consensus_seq=consensus_seq,
            position_stats=position_stats,
            entropy=entropy,
            conservation_blocks=conservation_blocks,
            alignment=alignment
        )
        
        return enhanced_consensus

    def select_representative_variants(self, sequences, alignment, tree_file=None, max_variants=3):
        """Select representative variants from the cluster"""
        if len(sequences) <= 1:
            return sequences
            
        if len(sequences) <= max_variants:
            return sequences
            
        # Try selecting variants based on phylogenetic tree
        if tree_file and os.path.exists(tree_file):
            try:
                # Use Phylo to read the tree
                tree = Phylo.read(tree_file, "newick")
                
                # Get the deepest branches
                depths = {}
                for clade in tree.find_clades():
                    if clade.name:
                        depths[clade.name] = tree.distance(tree.root, clade)
                
                # Select representatives from different branches
                variants = []
                seq_by_id = {seq.id: seq for seq in sequences}
                
                # Sort by depth and select the first few
                sorted_seqs = sorted(depths.items(), key=lambda x: x[1], reverse=True)
                for name, _ in sorted_seqs[:max_variants]:
                    if name in seq_by_id:
                        variants.append(seq_by_id[name])
                
                if variants:
                    logger.info(f"Selected {len(variants)} phylogenetic variants")
                    return variants
                    
            except Exception as e:
                logger.warning(f"Error selecting variants from phylogenetic tree: {e}")
        
        # Alternative method: Based on sequence length and variation
        try:
            # Calculate features for each sequence
            seq_features = []
            avg_len = sum(len(seq.seq) for seq in sequences) / len(sequences)
            
            for i, seq in enumerate(sequences):
                # Length deviation
                len_dev = abs(len(seq.seq) - avg_len) / avg_len if avg_len > 0 else 0
                
                # Sequence variability
                n_count = str(seq.seq).upper().count('N')
                n_ratio = n_count / len(seq.seq) if len(seq.seq) > 0 else 0
                
                seq_features.append((seq, len_dev, n_ratio, i))
            
            # Select a sequence close to average length
            seq_features.sort(key=lambda x: x[1])  # Sort by length deviation
            variants = [seq_features[0][0]]
            
            # Then select a few most different sequences
            if len(seq_features) > 1:
                # First remove already selected sequences
                remaining = seq_features[1:]
                
                # Select based on N content and length deviation
                remaining.sort(key=lambda x: (-x[2], x[1]))  # Lower N rate and different length
                
                for i in range(min(max_variants-1, len(remaining))):
                    variants.append(remaining[i][0])
            
            logger.info(f"Selected {len(variants)} feature-based variants")
            return variants
            
        except Exception as e:
            logger.warning(f"Error selecting variants based on features: {e}")
            
        # Most conservative method: Random selection
        return random.sample(sequences, min(max_variants, len(sequences)))

    def mash_precluster(self, sequences, mash_threshold=0.1):
        temp_fa = None
        sketch_prefix = None
        try:
            # Create temporary file and write sequences
            with tempfile.NamedTemporaryFile(mode="w+", delete=False, suffix=".fa", dir=self.temp_dir) as temp:
                SeqIO.write(sequences, temp, "fasta")
                temp_fa = temp.name
                logger.info(f"Created temporary FASTA file: {temp_fa}")
                temp.flush()
                os.fsync(temp.fileno())

            # Save mapping of sequence IDs to filenames
            seq_id_map = {str(seq.id): seq for seq in sequences}
            
            sketch_prefix = temp_fa + ".mash"
            
            # Check mash command
            mash_path = shutil.which("mash")
            if not mash_path:
                raise FileNotFoundError("mash command not found in system PATH")
            logger.info(f"Found mash at: {mash_path}")
                
            # Run mash sketch
            sketch_cmd = [mash_path, "sketch", "-k", "16", "-m", "2", "-o", sketch_prefix, temp_fa]
            logger.info("Running mash sketch: " + " ".join(sketch_cmd))
            result = subprocess.run(sketch_cmd, 
                                check=True, 
                                capture_output=True, 
                                text=True)
            if result.stderr:
                logger.info(f"Mash sketch stderr: {result.stderr}")

            # Check mash sketch output file
            msh_file = sketch_prefix + ".msh"
            if not os.path.exists(msh_file):
                raise FileNotFoundError(f"Mash sketch output file not found: {msh_file}")
            logger.info(f"Mash sketch file created: {msh_file}")

            # Run mash dist
            dist_cmd = [mash_path, "dist", msh_file, temp_fa]
            logger.info("Running mash dist: " + " ".join(dist_cmd))
            result = subprocess.run(dist_cmd, 
                                check=True, 
                                capture_output=True, 
                                text=True)
            
            if result.stderr:
                logger.info(f"Mash dist stderr: {result.stderr}")
            
            mash_output = result.stdout
            if not mash_output.strip():
                raise RuntimeError("Mash dist produced no output")

            # Process clustering
            try:
                # Initialize UnionFind with sequence IDs, not file paths
                uf = UnionFind([str(seq.id) for seq in sequences])
                logger.info(f"Initialized UnionFind with {len(sequences)} sequences")
                
                cluster_count = 0
                for line in mash_output.strip().split("\n"):
                    parts = line.split("\t")  # Use tab as separator
                    if len(parts) < 3:
                        continue
                    
                    # Extract sequence ID from filename
                    seq1_path = parts[0]
                    seq2_path = parts[1]
                    seq1_id = os.path.basename(seq1_path)
                    seq2_id = os.path.basename(seq2_path)
                    
                    try:
                        dist = float(parts[2])
                        if dist < mash_threshold and seq1_id in seq_id_map and seq2_id in seq_id_map:
                            uf.union(seq1_id, seq2_id)
                            cluster_count += 1
                    except (ValueError, KeyError) as e:
                        logger.warning(f"Error processing distance for {seq1_id}-{seq2_id}: {e}")
                        continue
                        
                logger.info(f"Created {cluster_count} initial clusters")

                # Build clustering results
                clusters_dict = defaultdict(list)
                for seq in sequences:
                    seq_id = str(seq.id)
                    root_id = uf.find(seq_id)
                    clusters_dict[root_id].append(seq)
                    
                preclusters = list(clusters_dict.values())
                logger.info(f"Formed {len(preclusters)} preclusters")

                # Perform fine-grained clustering
                final_clusters = []
                clusterer = SequenceClusterer(self)
                for i, group in enumerate(preclusters):
                    logger.info(f"Processing precluster {i+1}/{len(preclusters)} with {len(group)} sequences")
                    if len(group) == 1:
                        final_clusters.append(group)
                    else:
                        refined = clusterer.cluster_sequences(group)
                        final_clusters.extend(refined)
                        
                logger.info(f"Created {len(final_clusters)} final clusters")
                return final_clusters

            except Exception as e:
                logger.error(f"Error during clustering: {str(e)}")
                raise

        except Exception as e:
            logger.error(f"Error in mash_precluster: {str(e)}")
            import traceback
            logger.error(f"Full traceback: {traceback.format_exc()}")
            raise

        finally:
            # Clean up temporary files
            try:
                if temp_fa and os.path.exists(temp_fa):
                    os.remove(temp_fa)
                    logger.info(f"Removed temporary file: {temp_fa}")
                if sketch_prefix:
                    msh_file = sketch_prefix + ".msh"
                    if os.path.exists(msh_file):
                        os.remove(msh_file)
                        logger.info(f"Removed mash sketch file: {msh_file}")
            except OSError as e:
                logger.warning(f"Error cleaning up temporary files: {e}")

    def hybrid_cluster(self, sequences):
        with tempfile.NamedTemporaryFile(mode="w+", delete=False, suffix=".fa", dir=self.temp_dir) as temp_in:
            SeqIO.write(sequences, temp_in, "fasta")
            temp_in_name = temp_in.name

        temp_out_name = temp_in_name + ".cdhit"
        cmd = [
            "cd-hit-est",
            "-i", temp_in_name,
            "-o", temp_out_name,
            "-aS", "0.6",
            "-c", "0.8",
            "-g", "0",
            "-G", "0",
            "-A", "80",
            "-M", "10000",
            "-t", str(self.threads)
        ]
        logger.info("Running cd-hit-est: " + " ".join(cmd))
        try:
            subprocess.run(cmd, check=True, capture_output=True, text=True)
            logger.info("cd-hit-est finished successfully.")
        except subprocess.CalledProcessError as e:
            logger.error("cd-hit-est failed: " + e.stderr)
            raise

        preclustered_sequences = list(SeqIO.parse(temp_out_name, "fasta"))

        os.remove(temp_in_name)
        os.remove(temp_out_name)

        clusterer = SequenceClusterer(self)
        refined_clusters = clusterer.cluster_sequences(preclustered_sequences)
        return refined_clusters

    def build_clustered_consensus(self, input_file, output_file, stats_file=None, details_dir=None):
        original_dir = os.getcwd()
        try:
            input_path = os.path.abspath(input_file)
            output_path = os.path.abspath(output_file)
            output_dir = os.path.dirname(output_path)
            logger.info(f"Using base directory for consensus building: {output_dir}")

            timestamp = int(time.time())
            self.temp_dir = os.path.join(output_dir, f'tmp_{timestamp}')
            os.makedirs(self.temp_dir, exist_ok=True)
            os.chdir(self.temp_dir)
            logger.info(f"Created temporary directory: {self.temp_dir}")
            
            # Create details directory
            if details_dir:
                details_path = os.path.abspath(details_dir)
                os.makedirs(details_path, exist_ok=True)
            else:
                details_path = os.path.join(output_dir, "cluster_details")
                os.makedirs(details_path, exist_ok=True)

            logger.info(f"Reading sequences from: {input_path}")
            sequences = list(SeqIO.parse(input_path, "fasta"))
            if not sequences:
                raise ValueError(f"No sequences found in {input_path}")
            logger.info(f"Read {len(sequences)} sequences")

            if any(len(seq.seq) > 40000 for seq in sequences):
                logger.info("Long sequences (>40k) detected; using k-mer (Mash) pre-clustering.")
                clusters = self.mash_precluster(sequences)
            elif os.path.getsize(input_path) > 2 * 1024 * 1024:
                logger.info("Large input file detected; using hybrid clustering strategy with cd-hit-est pre-clustering.")
                clusters = self.hybrid_cluster(sequences)
            else:
                logger.info("Using standard RMBlast-based clustering.")
                clusterer = SequenceClusterer(self)
                clusters = clusterer.cluster_sequences(sequences)

            logger.info(f"Found {len(clusters)} clusters")

            consensus_records = []
            variant_records = []
            cluster_stats = []
            
            for i, cluster in enumerate(clusters, 1):
                cluster_id = f"{os.path.splitext(os.path.basename(input_file))[0]}_cluster_{i}"
                logger.info(f"Processing cluster {i} with {len(cluster)} sequences")
                
                if len(cluster) == 1:
                    # Single sequence cluster
                    consensus_seq = str(cluster[0].seq)
                    consensus_desc = f"single sequence from cluster {i}"
                    consensus_record = SeqRecord(
                        Seq(consensus_seq),
                        id=cluster_id,
                        description=consensus_desc
                    )
                    consensus_records.append(consensus_record)
                    
                    # No variants for single sequences
                    cluster_stats.append({
                        "cluster_id": cluster_id,
                        "size": 1,
                        "mean_length": len(consensus_seq),
                        "is_single": True,
                        "conserved_blocks": 0
                    })
                    
                else:
                    # Multiple sequence cluster
                    # 1. Multiple sequence alignment
                    alignment, aln_file, tree_file = self.build_multiple_alignment(cluster)
                    
                    # Save alignment file for reference
                    cluster_aln_file = os.path.join(details_path, f"{cluster_id}_alignment.fa")
                    shutil.copy(aln_file, cluster_aln_file)
                    
                    # 2. Build enhanced consensus sequence
                    enhanced_consensus = self.build_enhanced_consensus(alignment, cluster_id)
                    
                    # 3. Add to results
                    consensus_records.append(enhanced_consensus.to_seqrecord())
                    
                    # 4. If needed, select representative variants
                    if self.keep_variants:
                        variants = self.select_representative_variants(
                            cluster, alignment, tree_file, max_variants=min(3, len(cluster))
                        )
                        
                        for j, var in enumerate(variants, 1):
                            var_id = f"{cluster_id}_variant_{j}"
                            var_record = SeqRecord(
                                var.seq,
                                id=var_id,
                                description=f"representative variant {j} from cluster {i}"
                            )
                            variant_records.append(var_record)
                    
                    # 5. Collect statistics
                    cluster_stats.append({
                        "cluster_id": cluster_id,
                        "size": len(cluster),
                        "mean_length": len(enhanced_consensus.consensus_seq),
                        "is_single": False,
                        "conserved_blocks": len(enhanced_consensus.conservation_blocks),
                        "mean_entropy": sum(enhanced_consensus.entropy) / len(enhanced_consensus.entropy) if enhanced_consensus.entropy else 0
                    })
            
            # Write results
            all_records = consensus_records + variant_records
            SeqIO.write(all_records, output_path, "fasta")
            logger.info(f"Written {len(all_records)} sequences to {output_path}")
            
            # Write statistics
            stats_output = stats_file or f"{output_path}.stats"
            with open(stats_output, "w") as f:
                f.write(f"Original sequences: {len(sequences)}\n")
                f.write(f"Number of clusters: {len(clusters)}\n\n")
                
                f.write("Cluster statistics:\n")
                for stat in cluster_stats:
                    f.write(f"Cluster {stat['cluster_id']}:\n")
                    f.write(f"  Size: {stat['size']}\n")
                    f.write(f"  Mean length: {stat['mean_length']}\n")
                    
                    if not stat['is_single']:
                        f.write(f"  Conserved blocks: {stat['conserved_blocks']}\n")
                        f.write(f"  Mean entropy: {stat['mean_entropy']:.4f}\n")
                    f.write("\n")
                    
            logger.info(f"Written statistics to {stats_output}")

            return {
                "consensus_file": output_path,
                "stats_file": stats_output,
                "details_dir": details_path,
                "num_clusters": len(clusters),
                "num_sequences": len(all_records)
            }

        except Exception as e:
            logger.error(f"Error in consensus building: {str(e)}")
            import traceback
            logger.error(f"Full traceback: {traceback.format_exc()}")
            raise
            
        finally:
            os.chdir(original_dir)
            if self.temp_dir and os.path.exists(self.temp_dir):
                logger.info("Temporary directory will be cleaned by calling script")


class UnionFind:
    def __init__(self, elements):
        self.parent = {e: e for e in elements}

    def find(self, a):
        if self.parent[a] != a:
            self.parent[a] = self.find(self.parent[a])
        return self.parent[a]

    def union(self, a, b):
        rootA = self.find(a)
        rootB = self.find(b)
        if rootA != rootB:
            self.parent[rootB] = rootA


class Config:
    def __init__(self):
        conda_env_path = self.get_current_conda_env()
        if conda_env_path:
            self.rmblastn = os.path.join(conda_env_path, 'bin', 'rmblastn')
            self.makeblastdb = os.path.join(conda_env_path, 'bin', 'makeblastdb')
            logger.info(f"Using Conda environment: {conda_env_path}")
        else:
            self.rmblastn = shutil.which('rmblastn')
            self.makeblastdb = shutil.which('makeblastdb')
            logger.info("No Conda environment detected, using system PATH")

        if not self.rmblastn or not os.path.exists(self.rmblastn):
            raise FileNotFoundError(f"rmblastn not found at {self.rmblastn}")
        if not self.makeblastdb or not os.path.exists(self.makeblastdb):
            raise FileNotFoundError(f"makeblastdb not found at {self.makeblastdb}")

        conda_env_dir = conda_env_path if conda_env_path else os.path.dirname(os.path.dirname(self.rmblastn))

        possible_matrix_paths = [
            os.path.join(conda_env_dir, 'share/RepeatModeler/Matrices/ncbi/nt/comparison.matrix'),
            os.path.join(conda_env_dir, 'share/RepeatMasker/Libraries/Dfam.hmm'),
            os.path.join(conda_env_dir, 'share/RepeatMasker/Matrices/nt'),
            os.path.join(conda_env_dir, 'share/RepeatMasker/Matrices/BLOSUM62')
        ]

        self.matrix_path = None
        for path in possible_matrix_paths:
            if os.path.exists(path):
                self.matrix_path = path
                break

        if self.matrix_path is None:
            self.matrix_path = 'BLOSUM62'

        logger.info(f"Using rmblastn from: {self.rmblastn}")
        logger.info(f"Using scoring matrix: {self.matrix_path}")

    def get_current_conda_env(self):
        conda_prefix = os.environ.get('CONDA_PREFIX')
        if conda_prefix:
            return conda_prefix

        import sys
        if 'conda' in sys.prefix:
            return sys.prefix

        try:
            conda_path = subprocess.check_output(['which', 'conda']).decode().strip()
            if conda_path:
                result = subprocess.check_output(['conda', 'info', '--json']).decode()
                conda_info = json.loads(result)
                active_prefix = conda_info.get('active_prefix')
                if active_prefix:
                    return active_prefix
        except (subprocess.CalledProcessError, json.JSONDecodeError):
            pass

        return None


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description='Enhanced Refiner_for_Graph: Build consensus sequences for TE families with improved information retention and evolutionary awareness.')
    parser.add_argument('input', help='Input FASTA file')
    parser.add_argument('output', help='Output FASTA file')
    parser.add_argument('-t', '--threads', type=int,
                        help='Number of threads', default=1)
    parser.add_argument('--min-score', type=float, default=150,
                        help='Minimum alignment score (default: 150)')
    parser.add_argument('--gap-init', type=int, default=20,
                        help='Gap initiation penalty (default: 20)')
    parser.add_argument('--gap-ext', type=int, default=5,
                        help='Gap extension penalty (default: 5)')
    parser.add_argument('--distance-threshold', type=float, default=0.7,
                        help='Distance threshold for clustering (default: 0.7)')
    parser.add_argument('--entropy-threshold', type=float, default=0.4,
                        help='Entropy threshold for conserved positions (default: 0.4)')
    parser.add_argument('--min-conserved-block', type=int, default=15,
                        help='Minimum length for conserved blocks (default: 15)')
    parser.add_argument('--max-cluster-size', type=int, default=50,
                        help='Maximum cluster size before splitting (default: 50)')
    parser.add_argument('--use-iupac', action='store_true', default=True,
                        help='Use IUPAC ambiguity codes for variable positions')
    parser.add_argument('--keep-variants', action='store_true', default=True,
                        help='Keep representative variants from each cluster')
    parser.add_argument('--details-dir', type=str,
                        help='Directory to store detailed information for each cluster')
    parser.add_argument('-v', '--verbose', action='store_true',
                        help='Enable verbose logging')

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger(__name__).setLevel(logging.DEBUG)

    try:
        config = Config()

        builder = TEConsensusBuilder(
            rmblast_dir=os.path.dirname(config.rmblastn),
            makeblastdb_path=config.makeblastdb,
            matrix_path=config.matrix_path,
            min_score=args.min_score,
            gap_init=args.gap_init,
            gap_ext=args.gap_ext,
            threads=args.threads,
            entropy_threshold=args.entropy_threshold,
            min_conserved_block=args.min_conserved_block,
            use_iupac=args.use_iupac,
            keep_variants=args.keep_variants
        )
        
        # Create SequenceClusterer and set maximum cluster size
        clusterer = SequenceClusterer(
            builder,
            distance_threshold=args.distance_threshold,
            max_cluster_size=args.max_cluster_size,
            subclustering_threshold=args.distance_threshold * 0.6  # Stricter secondary clustering threshold
        )
        
        result = builder.build_clustered_consensus(
            args.input, 
            args.output,
            details_dir=args.details_dir
        )
        
        logger.info(f"Process completed successfully.")
        logger.info(f"Generated {result['num_clusters']} clusters and {result['num_sequences']} sequences.")
        logger.info(f"Results saved to {result['consensus_file']}")
        logger.info(f"Statistics saved to {result['stats_file']}")
        logger.info(f"Detailed information saved to {result['details_dir']}")

    except Exception as e:
        logger.error(f"Error: {str(e)}")
        import traceback
        logger.error(f"Full traceback: {traceback.format_exc()}")
        sys.exit(1)
