#!/usr/bin/env python3

import os
import sys
import logging
import subprocess
import tempfile
import argparse
import json
import time
from collections import defaultdict
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq
import networkx as nx
from concurrent.futures import ThreadPoolExecutor
import shutil
from tqdm import tqdm
import numpy as np
# Removed matplotlib imports
import pandas as pd

# Add compatibility layer for np.bool deprecation
if hasattr(np, 'bool'):
    # For backwards compatibility with code that might use np.bool
    pass
else:
    # Create alias for backward compatibility
    np.bool = np.bool_

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class FastClusteringPipeline:
    def __init__(self, input_fasta, output_dir, 
                 cluster_method="minimap2", threads=1,
                 identity_threshold=0.8, coverage_threshold=0.8,
                 cleanup_temp=True, max_iterations=3,
                 convergence_threshold=0.05, max_cluster_size=50,
                 keep_variants=True, store_iterations=True,
                 use_iupac=True):
        """
        Process large sequence datasets using high-performance clustering methods, optimizing information retention and evolutionary awareness
        
        Parameters:
            input_fasta (str): Input FASTA file path
            output_dir (str): Output directory path
            cluster_method (str): Clustering method (minimap2/mmseqs/vsearch)
            threads (int): Number of threads
            identity_threshold (float): Similarity threshold (0-1)
            coverage_threshold (float): Coverage threshold (0-1)
            cleanup_temp (bool): Whether to clean up temporary files
            max_iterations (int): Maximum number of iterations
            convergence_threshold (float): Convergence threshold, stop iterations when sequence count change rate is below this value
            max_cluster_size (int): Maximum cluster size, clusters larger than this will be split
            keep_variants (bool): Whether to keep subfamily representative variants
            store_iterations (bool): Whether to save intermediate iteration results
            use_iupac (bool): Use IUPAC ambiguous bases to represent variant sites
        """
        self.input_fasta = input_fasta
        self.output_dir = output_dir
        self.cluster_method = cluster_method
        self.threads = threads
        self.identity_threshold = identity_threshold
        self.coverage_threshold = coverage_threshold
        self.cleanup_temp = cleanup_temp
        self.max_iterations = max_iterations
        self.convergence_threshold = convergence_threshold
        self.max_cluster_size = max_cluster_size
        self.keep_variants = keep_variants
        self.store_iterations = store_iterations
        self.use_iupac = use_iupac
        
        # Create output directory
        os.makedirs(output_dir, exist_ok=True)
        
        # Create temporary directory
        self.temp_dir = os.path.join(output_dir, "temp")
        os.makedirs(self.temp_dir, exist_ok=True)
        
        # Create iteration results directory if needed
        if self.store_iterations:
            self.iterations_dir = os.path.join(output_dir, "iterations")
            os.makedirs(self.iterations_dir, exist_ok=True)
            
        # Create details directory
        self.details_dir = os.path.join(output_dir, "cluster_details")
        os.makedirs(self.details_dir, exist_ok=True)
        
        # Initialize clustering statistics
        self.cluster_stats = []
        
    def check_tools(self):
        """Check if necessary tools are available"""
        tools = ["Refiner_for_Graph"]
        
        if self.cluster_method == "minimap2":
            tools.append("minimap2")
        elif self.cluster_method == "mmseqs":
            tools.append("mmseqs")
        elif self.cluster_method == "vsearch":
            tools.append("vsearch")
        else:
            tools.append("cd-hit-est")
            
        # Check if enhancement tools are available
        helpful_tools = ["mafft", "fasttree", "mash"]
            
        missing_tools = []
        optional_missing = []
        
        for tool in tools:
            if not shutil.which(tool):
                missing_tools.append(tool)
                
        for tool in helpful_tools:
            if not shutil.which(tool):
                optional_missing.append(tool)
                
        if missing_tools:
            logger.error(f"Missing the following necessary tools: {', '.join(missing_tools)}")
            return False
            
        if optional_missing:
            logger.warning(f"The following optional tools are missing, some advanced features may be limited: {', '.join(optional_missing)}")
            
        return True
        
    def preprocess_sequences(self, input_file):
        """Preprocess sequences: length filtering, low complexity region filtering, etc."""
        logger.info(f"Preprocessing sequences: {input_file}")
        
        min_length = 20  # Minimum length threshold
        max_n_ratio = 0.1  # Maximum N ratio
        
        # Read all sequences
        sequences = list(SeqIO.parse(input_file, "fasta"))
        input_seq_count = len(sequences)
        
        # Filter sequences
        filtered_sequences = []
        for seq in sequences:
            seq_len = len(seq.seq)
            n_count = str(seq.seq).upper().count('N')
            n_ratio = n_count / seq_len if seq_len > 0 else 1.0
            
            if seq_len >= min_length and n_ratio <= max_n_ratio:
                filtered_sequences.append(seq)
                
        output_seq_count = len(filtered_sequences)
        
        # Write processed sequences
        preprocessed_file = os.path.join(self.temp_dir, "preprocessed.fa")
        SeqIO.write(filtered_sequences, preprocessed_file, "fasta")
        
        logger.info(f"Preprocessing complete: {input_seq_count} input sequences, {output_seq_count} sequences passed filtering")
        return preprocessed_file
        
    def run_clustering(self, input_file):
        """Perform sequence clustering using the selected method"""
        logger.info(f"Using {self.cluster_method} for sequence clustering...")
        
        if self.cluster_method == "minimap2":
            return self.run_minimap2_clustering(input_file)
        elif self.cluster_method == "mmseqs":
            return self.run_mmseqs_clustering(input_file)
        elif self.cluster_method == "vsearch":
            return self.run_vsearch_clustering(input_file)
        else:
            # Fall back to CD-HIT-EST
            return self.run_cdhit_clustering(input_file)
            
    def run_minimap2_clustering(self, input_file):
        """Use minimap2 for fast clustering, optimized for improved information retention"""
        paf_file = os.path.join(self.temp_dir, f"minimap2_{int(time.time())}.paf")
        
        # minimap2 parameters, optimized for LTR elements
        # -k 15: Use smaller k-mer to increase sensitivity
        # -p 0.8: Secondary alignment threshold, 0.8 is a moderate value
        # -c: Output CIGAR string for precise similarity calculation
        cmd = [
            "minimap2",
            "-x", "asm20",     # Preset more suitable for repeat sequences
            "-k", "15",        # Smaller k-mer for increased sensitivity
            "-w", "10",        # Window size
            "-c",              # Output CIGAR string
            "-p", "0.8",       # Secondary alignment threshold
            "-N", "100",       # More candidate chains
            "--cs",            # Output detailed alignment information
            "-D",              # Enable dense alignment mode
            "-t", str(self.threads),
            "-o", paf_file,
            input_file, input_file
        ]
        
        try:
            subprocess.run(cmd, check=True, capture_output=True)
            logger.info("minimap2 alignment complete")
            
            # Parse PAF file and build graph
            G = self.parse_minimap2_results(paf_file)
            
            # Use graph algorithms to identify clusters, adding evolutionary perspective
            clusters = self.identify_clusters_from_graph(G)
            
            return clusters, input_file
            
        except subprocess.CalledProcessError as e:
            logger.error(f"minimap2 execution failed: {e.stderr.decode() if hasattr(e, 'stderr') else str(e)}")
            raise
            
    def parse_minimap2_results(self, paf_file):
        """Parse minimap2 PAF output, build sequence similarity graph"""
        G = nx.Graph()
        
        # Set clustering thresholds
        min_identity = self.identity_threshold
        min_coverage = self.coverage_threshold
        
        # Read PAF file
        with open(paf_file) as f:
            for line in f:
                fields = line.strip().split("\t")
                if len(fields) < 12:
                    continue
                    
                query_name = fields[0]
                target_name = fields[5]
                
                # Skip self-alignment
                if query_name == target_name:
                    continue
                    
                query_len = int(fields[1])
                target_len = int(fields[6])
                alignment_len = int(fields[10])
                matches = int(fields[9])
                
                # Calculate similarity and coverage
                identity = matches / alignment_len if alignment_len > 0 else 0
                q_coverage = alignment_len / query_len if query_len > 0 else 0
                t_coverage = alignment_len / target_len if target_len > 0 else 0
                coverage = min(q_coverage, t_coverage)  # Use minimum coverage rather than maximum, more strict
                
                # If thresholds are met, add edge
                if identity >= min_identity and coverage >= min_coverage:
                    weight = identity * coverage  # Combined weight
                    
                    # Save more information
                    edge_data = {
                        'weight': weight,
                        'identity': identity,
                        'coverage': coverage,
                        'alignment_length': alignment_len,
                        'q_len': query_len,
                        't_len': target_len
                    }
                    
                    if G.has_edge(query_name, target_name):
                        # Use highest weight and update edge attributes
                        current_weight = G[query_name][target_name]['weight']
                        if weight > current_weight:
                            G[query_name][target_name].update(edge_data)
                    else:
                        G.add_edge(query_name, target_name, **edge_data)
                        
        # Ensure all sequence IDs are in the graph, including those without high similarity
        for line in open(paf_file):
            fields = line.strip().split("\t")
            if len(fields) < 2:
                continue
            G.add_node(fields[0])
            G.add_node(fields[5])
            
        logger.info(f"Graph built from minimap2 results, contains {len(G.nodes())} nodes, {len(G.edges())} edges")
        return G
        
    def identify_clusters_from_graph(self, G):
        """
        Identify clusters from graph, considering evolutionary relationships, and limiting cluster size
        """
        # First use connected components algorithm for basic clustering
        basic_clusters = list(nx.connected_components(G))
        logger.info(f"Basic graph clustering identified {len(basic_clusters)} clusters")
        
        # Identify and process large clusters
        final_clusters = []
        for i, cluster in enumerate(basic_clusters):
            if len(cluster) > self.max_cluster_size:
                logger.info(f"Large cluster found (ID {i+1}): {len(cluster)} sequences, attempting to split")
                subclusters = self.split_large_cluster(G, cluster)
                logger.info(f"Cluster {i+1} split into {len(subclusters)} subclusters")
                final_clusters.extend(subclusters)
            else:
                final_clusters.append(cluster)
                
        logger.info(f"Finally identified {len(final_clusters)} clusters")
        
        # Record clustering statistics
        self.record_cluster_stats(final_clusters)
        
        return final_clusters
        
    def split_large_cluster(self, G, cluster_nodes):
        """
        Split large clusters using community detection and subgraph partitioning methods
        """
        # Extract subgraph
        subgraph = G.subgraph(cluster_nodes).copy()
        
        # Method 1: Try using community detection algorithm
        try:
            from community import best_partition
            partition = best_partition(subgraph)
            
            # Build communities
            communities = defaultdict(set)
            for node, community_id in partition.items():
                communities[community_id].add(node)
                
            subclusters = list(communities.values())
            
            # Check if effective split
            largest_subcluster = max(subclusters, key=len)
            if len(largest_subcluster) < len(cluster_nodes) * 0.9:
                logger.info(f"Successfully split cluster using community detection: {len(subclusters)} subclusters")
                return subclusters
                
            logger.info("Community detection did not effectively split the cluster, trying other methods")
            
        except ImportError:
            logger.info("python-louvain library not installed, cannot use community detection, trying other methods")
        
        # Method 2: Use stricter edge weight threshold
        try:
            # Copy subgraph and sort edges by weight
            strict_subgraph = subgraph.copy()
            edges = sorted(strict_subgraph.edges(data=True), key=lambda x: x[2]['weight'])
            
            # Remove low weight edges until desired split level is reached
            for i in range(len(edges)):
                u, v, data = edges[i]
                strict_subgraph.remove_edge(u, v)
                
                # Check connected components
                components = list(nx.connected_components(strict_subgraph))
                
                # If components are small enough or numerous enough, stop
                largest_comp = max(components, key=len)
                if len(largest_comp) <= self.max_cluster_size or len(components) >= len(cluster_nodes) // self.max_cluster_size:
                    logger.info(f"Successfully split by removing edges: {len(components)} subclusters")
                    return components
                    
            # If ideal split cannot be achieved, return final state
            components = list(nx.connected_components(strict_subgraph))
            if len(components) > 1:
                logger.info(f"Partially split by removing edges: {len(components)} subclusters")
                return components
                
        except Exception as e:
            logger.warning(f"Edge weight splitting failed: {e}")
            
        # Method 3: Use graph partitioning algorithm
        try:
            from networkx.algorithms import community
            
            communities = list(community.girvan_newman(subgraph))
            
            # Take best partition
            for comm in communities:
                subclusters = list(comm)
                largest = max(subclusters, key=len)
                
                if len(largest) <= self.max_cluster_size or len(subclusters) >= 2:
                    logger.info(f"Successfully split using Girvan-Newman algorithm: {len(subclusters)} subclusters")
                    return subclusters
                    
        except ImportError:
            logger.info("Graph partitioning algorithm not available")
            
        # Method 4: If all methods fail, use size limit for simple division
        logger.warning(f"Cannot effectively split cluster, using size limit for simple division")
        nodes = list(cluster_nodes)
        subclusters = []
        
        for i in range(0, len(nodes), self.max_cluster_size):
            subclusters.append(set(nodes[i:i+self.max_cluster_size]))
            
        return subclusters
        
    def record_cluster_stats(self, clusters):
        """Record clustering statistics for monitoring"""
        stats = {
            'total_clusters': len(clusters),
            'cluster_sizes': [len(c) for c in clusters],
            'largest_cluster': max([len(c) for c in clusters]) if clusters else 0,
            'smallest_cluster': min([len(c) for c in clusters]) if clusters else 0,
            'avg_cluster_size': np.mean([len(c) for c in clusters]) if clusters else 0,
            'singleton_clusters': sum(1 for c in clusters if len(c) == 1),
        }
        
        self.cluster_stats.append(stats)
        
        # Output statistics
        logger.info(f"Clustering statistics: Total={stats['total_clusters']}, Largest={stats['largest_cluster']}, "
                    f"Average={stats['avg_cluster_size']:.1f}, Singletons={stats['singleton_clusters']}")
        
    def run_mmseqs_clustering(self, input_file):
        """Use MMseqs2 for ultra-fast clustering, especially suitable for large datasets"""
        # Create temporary directory
        mmseqs_tmp = os.path.join(self.temp_dir, f"mmseqs_tmp_{int(time.time())}")
        os.makedirs(mmseqs_tmp, exist_ok=True)
        
        # Set output files
        prefix = os.path.join(self.temp_dir, f"mmseqs_{int(time.time())}")
        db_file = f"{prefix}_DB"
        cluster_file = f"{prefix}_cluster"
        cluster_tsv = f"{prefix}_cluster.tsv"
        
        try:
            # Step 1: Create database
            cmd = [
                "mmseqs", "createdb", 
                input_file, 
                db_file
            ]
            subprocess.run(cmd, check=True, capture_output=True)
            
            # Step 2: Run clustering
            # Set --min-seq-id as identity threshold
            # Set -c as coverage threshold
            cmd = [
                "mmseqs", "cluster",
                db_file, 
                cluster_file,
                mmseqs_tmp,
                "--min-seq-id", str(self.identity_threshold),
                "-c", str(self.coverage_threshold),
                "--cov-mode", "2",  # 2 represents target sequence coverage
                "--cluster-mode", "0",  # 0: connected component clustering, 1: single-linkage clustering, 2: balanced clustering
                "--cluster-steps", "5",  # More clustering steps, increases sensitivity
                "--max-seqs", "300",  # Keep more similar sequences
                "--threads", str(self.threads)
            ]
            subprocess.run(cmd, check=True, capture_output=True)
            
            # Step 3: Convert results to human-readable format
            cmd = [
                "mmseqs", "createtsv",
                db_file,
                db_file,
                cluster_file,
                cluster_tsv
            ]
            subprocess.run(cmd, check=True, capture_output=True)
            
            # Parse clustering results
            clusters = self.parse_mmseqs_results(cluster_tsv)
            
            # Record clustering statistics
            self.record_cluster_stats(clusters)
            
            return clusters, input_file
            
        except subprocess.CalledProcessError as e:
            logger.error(f"MMseqs2 execution failed: {e.stderr.decode() if hasattr(e, 'stderr') else str(e)}")
            # Try falling back to minimap2
            logger.info("Trying minimap2 as fallback method")
            return self.run_minimap2_clustering(input_file)
        finally:
            # Clean up temporary files
            if self.cleanup_temp:
                try:
                    shutil.rmtree(mmseqs_tmp)
                except:
                    pass
                    
    def parse_mmseqs_results(self, cluster_tsv):
        """Parse MMseqs2 clustering results"""
        cluster_dict = defaultdict(set)
        
        with open(cluster_tsv) as f:
            for line in f:
                fields = line.strip().split("\t")
                if len(fields) >= 2:
                    rep_id = fields[0]  # Representative sequence ID
                    member_id = fields[1]  # Member sequence ID
                    cluster_dict[rep_id].add(member_id)
                    # Ensure representative sequence is also in the set
                    cluster_dict[rep_id].add(rep_id)
        
        # Convert to list and check large clusters
        basic_clusters = list(cluster_dict.values())
        
        # Split large clusters
        final_clusters = []
        for i, cluster in enumerate(basic_clusters):
            if len(cluster) > self.max_cluster_size:
                logger.info(f"Large cluster found (ID {i+1}): {len(cluster)} sequences, attempting to split")
                # MMseqs2 cannot use graph splitting method, use cd-hit-est for secondary clustering
                subclusters = self.split_large_cluster_cdhit(list(cluster), input_file)
                logger.info(f"Cluster {i+1} split into {len(subclusters)} subclusters")
                final_clusters.extend(subclusters)
            else:
                final_clusters.append(cluster)
                
        logger.info(f"MMseqs2 clustering generated {len(final_clusters)} clusters")
        return final_clusters
        
    def split_large_cluster_cdhit(self, seq_ids, input_file):
        """Split large clusters using CD-HIT-EST"""
        # Extract sequences from input file
        sequences = {}
        for record in SeqIO.parse(input_file, "fasta"):
            if record.id in seq_ids:
                sequences[record.id] = record
                
        if not sequences:
            logger.warning("Could not find sequences in cluster, returning original cluster")
            return [set(seq_ids)]
            
        # Write to temporary file
        temp_input = os.path.join(self.temp_dir, f"large_cluster_{int(time.time())}.fa")
        temp_output = temp_input + ".out"
        
        with open(temp_input, "w") as f:
            for record in sequences.values():
                SeqIO.write(record, f, "fasta")
                
        # Run CD-HIT-EST with stricter thresholds
        # Increase identity threshold, decrease coverage threshold
        stricter_identity = min(0.95, self.identity_threshold + 0.1)
        stricter_coverage = max(0.5, self.coverage_threshold - 0.1)
        
        cmd = [
            "cd-hit-est",
            "-i", temp_input,
            "-o", temp_output,
            "-c", str(stricter_identity),
            "-aS", str(stricter_coverage),
            "-g", "1",  # Use local best match
            "-G", "0",  # Allow clustering in both directions
            "-M", "8000",  # Memory limit (MB)
            "-T", str(min(self.threads, 4))  # Limit number of threads
        ]
        
        try:
            subprocess.run(cmd, check=True, capture_output=True)
            
            # Parse CD-HIT clustering results
            clusters_file = temp_output + ".clstr"
            subclusters = self.parse_cdhit_clusters(clusters_file)
            
            # Check results
            if len(subclusters) <= 1:
                logger.warning("CD-HIT failed to split cluster, trying with stricter parameters")
                
                # Stricter parameters
                stricter_cmd = [
                    "cd-hit-est",
                    "-i", temp_input,
                    "-o", temp_output + ".strict",
                    "-c", "0.98",  # Very high similarity requirement
                    "-aS", "0.4",  # Lower coverage requirement
                    "-g", "1",
                    "-G", "0",
                    "-M", "8000",
                    "-T", str(min(self.threads, 4))
                ]
                
                subprocess.run(stricter_cmd, check=True, capture_output=True)
                stricter_clusters = self.parse_cdhit_clusters(temp_output + ".strict.clstr")
                
                if len(stricter_clusters) > 1:
                    return stricter_clusters
                    
                # If still can't split, use simple size limit
                logger.warning("Cannot effectively split cluster, using size limit for simple division")
                seq_list = list(seq_ids)
                simple_subclusters = []
                
                for i in range(0, len(seq_list), self.max_cluster_size):
                    simple_subclusters.append(set(seq_list[i:i+self.max_cluster_size]))
                    
                return simple_subclusters
                
            return subclusters
            
        except subprocess.CalledProcessError as e:
            logger.error(f"CD-HIT-EST splitting failed: {e}")
            return [set(seq_ids)]
        finally:
            # Clean up temporary files
            if self.cleanup_temp:
                for f in [temp_input, temp_output, temp_output + ".clstr",
                          temp_output + ".strict", temp_output + ".strict.clstr"]:
                    if os.path.exists(f):
                        try:
                            os.remove(f)
                        except:
                            pass
        
    def run_vsearch_clustering(self, input_file):
        """Use vsearch for fast clustering"""
        # Set output file
        cluster_file = os.path.join(self.temp_dir, f"vsearch_{int(time.time())}.uc")
        
        try:
            # Run vsearch clustering
            # --id: Identity threshold
            # --strand both: Check both strands
            cmd = [
                "vsearch",
                "--cluster_fast", input_file,
                "--id", str(self.identity_threshold),
                "--strand", "both",
                "--uc", cluster_file,
                "--threads", str(self.threads),
                "--qmask", "none",  # Don't use masking
                "--minseqlength", "100",  # Minimum sequence length
                "--query_cov", str(self.coverage_threshold),  # Query coverage
                "--target_cov", str(self.coverage_threshold)  # Target coverage
            ]
            subprocess.run(cmd, check=True, capture_output=True)
            
            # Parse clustering results
            clusters = self.parse_vsearch_results(cluster_file)
            
            # Check and split large clusters
            final_clusters = []
            for i, cluster in enumerate(clusters):
                if len(cluster) > self.max_cluster_size:
                    logger.info(f"Large cluster found (ID {i+1}): {len(cluster)} sequences, attempting to split")
                    subclusters = self.split_large_cluster_cdhit(list(cluster), input_file)
                    logger.info(f"Cluster {i+1} split into {len(subclusters)} subclusters")
                    final_clusters.extend(subclusters)
                else:
                    final_clusters.append(cluster)
            
            # Record clustering statistics
            self.record_cluster_stats(final_clusters)
            
            return final_clusters, input_file
            
        except subprocess.CalledProcessError as e:
            logger.error(f"vsearch execution failed: {e.stderr.decode() if hasattr(e, 'stderr') else str(e)}")
            # Try falling back to minimap2
            logger.info("Trying minimap2 as fallback method")
            return self.run_minimap2_clustering(input_file)
            
    def parse_vsearch_results(self, cluster_file):
        """Parse vsearch clustering results"""
        cluster_dict = defaultdict(set)
        
        with open(cluster_file) as f:
            for line in f:
                if line.startswith('C') or line.startswith('H'):
                    fields = line.strip().split("\t")
                    if len(fields) >= 9:
                        cluster_id = fields[1]
                        seq_id = fields[8]
                        if seq_id == '*':
                            seq_id = fields[9]  # Sometimes representative sequence is in the 10th field
                        cluster_dict[cluster_id].add(seq_id)
        
        # Convert to list
        clusters = list(cluster_dict.values())
        logger.info(f"vsearch clustering generated {len(clusters)} clusters")
        return clusters
        
    def run_cdhit_clustering(self, input_file):
        """Use CD-HIT-EST for initial clustering"""
        output_file = os.path.join(self.temp_dir, f"cdhit_{int(time.time())}.fa")
        clusters_file = output_file + ".clstr"
        
        # CD-HIT parameters
        cmd = [
            "cd-hit-est",
            "-i", input_file,
            "-o", output_file,
            "-aS", str(self.coverage_threshold),  # Sequence coverage
            "-c", str(self.identity_threshold),   # Sequence identity
            "-g", "1",             # Use local best match
            "-G", "0",             # Allow clustering in both directions
            "-A", "80",            # Minimum alignment length
            "-M", "10000",         # Memory limit (MB)
            "-T", str(self.threads)
        ]
        
        try:
            subprocess.run(cmd, check=True, capture_output=True)
            logger.info("CD-HIT clustering complete")
            
            # Parse clustering results
            if os.path.exists(clusters_file):
                clusters = self.parse_cdhit_clusters(clusters_file)
                
                # Check and split large clusters
                final_clusters = []
                for i, cluster in enumerate(clusters):
                    if len(cluster) > self.max_cluster_size:
                        logger.info(f"Large cluster found (ID {i+1}): {len(cluster)} sequences, attempting to split")
                        
                        # For CD-HIT, can directly re-cluster with stricter parameters
                        stricter_identity = min(0.95, self.identity_threshold + 0.1)
                        
                        # Prepare temporary files
                        temp_input = os.path.join(self.temp_dir, f"large_cluster_{i+1}.fa")
                        temp_output = temp_input + ".refined"
                        
                        # Extract sequences
                        seqs_to_extract = {}
                        for seq in SeqIO.parse(input_file, "fasta"):
                            if seq.id in cluster:
                                seqs_to_extract[seq.id] = seq
                                
                        # Write to temporary file
                        with open(temp_input, "w") as f:
                            SeqIO.write(seqs_to_extract.values(), f, "fasta")
                            
                        # Run CD-HIT with stricter parameters
                        stricter_cmd = [
                            "cd-hit-est",
                            "-i", temp_input,
                            "-o", temp_output,
                            "-c", str(stricter_identity),
                            "-aS", str(self.coverage_threshold),
                            "-g", "1",
                            "-G", "0",
                            "-A", "80",
                            "-M", "10000",
                            "-T", str(min(self.threads, 4))
                        ]
                        
                        try:
                            subprocess.run(stricter_cmd, check=True, capture_output=True)
                            strict_clusters = self.parse_cdhit_clusters(temp_output + ".clstr")
                            
                            if len(strict_clusters) > 1:
                                logger.info(f"Successfully split cluster {i+1} into {len(strict_clusters)} subclusters")
                                final_clusters.extend(strict_clusters)
                            else:
                                logger.warning(f"Cannot effectively split cluster {i+1}, using simple division")
                                seq_list = list(cluster)
                                for j in range(0, len(seq_list), self.max_cluster_size):
                                    final_clusters.append(set(seq_list[j:j+self.max_cluster_size]))
                        except:
                            logger.warning(f"Splitting cluster {i+1} failed, using simple division")
                            seq_list = list(cluster)
                            for j in range(0, len(seq_list), self.max_cluster_size):
                                final_clusters.append(set(seq_list[j:j+self.max_cluster_size]))
                                
                        # Clean up temporary files
                        if self.cleanup_temp:
                            for f in [temp_input, temp_output, temp_output + ".clstr"]:
                                if os.path.exists(f):
                                    try:
                                        os.remove(f)
                                    except:
                                        pass
                    else:
                        final_clusters.append(cluster)
                
                # Record clustering statistics
                self.record_cluster_stats(final_clusters)
                
                logger.info(f"CD-HIT generated {len(final_clusters)} clusters")
                return final_clusters, output_file
            else:
                logger.warning(f"Cluster file {clusters_file} does not exist")
                return [], input_file
                
        except subprocess.CalledProcessError as e:
            logger.error(f"CD-HIT execution failed: {e.stderr.decode() if hasattr(e, 'stderr') else str(e)}")
            # If CD-HIT fails, return empty cluster list
            return [], input_file
            
    def parse_cdhit_clusters(self, clusters_file):
        """Parse CD-HIT cluster file"""
        cluster_dict = defaultdict(set)
        current_cluster = None
        
        with open(clusters_file) as f:
            for line in f:
                line = line.strip()
                if line.startswith(">Cluster"):
                    current_cluster = line.split()[1]
                elif line and current_cluster is not None:
                    # Extract sequence ID
                    parts = line.split(">")
                    if len(parts) > 1:
                        seq_id = parts[1].split("...")[0]
                        cluster_dict[current_cluster].add(seq_id)
        
        # Convert to list
        return list(cluster_dict.values())
    
    def refine_cluster(self, cluster_seqs, cluster_id):
        """
        Refine clusters containing multiple sequences, generate representative sequences and variants
        Enhance information retention and evolutionary awareness
        
        Parameters:
            cluster_seqs (list): List of sequence objects in the cluster
            cluster_id (str/int): Cluster ID
            
        Returns:
            dict: Dictionary containing consensus sequence and variants
        """
        if len(cluster_seqs) == 1:
            # Single sequence cluster, return directly
            return {
                'consensus': cluster_seqs[0],
                'variants': [],
                'alignment': None,
                'statistics': {
                    'size': 1,
                    'is_single': True,
                    'mean_identity': 1.0
                }
            }
            
        logger.info(f"Refining cluster {cluster_id} (containing {len(cluster_seqs)} sequences)")
        
        # Validate sequences
        valid_seqs = []
        for seq in cluster_seqs:
            if len(seq.seq) >= 50:  # Ensure sequence length is sufficient
                valid_seqs.append(seq)
                
        if not valid_seqs:
            logger.warning(f"Cluster {cluster_id} has no valid sequences, skipping refinement")
            return {
                'consensus': cluster_seqs[0] if cluster_seqs else None,
                'variants': [],
                'alignment': None,
                'statistics': {
                    'size': len(cluster_seqs),
                    'is_single': False,
                    'mean_identity': 0.0
                }
            }
            
        if len(valid_seqs) == 1:
            # If only one valid sequence, return directly
            return {
                'consensus': valid_seqs[0],
                'variants': [],
                'alignment': None,
                'statistics': {
                    'size': 1,
                    'is_single': True,
                    'mean_identity': 1.0
                }
            }
        
        # Create temporary files for Refiner_for_Graph
        cluster_fasta = os.path.join(self.temp_dir, f"cluster_{cluster_id}.fa")
        output_fasta = os.path.join(self.temp_dir, f"refined_{cluster_id}.fa")
        
        # Write cluster sequences
        SeqIO.write(valid_seqs, cluster_fasta, "fasta")
        
        # Check file size to ensure successful write
        if os.path.getsize(cluster_fasta) == 0:
            logger.warning(f"Cluster {cluster_id} write file is empty, using longest sequence as representative")
            return {
                'consensus': max(valid_seqs, key=lambda x: len(x.seq)),
                'variants': [],
                'alignment': None,
                'statistics': {
                    'size': len(valid_seqs),
                    'is_single': False,
                    'mean_identity': 0.0
                }
            }
        
        try:
            # Avoid floating point precision issues
            distance_threshold = round(1 - self.identity_threshold, 2)
            
            # Prepare enhanced Refiner_for_Graph parameters
            cmd = [
                "Refiner_for_Graph",
                cluster_fasta,
                output_fasta,
                "--distance-threshold", str(distance_threshold),
                "--entropy-threshold", "0.4",  # Entropy threshold, used for conserved block identification
                "--min-conserved-block", "15",  # Minimum conserved block length
                "--max-cluster-size", str(self.max_cluster_size),  # Maximum cluster size
                "--use-iupac" if self.use_iupac else "",  # Use IUPAC ambiguous bases
                "--keep-variants" if self.keep_variants else "",  # Keep variants
                "--details-dir", self.details_dir,  # Output detailed information
                "-t", str(min(self.threads, 4))  # Limit number of threads used per refinement
            ]
            
            # Filter out empty parameters
            cmd = [arg for arg in cmd if arg]
            
            # Try to capture detailed error information
            process = subprocess.run(
                cmd, 
                capture_output=True, 
                text=True
            )
            
            if process.returncode != 0:
                logger.warning(f"Refiner_for_Graph returned error (code {process.returncode}):")
                logger.warning(f"STDERR: {process.stderr}")
                logger.warning(f"STDOUT: {process.stdout}")
                # If Refiner_for_Graph fails, try using other methods
                return self._fallback_refine(valid_seqs, cluster_id)
            
            # Read refinement results
            if os.path.exists(output_fasta) and os.path.getsize(output_fasta) > 0:
                refined_seqs = list(SeqIO.parse(output_fasta, "fasta"))
                
                if refined_seqs:
                    # Parse results, distinguish consensus and variants
                    consensus = None
                    variants = []
                    
                    for seq in refined_seqs:
                        if "_variant_" in seq.id:
                            variants.append(seq)
                        else:
                            # Found consensus sequence
                            consensus = seq
                            # Update sequence description, add cluster information
                            consensus.description = f"refined from cluster {cluster_id} ({len(valid_seqs)} sequences)"
                            
                    # If no consensus sequence found, use first sequence
                    if not consensus and refined_seqs:
                        consensus = refined_seqs[0]
                        
                    # Try to read statistics file
                    stats_file = output_fasta + ".stats"
                    statistics = {
                        'size': len(valid_seqs),
                        'is_single': False,
                        'mean_identity': 0.0
                    }
                    
                    if os.path.exists(stats_file):
                        with open(stats_file) as f:
                            for line in f:
                                if "Mean entropy:" in line:
                                    try:
                                        entropy = float(line.split("=")[1].strip())
                                        statistics['mean_entropy'] = entropy
                                        # Approximate average identity calculation (1 - entropy)
                                        statistics['mean_identity'] = 1.0 - entropy
                                    except:
                                        pass
                                        
                                if "Conserved blocks:" in line:
                                    try:
                                        blocks = int(line.split(":")[1].strip())
                                        statistics['conserved_blocks'] = blocks
                                    except:
                                        pass
                    
                    return {
                        'consensus': consensus,
                        'variants': variants,
                        'alignment': None,  # Multiple sequence alignment not easily extracted from Refiner_for_Graph
                        'statistics': statistics
                    }
            
            logger.warning(f"Refiner_for_Graph failed to generate results, using fallback method")
            return self._fallback_refine(valid_seqs, cluster_id)
            
        except Exception as e:
            logger.error(f"Refiner_for_Graph processing cluster {cluster_id} failed: {e}")
            import traceback
            logger.error(f"Exception details: {traceback.format_exc()}")
            # Use fallback method on failure
            return self._fallback_refine(valid_seqs, cluster_id)
        finally:
            # Optionally clean up temporary files
            if self.cleanup_temp:
                for f in [cluster_fasta, output_fasta]:
                    if os.path.exists(f):
                        try:
                            os.remove(f)
                        except:
                            pass
                            
    def _fallback_refine(self, sequences, cluster_id):
        """Fallback method when Refiner_for_Graph fails"""
        logger.info(f"Using fallback method for cluster {cluster_id}")
        
        # Strategy 1: Try using cd-hit-est
        try:
            # Create temporary files
            input_file = os.path.join(self.temp_dir, f"fallback_{cluster_id}_input.fa")
            output_file = os.path.join(self.temp_dir, f"fallback_{cluster_id}_output.fa")
            
            # Write sequences
            SeqIO.write(sequences, input_file, "fasta")
            
            # Run cd-hit-est
            cmd = [
                "cd-hit-est",
                "-i", input_file,
                "-o", output_file,
                "-c", "0.9",       # Higher identity threshold
                "-aS", "0.8",
                "-g", "1",
                "-d", "0",         # Use full sequence description
                "-T", "1"          # Single thread, avoid concurrency issues
            ]
            
            process = subprocess.run(cmd, capture_output=True, text=True)
            
            if process.returncode == 0 and os.path.exists(output_file):
                # Read results
                cdhit_seqs = list(SeqIO.parse(output_file, "fasta"))
                
                if cdhit_seqs:
                    consensus = cdhit_seqs[0]
                    consensus.description = f"cd-hit consensus for cluster {cluster_id} ({len(sequences)} sequences)"
                    
                    # Select representative variants
                    variants = []
                    if len(cdhit_seqs) > 1 and self.keep_variants:
                        # If multiple sequences, select 2-3 most different ones as variants
                        var_candidates = cdhit_seqs[1:min(4, len(cdhit_seqs))]
                        
                        for i, var in enumerate(var_candidates):
                            var_id = f"cluster_{cluster_id}_variant_{i+1}"
                            var.id = var_id
                            var.description = f"Representative variant {i+1} for cluster {cluster_id}"
                            variants.append(var)
                    
                    return {
                        'consensus': consensus,
                        'variants': variants,
                        'alignment': None,
                        'statistics': {
                            'size': len(sequences),
                            'is_single': False,
                            'mean_identity': 0.9  # Estimated value, based on cd-hit-est parameters
                        }
                    }
        except Exception as e:
            logger.warning(f"cd-hit-est fallback method failed: {e}")
        
        # Strategy 2: If cd-hit-est also fails, use multiple sequence alignment and simple consensus building
        try:
            # Create temporary files
            input_file = os.path.join(self.temp_dir, f"mafft_{cluster_id}_input.fa")
            output_file = os.path.join(self.temp_dir, f"mafft_{cluster_id}_output.fa")
            
            # Write sequences
            SeqIO.write(sequences, input_file, "fasta")
            
            # Use MAFFT for multiple sequence alignment
            cmd = ["mafft", "--auto", "--quiet", input_file]
            
            process = subprocess.run(cmd, capture_output=True, text=True)
            
            if process.returncode == 0:
                # Save alignment results
                with open(output_file, "w") as f:
                    f.write(process.stdout)
                    
                # Parse aligned sequences from results
                align_seqs = list(SeqIO.parse(StringIO(process.stdout), "fasta"))
                
                if align_seqs:
                    # Build consensus
                    from Bio.Align import MultipleSeqAlignment
                    from Bio.Seq import Seq
                    
                    # Create alignment object
                    alignment = MultipleSeqAlignment(align_seqs)
                    
                    # Build consensus sequence
                    consensus_seq = ""
                    for i in range(alignment.get_alignment_length()):
                        column = alignment[:, i]
                        # Remove gaps
                        bases = [b for b in column if b != '-']
                        
                        if not bases:
                            continue  # Skip columns that are all gaps
                            
                        # Calculate frequencies
                        base_counts = {}
                        for base in bases:
                            base_counts[base] = base_counts.get(base, 0) + 1
                            
                        # Find most common base
                        most_common = max(base_counts.items(), key=lambda x: x[1])
                        
                        if self.use_iupac and len(base_counts) > 1:
                            # Use IUPAC to represent multiple bases
                            total = sum(base_counts.values())
                            significant_bases = set()
                            
                            for base, count in base_counts.items():
                                if count / total >= 0.2:  # At least 20% frequency
                                    significant_bases.add(base)
                                    
                            if len(significant_bases) > 1:
                                # Look up IUPAC code
                                for iupac_set, code in {
                                    frozenset(['A', 'G']): 'R',
                                    frozenset(['C', 'T']): 'Y',
                                    frozenset(['G', 'C']): 'S',
                                    frozenset(['A', 'T']): 'W',
                                    frozenset(['G', 'T']): 'K',
                                    frozenset(['A', 'C']): 'M',
                                    frozenset(['C', 'G', 'T']): 'B',
                                    frozenset(['A', 'G', 'T']): 'D',
                                    frozenset(['A', 'C', 'T']): 'H',
                                    frozenset(['A', 'C', 'G']): 'V',
                                    frozenset(['A', 'C', 'G', 'T']): 'N',
                                }.items():
                                    if significant_bases == iupac_set:
                                        consensus_seq += code
                                        break
                                else:
                                    consensus_seq += most_common[0]
                            else:
                                consensus_seq += most_common[0]
                        else:
                            consensus_seq += most_common[0]
                    
                    # Create new sequence record
                    consensus = SeqRecord(
                        Seq(consensus_seq),
                        id=f"cluster_{cluster_id}_consensus",
                        description=f"MAFFT consensus for cluster {cluster_id} ({len(sequences)} sequences)"
                    )
                    
                    # Select variants (if needed)
                    variants = []
                    if self.keep_variants and len(align_seqs) > 1:
                        # Select the most different sequences as variants
                        # Calculate differences between each sequence and consensus
                        differences = []
                        for i, seq in enumerate(align_seqs):
                            diff_count = sum(1 for a, b in zip(str(seq.seq), consensus_seq) if a != b)
                            diff_ratio = diff_count / len(consensus_seq) if consensus_seq else 0
                            differences.append((i, diff_ratio))
                            
                        # Sort and select 2-3 most different ones
                        differences.sort(key=lambda x: x[1], reverse=True)
                        for i, (idx, _) in enumerate(differences[:min(3, len(differences))]):
                            var = align_seqs[idx]
                            # Remove gaps from alignment
                            var_seq = str(var.seq).replace('-', '')
                            
                            var_record = SeqRecord(
                                Seq(var_seq),
                                id=f"cluster_{cluster_id}_variant_{i+1}",
                                description=f"Representative variant {i+1} for cluster {cluster_id}"
                            )
                            variants.append(var_record)
                    
                    return {
                        'consensus': consensus,
                        'variants': variants,
                        'alignment': alignment,
                        'statistics': {
                            'size': len(sequences),
                            'is_single': False,
                            'mean_identity': 0.8  # Estimated value
                        }
                    }
                    
        except Exception as e:
            logger.warning(f"MAFFT fallback method failed: {e}")
        
        # Strategy 3: If all else fails, use longest sequence
        logger.info(f"Using longest sequence as representative for cluster {cluster_id}")
        longest_seq = max(sequences, key=lambda x: len(x.seq))
        longest_seq.description = f"Longest sequence from cluster {cluster_id} ({len(sequences)} sequences)"
        
        return {
            'consensus': longest_seq,
            'variants': [],
            'alignment': None,
            'statistics': {
                'size': len(sequences),
                'is_single': False,
                'mean_identity': 0.0  # Unknown
            }
        }

    def process_clusters(self, clusters, sequences_file):
        """
        Process clustering results:
        - Refine multi-sequence clusters
        - Use single-sequence clusters directly
        - Keep variants and statistics
        
        Parameters:
            clusters (list): List of clusters, each element is a set of sequence IDs
            sequences_file (str): Sequence file path
            
        Returns:
            Processed sequence list and statistics
        """
        logger.info(f"Processing {len(clusters)} clusters...")
        
        # 修复: 解决读取序列时ID重复的问题
        # 首先读取所有序列作为列表而不是字典
        all_sequences = list(SeqIO.parse(sequences_file, "fasta"))
        
        # 检查重复ID
        seen_ids = set()
        duplicate_ids = set()
        for seq in all_sequences:
            if seq.id in seen_ids:
                duplicate_ids.add(seq.id)
            seen_ids.add(seq.id)
        
        # 如果有重复ID, 重命名它们
        if duplicate_ids:
            logger.warning(f"Found {len(duplicate_ids)} duplicate sequence IDs. Renaming them.")
            for i, seq in enumerate(all_sequences):
                if seq.id in duplicate_ids:
                    original_id = seq.id
                    seq.id = f"{seq.id}_unique_{i}"
                    logger.info(f"Renamed sequence ID from '{original_id}' to '{seq.id}'")
        
        # 创建ID到序列的映射
        sequences = {seq.id: seq for seq in all_sequences}
        
        # 准备结果
        consensus_results = []
        variant_results = []
        all_stats = []
        
        # 使用多线程处理集群
        with ThreadPoolExecutor(max_workers=self.threads) as executor:
            future_to_cluster = {}
            
            for i, cluster in enumerate(clusters):
                # 获取此集群的序列
                cluster_seqs = [sequences[seq_id] for seq_id in cluster if seq_id in sequences]
                
                if not cluster_seqs:
                    continue
                    
                # 提交处理任务
                future = executor.submit(self.refine_cluster, cluster_seqs, i)
                future_to_cluster[future] = i
            
            # 收集结果
            for future in tqdm(future_to_cluster, desc="Processing clusters"):
                cluster_id = future_to_cluster[future]
                try:
                    result = future.result()
                    if result:
                        consensus = result['consensus']
                        variants = result['variants']
                        stats = result['statistics']
                        
                        if consensus:
                            consensus_results.append(consensus)
                            
                        if variants:
                            variant_results.extend(variants)
                            
                        # 添加集群ID并保存统计信息
                        stats['cluster_id'] = cluster_id
                        all_stats.append(stats)
                        
                except Exception as e:
                    logger.error(f"Processing cluster {cluster_id} failed: {e}")
        
        # 记录集群处理统计
        self.save_cluster_stats(all_stats)
        
        logger.info(f"Generated {len(consensus_results)} consensus sequences and {len(variant_results)} variant sequences")
        return consensus_results, variant_results, all_stats
        
    def save_cluster_stats(self, stats):
        """Save cluster statistics to file"""
        stats_file = os.path.join(self.output_dir, "cluster_stats.tsv")
        
        try:
            # Convert to DataFrame
            df = pd.DataFrame(stats)
            
            # Sort
            if not df.empty:
                df = df.sort_values(by='cluster_id')
                
                # Save to TSV
                df.to_csv(stats_file, sep='\t', index=False)
                
        except Exception as e:
            logger.warning(f"Failed to save cluster statistics: {e}")

    def run_iteration(self, input_file, iteration):
        """Run a single clustering iteration"""
        logger.info(f"Starting iteration {iteration}...")
        
        # Create iteration output directory (if needed)
        if self.store_iterations:
            iter_dir = os.path.join(self.iterations_dir, f"iteration_{iteration}")
            os.makedirs(iter_dir, exist_ok=True)
        else:
            iter_dir = self.temp_dir
            
        # Preprocess sequences
        if iteration == 1:
            # Complete preprocessing on first iteration
            preprocessed_file = self.preprocess_sequences(input_file)
        else:
            # Use previous results for subsequent iterations
            preprocessed_file = input_file
            
        # Run clustering
        # Adjust thresholds based on iteration number
        if iteration > 1:
            # Relax thresholds as iterations increase to capture more distant relationships
            adjusted_identity = max(0.6, self.identity_threshold - 0.1 * (iteration - 1))
            adjusted_coverage = max(0.5, self.coverage_threshold - 0.1 * (iteration - 1))
            
            logger.info(f"Adjusting thresholds for iteration {iteration}: identity={adjusted_identity:.2f}, coverage={adjusted_coverage:.2f}")
            
            # Temporarily adjust parameters
            original_identity = self.identity_threshold
            original_coverage = self.coverage_threshold
            
            self.identity_threshold = adjusted_identity
            self.coverage_threshold = adjusted_coverage
            
            # Run clustering
            clusters, seq_file = self.run_clustering(preprocessed_file)
            
            # Restore original parameters
            self.identity_threshold = original_identity
            self.coverage_threshold = original_coverage
        else:
            # Use original parameters for first iteration
            clusters, seq_file = self.run_clustering(preprocessed_file)
            
        # Process clusters
        consensus_seqs, variant_seqs, stats = self.process_clusters(clusters, seq_file)
        
        # Save results
        if self.store_iterations:
            iter_consensus_file = os.path.join(iter_dir, "consensus.fa")
            iter_all_file = os.path.join(iter_dir, "all_sequences.fa")
            
            # Save consensus sequences
            SeqIO.write(consensus_seqs, iter_consensus_file, "fasta")
            
            # Save all sequences (consensus + variants)
            SeqIO.write(consensus_seqs + variant_seqs, iter_all_file, "fasta")
            
            # Generate iteration report
            with open(os.path.join(iter_dir, "report.txt"), "w") as f:
                f.write(f"Iteration {iteration} Report\n")
                f.write(f"Input sequence count: {len(list(SeqIO.parse(preprocessed_file, 'fasta')))}\n")
                f.write(f"Identified clusters count: {len(clusters)}\n")
                f.write(f"Generated consensus sequences count: {len(consensus_seqs)}\n")
                f.write(f"Retained variant sequences count: {len(variant_seqs)}\n")
                
                if iteration > 1:
                    f.write(f"Adjusted parameters: identity={adjusted_identity:.2f}, coverage={adjusted_coverage:.2f}\n")
                    
                # Add more statistics
                largest_cluster = max([len(c) for c in clusters]) if clusters else 0
                f.write(f"Largest cluster size: {largest_cluster}\n")
                
                singleton_clusters = sum(1 for c in clusters if len(c) == 1)
                f.write(f"Single-sequence clusters count: {singleton_clusters}\n")
                
        # Create output file for next iteration
        output_file = os.path.join(self.temp_dir, f"iteration_{iteration}_output.fa")
        
        # Decide whether to include variants based on strategy
        if self.keep_variants:
            SeqIO.write(consensus_seqs + variant_seqs, output_file, "fasta")
            return output_file, len(consensus_seqs) + len(variant_seqs)
        else:
            SeqIO.write(consensus_seqs, output_file, "fasta")
            return output_file, len(consensus_seqs)

    def run(self):
        """Run complete LTR redundancy removal process, including iterative optimization"""
        logger.info("Starting enhanced LTR clustering and redundancy removal process...")
        
        try:
            # Check tools
            if not self.check_tools():
                return None
                
            # Iterative optimization
            current_input = self.input_fasta
            prev_seq_count = float('inf')
            current_iteration = 1
            
            while current_iteration <= self.max_iterations:
                # Run current iteration
                new_input, seq_count = self.run_iteration(current_input, current_iteration)
                
                # Check convergence
                if prev_seq_count > 0:
                    reduction_rate = (prev_seq_count - seq_count) / prev_seq_count
                    logger.info(f"Iteration {current_iteration} result: {prev_seq_count} → {seq_count} sequences (reduction rate: {reduction_rate:.2%})")
                    
                    # If sequence count reduction rate is below threshold, consider converged
                    if reduction_rate < self.convergence_threshold:
                        logger.info(f"Iteration converged! Sequence count reduction rate ({reduction_rate:.2%}) below threshold ({self.convergence_threshold:.2%})")
                        break
                else:
                    logger.info(f"Iteration {current_iteration} result: {seq_count} sequences")
                
                # Set up next iteration
                current_input = new_input
                prev_seq_count = seq_count
                current_iteration += 1
                
                # If maximum iteration count reached, exit
                if current_iteration > self.max_iterations:
                    logger.info(f"Maximum iteration count reached ({self.max_iterations})")
                    break
            
            # Generate final output
            final_file = os.path.join(self.output_dir, "consensi.fa")
            final_variants_file = os.path.join(self.output_dir, "variants.fa")
            final_all_file = os.path.join(self.output_dir, "all_sequences.fa")
            
            # Read final results
            final_seqs = list(SeqIO.parse(current_input, "fasta"))
            
            # Separate consensus and variants
            consensus_seqs = []
            variant_seqs = []
            
            for seq in final_seqs:
                if "_variant_" in seq.id:
                    variant_seqs.append(seq)
                else:
                    consensus_seqs.append(seq)
            
            # Save results
            SeqIO.write(consensus_seqs, final_file, "fasta")
            
            if variant_seqs:
                SeqIO.write(variant_seqs, final_variants_file, "fasta")
                
            SeqIO.write(final_seqs, final_all_file, "fasta")
            
            # Generate final report
            self.generate_final_report(
                input_file=self.input_fasta,
                consensus_file=final_file,
                variants_file=final_variants_file,
                iterations=current_iteration - 1
            )
            
            logger.info(f"Processing complete, results saved to: {final_file}")
            logger.info(f"Total {len(consensus_seqs)} consensus sequences and {len(variant_seqs)} variant sequences")
            
            return final_file
            
        except Exception as e:
            logger.error(f"Error occurred during processing: {e}")
            import traceback
            logger.error(f"Exception details: {traceback.format_exc()}")
            return None
            
        finally:
            # Clean up temporary files
            if self.cleanup_temp:
                try:
                    logger.info("Cleaning up temporary files...")
                    shutil.rmtree(self.temp_dir)
                except Exception as e:
                    logger.warning(f"Failed to clean up temporary files: {e}")
                    
    def generate_final_report(self, input_file, consensus_file, variants_file, iterations):
        """Generate final report"""
        report_file = os.path.join(self.output_dir, "final_report.txt")
        
        try:
            # Calculate statistics
            input_seqs = list(SeqIO.parse(input_file, "fasta"))
            consensus_seqs = list(SeqIO.parse(consensus_file, "fasta"))
            
            variant_count = 0
            if os.path.exists(variants_file):
                variant_seqs = list(SeqIO.parse(variants_file, "fasta"))
                variant_count = len(variant_seqs)
                
            # Prepare report content
            report = []
            report.append("# LTR Sequence Clustering and Redundancy Removal Final Report")
            report.append(f"Generation date: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            report.append("\n## Processing Overview")
            report.append(f"Input sequence count: {len(input_seqs)}")
            report.append(f"Generated consensus sequence count: {len(consensus_seqs)}")
            report.append(f"Retained variant sequence count: {variant_count}")
            report.append(f"Completed iteration count: {iterations}")
            report.append(f"Reduction rate: {(len(input_seqs) - len(consensus_seqs)) / len(input_seqs):.2%}")
            
            # Add parameter information
            report.append("\n## Run Parameters")
            report.append(f"Clustering method: {self.cluster_method}")
            report.append(f"Initial identity threshold: {self.identity_threshold}")
            report.append(f"Initial coverage threshold: {self.coverage_threshold}")
            report.append(f"Maximum cluster size: {self.max_cluster_size}")
            report.append(f"Use IUPAC for variants: {self.use_iupac}")
            report.append(f"Keep variants: {self.keep_variants}")
            
            # Add clustering statistics
            if self.cluster_stats:
                report.append("\n## Iteration Statistics")
                for i, stats in enumerate(self.cluster_stats, 1):
                    report.append(f"\n### Iteration {i}")
                    report.append(f"Total clusters: {stats['total_clusters']}")
                    report.append(f"Largest cluster size: {stats['largest_cluster']}")
                    report.append(f"Average cluster size: {stats['avg_cluster_size']:.1f}")
                    report.append(f"Single-sequence cluster count: {stats['singleton_clusters']}")
                    
            # Save report
            with open(report_file, "w") as f:
                f.write("\n".join(report))
                
            logger.info(f"Final report saved to: {report_file}")
                
        except Exception as e:
            logger.warning(f"Failed to generate final report: {e}")

def main():
    """Command line entry function"""
    parser = argparse.ArgumentParser(
        description="Enhanced LTR Sequence Clustering and Redundancy Removal Tool - Reducing Information Loss and Increasing Evolutionary Awareness"
    )
    
    parser.add_argument("input_fasta", help="Input FASTA file")
    parser.add_argument("output_dir", help="Output directory")
    
    parser.add_argument("--method", choices=["minimap2", "mmseqs", "vsearch", "cdhit"], 
                       default="minimap2", help="Clustering method (default: minimap2)")
    parser.add_argument("--identity", type=float, default=0.8,
                       help="Sequence identity threshold (0-1, default: 0.8)")
    parser.add_argument("--coverage", type=float, default=0.8,
                       help="Sequence coverage threshold (0-1, default: 0.8)")
    parser.add_argument("--threads", type=int, default=1,
                       help="Number of threads (default: 1)")
    parser.add_argument("--keep-temp", action="store_false", dest="cleanup_temp",
                       help="Keep temporary files")
    parser.add_argument("--max-iterations", type=int, default=3,
                        help="Maximum number of iterations (default: 3)")
    parser.add_argument("--convergence", type=float, default=0.05,
                        help="Convergence threshold, stop when sequence count change rate is below this value (default: 0.05)")
    parser.add_argument("--max-cluster-size", type=int, default=50,
                        help="Maximum cluster size before splitting (default: 50)")
    parser.add_argument("--use-iupac", action="store_true", default=True,
                        help="Use IUPAC ambiguity codes for variable positions (default: enabled)")
    parser.add_argument("--no-iupac", action="store_false", dest="use_iupac",
                        help="Don't use IUPAC ambiguous bases")
    parser.add_argument("--keep-variants", action="store_true", default=True,
                        help="Keep subfamily representative variants (default: enabled)")
    parser.add_argument("--no-variants", action="store_false", dest="keep_variants",
                        help="Don't keep subfamily representative variants")
    parser.add_argument("--store-iterations", action="store_true", default=True,
                        help="Save intermediate iteration results (default: enabled)")
    parser.add_argument("--no-store-iterations", action="store_false", dest="store_iterations",
                        help="Don't save intermediate iteration results")
    
    args = parser.parse_args()
    
    pipeline = FastClusteringPipeline(
        args.input_fasta,
        args.output_dir,
        cluster_method=args.method,
        threads=args.threads,
        identity_threshold=args.identity,
        coverage_threshold=args.coverage,
        cleanup_temp=args.cleanup_temp,
        max_iterations=args.max_iterations,
        convergence_threshold=args.convergence,
        max_cluster_size=args.max_cluster_size,
        keep_variants=args.keep_variants,
        store_iterations=args.store_iterations,
        use_iupac=args.use_iupac
    )
    
    pipeline.run()
    
if __name__ == "__main__":
    main()
