#!/usr/bin/env python3

import os
import sys
import logging
import argparse
import subprocess
import time
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from typing import Dict, Optional

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    handlers=[
        logging.FileHandler('pan_te.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class PanTEPipeline:
    """Implementation of the Pan TE Pipeline with step tracking and checkpoints."""
    
    def __init__(self, genome_file: Optional[str], output_dir: str, threads: int = 4, 
                 model_dir: Optional[str] = None, vcf_dir: Optional[str] = None):
        """Initialize the pipeline with configuration parameters."""
        self.genome_file = os.path.abspath(genome_file) if genome_file else None
        self.vcf_dir = os.path.abspath(vcf_dir) if vcf_dir else None
        self.output_dir = os.path.abspath(output_dir)
        self.threads = threads
        self.model_dir = os.path.abspath(model_dir)
        self.processed_genome = os.path.join(self.output_dir, 'genome', 'genome.fa')

        # Verify inputs
        if not self.genome_file:
            raise ValueError("genome_file must be provided")
        if self.vcf_dir and not self.genome_file:
            raise ValueError("genome_file must be provided when using vcf_dir")
        
        self.ltr_threads = max(1, threads - 1)
        self.rs_threads = max(1, threads)

        # Pipeline steps and their checkpoints
        self.steps = {
            'genome': 'genome.ok',
            'ltr': 'LTR.ok',
            'repeatscout': 'RepeatScout.ok',
            'recon': 'RECON.ok',
            'combine': 'Combine.ok'
        }
        self.processes = {
            'genome': self.process_genome,
            'ltr': self.process_ltr,
            'repeatscout': self.process_repeatscout,
            'recon': self.process_recon,
            'combine': self.combine_results
        }
        # Create output directory structure
        self.setup_directories()

    def setup_directories(self):
        """Create the necessary directory structure."""
        directories = [
            self.output_dir,
            os.path.join(self.output_dir, 'genome'),
            os.path.join(self.output_dir, 'look4ltrs'),
            os.path.join(self.output_dir, 'RepeatScout'),
            os.path.join(self.output_dir, 'RECON'),
            os.path.join(self.output_dir, 'Combine')
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
            logger.info(f"Created directory: {directory}")

    def create_checkpoint(self, step: str):
        """Create a checkpoint file for a completed step."""
        checkpoint_file = os.path.join(self.output_dir, self.steps[step])
        Path(checkpoint_file).touch()
        logger.info(f"Created checkpoint for {step}: {checkpoint_file}")

    def check_checkpoint(self, step: str) -> bool:
        """Check if a step's checkpoint exists."""
        return os.path.exists(os.path.join(self.output_dir, self.steps[step]))

    def run_cmd(self, cmd: str, workdir: Optional[str] = None) -> subprocess.CompletedProcess:
        """Run shell command with error handling and logging."""
        start_time = time.time()
        current_dir = os.getcwd()
        
        try:
            if workdir:
                os.chdir(workdir)
                
            logger.info(f"Running command: {cmd}")
            result = subprocess.run(
                cmd,
                shell=True,
                check=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                encoding='utf-8'
            )
            
            duration = time.time() - start_time
            logger.info(f"Command completed in {duration:.2f} seconds")
            return result
            
        except subprocess.CalledProcessError as e:
            duration = time.time() - start_time
            logger.error(f"Command failed after {duration:.2f} seconds")
            logger.error(f"Error output: {e.stderr}")
            raise
            
        finally:
            if workdir:
                os.chdir(current_dir)

    def process_genome(self):
        """Step 1: Process genome."""
        if self.check_checkpoint('genome'):
            logger.info("Genome processing already completed, skipping...")
            return
            
        genome_dir = os.path.join(self.output_dir, 'genome')
        processed_genome = os.path.join(genome_dir, 'genome.fa')
        
        if self.vcf_dir:
            # Process both VCF and genome file
            logger.info("Processing both VCF and genome files")
            vcf_result = os.path.join(genome_dir, 'vcf_sequences.fa')
            genome_result = os.path.join(genome_dir, 'genome_sequence.fa')
            
            # Process genome file
            self.process_single_genome(genome_result)
            
            # Process VCF files
            self.process_graph_genome(vcf_result)
            
            # Combine both results
            self.combine_genome_results(genome_dir, [vcf_result, genome_result])
        else:
            # Process only genome file
            logger.info("Processing single genome file")
            self.process_single_genome(processed_genome)
        
        # Update genome file path to processed version
        self.genome_file = processed_genome
        self.create_checkpoint('genome')
        logger.info("Genome processing completed")

    def process_ltr(self):
        """Process LTRs using allocated threads."""
        if self.check_checkpoint('ltr'):
            logger.info("LTR processing already completed")
            return
            
        logger.info(f"Starting LTR processing with {self.ltr_threads} threads")
        ltr_dir = os.path.join(self.output_dir, 'look4ltrs')
        os.makedirs(ltr_dir, exist_ok=True)
        
        # Use the processed genome file from genome directory
        genome_file = os.path.join(self.output_dir, 'genome', 'genome.fa')
        if not os.path.exists(genome_file):
            raise FileNotFoundError(f"Processed genome file not found: {genome_file}")
        
        # First create genome index if it doesn't exist
        if not os.path.exists(f"{genome_file}.fai"):
            self.run_cmd(f"samtools faidx {genome_file}")
        
        # Run para_look4ltrs with the processed genome file
        cmd = f"para_look4ltrs {self.processed_genome} {self.ltr_threads} {self.output_dir}"
        self.run_cmd(cmd, ltr_dir)
        
        self.create_checkpoint('ltr')
        logger.info("LTR processing completed")

    def process_repeatscout(self):
        """Process RepeatScout with allocated threads."""
        if self.check_checkpoint('repeatscout'):
            logger.info("RepeatScout processing already completed")
            return
            
        logger.info(f"Starting RepeatScout with {self.rs_threads} threads")
        rs_dir = os.path.join(self.output_dir, 'RepeatScout')
        tmp_dir = os.path.join(rs_dir, 'tmp')
        os.makedirs(tmp_dir, exist_ok=True)
        processed_genome = os.path.join(self.output_dir, 'genome', 'genome.fa')
        cmd = (f"build_RS --genome {processed_genome} --threads {self.rs_threads} "
               f"--tmp {tmp_dir} --workdir {rs_dir} ")
        
        if hasattr(self, 'rmblast_dir'):
            cmd += f" --rmblast {self.rmblast_dir}"
            
        self.run_cmd(cmd, rs_dir)
        self.create_checkpoint('repeatscout')
        logger.info("RepeatScout processing completed")

    def process_recon(self):
        """Step 4: Process RECON."""
        if self.check_checkpoint('recon'):
            logger.info("RECON processing already completed")
            return
            
        logger.info("Starting RECON processing")
        recon_dir = os.path.join(self.output_dir, 'RECON')
        
        self.run_cmd(f"cat ../RepeatScout/consensi.fa ../look4ltrs/consensi.fa > raw.fa", recon_dir)
        
        genome_size = os.path.getsize(self.processed_genome)

        if genome_size > 10_000_000_000:  # HUGE_GENOME_SIZE
            sample_percent = 30
            batch_size = 200000
        elif genome_size > 5_000_000_000:  # BIG_GENOME_SIZE
            sample_percent = 60
            batch_size = 200000
        elif genome_size > 1_000_000_000:  # MEDIUM_GENOME_SIZE
            sample_percent = 70
            batch_size = 100000
        elif genome_size > 500_000_000:  # SMALL_GENOME_SIZE
            sample_percent = 80
            batch_size = 80000
        else:  # TINY_GENOME_SIZE
            sample_percent = 90
            batch_size = 40000
        
        sample_size = min(int(genome_size * sample_percent / 100), 5_000_000_000)
        
        cmd = f"run_RECON 1 {self.threads} {self.processed_genome} {sample_size} {batch_size}"
        self.run_cmd(cmd, recon_dir)
        
        self.create_checkpoint('recon')

    def combine_results(self):
        """Step 5: Combine and classify results."""
        if self.check_checkpoint('combine'):
            logger.info("Results combination already completed, skipping...")
            return
            
        if not self.model_dir:
            logger.warning("No model directory specified, skipping classification")
            return
            
        logger.info("Starting results combination and classification")
        combine_dir = os.path.join(self.output_dir, 'Combine')
        
        # Combine results
        self.run_cmd("cat ../RECON/round-1/lib.fa ../RECON/round-1/consensi.fa > raw_TEs.fa",
                    combine_dir)
        
        # Run classifier
        self.run_cmd(f"run_Classifier {self.model_dir}", combine_dir)
        
        self.create_checkpoint('combine')
        logger.info("Results combination completed")

    def process_single_genome(self, output_file: str):
        """Process a single genome file."""
        logger.info("Starting single genome processing")
        
        # Process genome file
        tmp_fa = output_file + '.tmp'
        self.run_cmd(f"tr 'a-z' 'A-Z' < {self.genome_file} > {tmp_fa}")
        self.run_cmd(f"clean_seq {tmp_fa} {output_file}")
        
        # Index genome
        self.run_cmd(f"samtools faidx {output_file}")
        self.run_cmd(f"index {output_file}") 

        os.remove(tmp_fa)
        logger.info("Single genome processing completed")

    def process_graph_genome(self, output_file: str):
        """Process genome from VCF files."""
        logger.info("Starting graph genome processing from VCFs")
        
        if not self.vcf_dir:
            raise ValueError("VCF directory not specified")
            
        tmp_dir = os.path.join(os.path.dirname(output_file), 'tmp')
        os.makedirs(tmp_dir, exist_ok=True)
        
        # Get list of VCF files
        vcf_files = list(Path(self.vcf_dir).glob('*.vcf'))
        if not vcf_files:
            raise FileNotFoundError(f"No VCF files found in {self.vcf_dir}")
            
        logger.info(f"Found {len(vcf_files)} VCF files")
        
        # Process each VCF file
        for vcf_file in vcf_files:
            try:
                cmd = f"decode_gfa.pl --vcf {vcf_file} --threads {self.threads} --out {tmp_dir}"
                self.run_cmd(cmd)
                logger.info(f"Successfully processed {vcf_file}")
            except Exception as e:
                logger.error(f"Failed to process {vcf_file}: {str(e)}")
                raise
        
        # Combine results
        self.run_cmd(f"cat {tmp_dir}/*.fa > {output_file}")
        
        # Cleanup
        import shutil
        shutil.rmtree(tmp_dir)
        logger.info("Graph genome processing completed")

    def combine_genome_results(self, genome_dir: str, input_files: list):
        """Combine and process all genome sequences into final genome.fa."""
        logger.info("Combining genome results")
        
        # Combine all input files
        combined_fa = os.path.join(genome_dir, 'combined.fa')
        with open(combined_fa, 'w') as outfile:
            for infile in input_files:
                with open(infile) as indata:
                    outfile.write(indata.read())
        
        # Process combined file
        tmp_fa = os.path.join(genome_dir, 'tmp.fa')
        self.run_cmd(f"tr 'a-z' 'A-Z' < {combined_fa} > {tmp_fa}")
        self.run_cmd(f"clean_seq {tmp_fa} {os.path.join(genome_dir, 'genome.fa')}")
        self.run_cmd(f"samtools faidx {os.path.join(genome_dir, 'genome.fa')}")
        self.run_cmd(f"index {os.path.join(genome_dir, 'genome')}")
        
        # Cleanup
        os.remove(tmp_fa)
        os.remove(combined_fa)
        for file in input_files:
            os.remove(file)
        
        logger.info("Genome results combined successfully")

    def run(self):
        """Run complete pipeline."""
        try:
            # Process genome first
            if not self.check_checkpoint('genome'):
                self.process_genome()
                
            # Run parallel detection after genome processing
            logger.info("Starting parallel TE detection")
            with ThreadPoolExecutor(max_workers=2) as executor:
                ltr_future = executor.submit(self.process_ltr)
                rs_future = executor.submit(self.process_repeatscout)
                
                # Wait for both to complete
                for future in [ltr_future, rs_future]:
                    try:
                        future.result()
                    except Exception as e:
                        logger.error(f"Detection task failed: {str(e)}")
                        raise
            
            # Run remaining steps
            if not self.check_checkpoint('recon'):
                self.process_recon()
                
            if not self.check_checkpoint('combine'):
                self.combine_results()
                
        except Exception as e:
            logger.error(f"Pipeline failed: {str(e)}")
            raise

def main():
    parser = argparse.ArgumentParser(description='Pan TE Pipeline')
    parser.add_argument('--genome', required=True, help='Input genome file')
    parser.add_argument('--out', required=True, help='Output directory')
    parser.add_argument('--threads', type=int, default=4, help='Number of threads')
    parser.add_argument('--model-dir', help='ClassifyTE model directory')
    parser.add_argument('--vcf-dir', help='Directory containing VCF files')
    
    args = parser.parse_args()
    
    try:
        # Check if at least one input source is provided
        if not (args.genome or args.vcf_dir):
            parser.error("Either --genome or --vcf-dir must be provided")
            
        pipeline = PanTEPipeline(
            genome_file=args.genome,
            output_dir=args.out,
            threads=args.threads,
            model_dir=args.model_dir,
            vcf_dir=args.vcf_dir
        )
        pipeline.run()
        logger.info("Pipeline completed successfully")
        
    except Exception as e:
        logger.error(f"Pipeline failed: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
