#!/usr/bin/env python3

import os
import sys
import logging
import argparse
import subprocess
import time
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from typing import Dict, Optional
from vcf_processor import VCFProcessor  

VERSION = '1.0.0'

# Usage and help message
USAGE = f'''Usage:
perl $0 [options]

Example:
    perl $0 --genome genome.fasta --threads 80 --model-dir Your_Path_To_ClassifyTE 

Parameters:
[General]
    --genome <string>         Required. Genome file in FASTA format.
    --model-dir <string>      Provide path to ClassifyTE for classification.

[Other]
    --vcf-dir <string>        Default: NA. Path for VCF, see gfa.list for format.
    --out <string>           Default: current directory. The work directory.
    -M <int>                 Memory limit (in MB), default: 0 (unlimited).
    --threads <int>          Default: 4. Number of threads, preferably in multiples of 4.
    --fragment_size <int>    Default: 40000. Length for fragment.
    --help|-h                Display this help information.

Version: {VERSION}
'''

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    handlers=[
        logging.FileHandler('pan_te.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class CustomArgumentParser(argparse.ArgumentParser):
    def format_help(self):
        return USAGE

    def error(self, message):
        sys.stderr.write(f'error: {message}\n\n')
        self.print_help()
        sys.exit(2)

def parse_arguments():
    """Parse command line arguments with custom help format."""
    parser = CustomArgumentParser(add_help=False)
    
    # General options
    parser.add_argument('--genome', 
                       type=str,
                       help='Genome file in FASTA format')
    parser.add_argument('--model-dir',
                       type=str,
                       help='Path to ClassifyTE for classification')
    
    # Other options
    parser.add_argument('--vcf-dir',
                       type=str,
                       help='Path for VCF files')
    parser.add_argument('--out',
                       type=str,
                       default=os.getcwd(),
                       help='Work directory')
    parser.add_argument('-M',
                       type=int,
                       default=0,
                       help='Memory limit in MB')
    parser.add_argument('--threads',
                       type=int,
                       default=4,
                       help='Number of threads')
    parser.add_argument('--fragment_size',
                       type=int,
                       default=40000,
                       help='Length for fragment')
    parser.add_argument('--help', '-h',
                       action='help',
                       help='Display help information')
    
    args = parser.parse_args()
    
    # Validate required arguments
    if not args.genome:
        parser.error('--genome is required')
        
    return args

class PanTEPipeline:
    """Implementation of the Pan TE Pipeline with step tracking and checkpoints."""
    
    def __init__(self, genome_file: Optional[str], output_dir: str, threads: int = 4, 
                 model_dir: Optional[str] = None, vcf_dir: Optional[str] = None):
        """Initialize the pipeline with configuration parameters."""
        self.genome_file = os.path.abspath(genome_file) if genome_file else None
        self.vcf_dir = os.path.abspath(vcf_dir) if vcf_dir else None
        self.output_dir = os.path.abspath(output_dir)
        self.threads = threads
        self.model_dir = os.path.abspath(model_dir)
        self.processed_genome = os.path.join(self.output_dir, 'genome', 'genome.fa')

        # Verify inputs
        if not self.genome_file:
            raise ValueError("genome_file must be provided")
        # Verify inputs
        if not self.genome_file and not self.vcf_dir:
            raise ValueError("Either genome_file or vcf_dir must be provided")
        
        self.ltr_threads = max(1, threads - 1)
        self.rs_threads = max(1, threads)

        # Pipeline steps and their checkpoints
        self.steps = {
            'genome': 'genome.ok',
            'ltr': 'LTR.ok',
            'repeatscout': 'RepeatScout.ok',
            'recon': 'RECON.ok',
            'combine': 'Combine.ok'
        }
        self.processes = {
            'genome': self.process_genome,
            'ltr': self.process_ltr,
            'repeatscout': self.process_repeatscout,
            'recon': self.process_recon,
            'combine': self.combine_results
        }
        # Create output directory structure
        self.setup_directories()

    def setup_directories(self):
        """Create the necessary directory structure."""
        directories = [
            self.output_dir,
            os.path.join(self.output_dir, 'genome'),
            os.path.join(self.output_dir, 'look4ltrs'),
            os.path.join(self.output_dir, 'RepeatScout'),
            os.path.join(self.output_dir, 'RECON'),
            os.path.join(self.output_dir, 'Combine')
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
            logger.info(f"Created directory: {directory}")

    def create_checkpoint(self, step: str):
        """Create a checkpoint file for a completed step."""
        checkpoint_file = os.path.join(self.output_dir, self.steps[step])
        Path(checkpoint_file).touch()
        logger.info(f"Created checkpoint for {step}: {checkpoint_file}")

    def check_checkpoint(self, step: str) -> bool:
        """Check if a step's checkpoint exists."""
        return os.path.exists(os.path.join(self.output_dir, self.steps[step]))

    def run_cmd(self, cmd: str, workdir: Optional[str] = None) -> subprocess.CompletedProcess:
        """Run shell command with error handling and logging."""
        start_time = time.time()
        current_dir = os.getcwd()
        
        try:
            if workdir:
                os.chdir(workdir)
                
            logger.info(f"Running command: {cmd}")
            result = subprocess.run(
                cmd,
                shell=True,
                check=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                encoding='utf-8'
            )
            
            duration = time.time() - start_time
            logger.info(f"Command completed in {duration:.2f} seconds")
            return result
            
        except subprocess.CalledProcessError as e:
            duration = time.time() - start_time
            logger.error(f"Command failed after {duration:.2f} seconds")
            logger.error(f"Error output: {e.stderr}")
            raise
            
        finally:
            if workdir:
                os.chdir(current_dir)

    def process_genome(self):
        """Step 1: Process genome and VCF files if provided."""
        if self.check_checkpoint('genome'):
            logger.info("Genome processing already completed, skipping...")
            return
                
        genome_dir = os.path.join(self.output_dir, 'genome')
        processed_genome = os.path.join(genome_dir, 'genome.fa')
        os.makedirs(genome_dir, exist_ok=True)
            
        # Step 1: Process VCF files if provided
        combined_input = None
        if self.vcf_dir:
            logger.info("Processing VCF files...")
            vcf_processor = VCFProcessor(
                vcf_dir=self.vcf_dir,
                output_dir=genome_dir,
                total_threads=self.threads
            )
            try:
                vcf_sequences = vcf_processor.process_all_vcfs()
                logger.info(f"VCF processing completed: {vcf_sequences}")
                
                # Step 2: Combine VCF results with input genome
                logger.info("Combining VCF results with input genome...")
                combined_input = os.path.join(genome_dir, 'combined_input.fa')
                with open(combined_input, 'w') as outfile:
                    # First write VCF sequences
                    with open(vcf_sequences) as infile:
                        outfile.write(infile.read())
                    # Then write genome sequences
                    with open(self.genome_file) as infile:
                        outfile.write(infile.read())
                
                # Clean up VCF results file
                os.remove(vcf_sequences)
                
                # Update genome file path to combined file for processing
                self.genome_file = combined_input
                
            except Exception as e:
                logger.error(f"VCF processing failed: {str(e)}")
                raise

        # Step 3: Process the genome file (either combined or original)
        logger.info("Processing genome file...")
        self.process_single_genome(processed_genome)

        # Clean up temporary files
        if combined_input and os.path.exists(combined_input):
            os.remove(combined_input)

        # Update genome file path to processed version
        self.genome_file = processed_genome
        self.create_checkpoint('genome')
        logger.info("Genome processing completed")

    def process_single_genome(self, output_file: str):
        """Process a single genome file."""
        logger.info("Starting single genome processing")
        
        # Process genome file
        tmp_fa = output_file + '.tmp'
        self.run_cmd(f"tr 'a-z' 'A-Z' < {self.genome_file} > {tmp_fa}")
        self.run_cmd(f"clean_seq {tmp_fa} {output_file}")
        
        # Index genome
        self.run_cmd(f"samtools faidx {output_file}")
        self.run_cmd(f"index {output_file}") 

        os.remove(tmp_fa)
        logger.info("Single genome processing completed")

    def process_ltr(self):
        """Process LTRs using allocated threads."""
        if self.check_checkpoint('ltr'):
            logger.info("LTR processing already completed")
            return
            
        logger.info(f"Starting LTR processing with {self.ltr_threads} threads")
        ltr_dir = os.path.join(self.output_dir, 'look4ltrs')
        os.makedirs(ltr_dir, exist_ok=True)
        
        # Use the processed genome file from genome directory
        genome_file = os.path.join(self.output_dir, 'genome', 'genome.fa')
        if not os.path.exists(genome_file):
            raise FileNotFoundError(f"Processed genome file not found: {genome_file}")
        
        # First create genome index if it doesn't exist
        if not os.path.exists(f"{genome_file}.fai"):
            self.run_cmd(f"samtools faidx {genome_file}")
        
        # Run para_look4ltrs with the processed genome file
        cmd = f"para_look4ltrs {self.processed_genome} {self.ltr_threads} {self.output_dir}"
        self.run_cmd(cmd, ltr_dir)
        
        self.create_checkpoint('ltr')
        logger.info("LTR processing completed")

    def process_repeatscout(self):
        """Process RepeatScout with allocated threads."""
        if self.check_checkpoint('repeatscout'):
            logger.info("RepeatScout processing already completed")
            return
            
        logger.info(f"Starting RepeatScout with {self.rs_threads} threads")
        rs_dir = os.path.join(self.output_dir, 'RepeatScout')
        tmp_dir = os.path.join(rs_dir, 'tmp')
        os.makedirs(tmp_dir, exist_ok=True)
        processed_genome = os.path.join(self.output_dir, 'genome', 'genome.fa')
        cmd = (f"build_RS --genome {processed_genome} --threads {self.rs_threads} "
               f"--tmp {tmp_dir} --workdir {rs_dir} ")
        
        if hasattr(self, 'rmblast_dir'):
            cmd += f" --rmblast {self.rmblast_dir}"
            
        self.run_cmd(cmd, rs_dir)
        self.create_checkpoint('repeatscout')
        logger.info("RepeatScout processing completed")

    def process_recon(self):
        """Step 4: Process RECON."""
        if self.check_checkpoint('recon'):
            logger.info("RECON processing already completed")
            return
            
        logger.info("Starting RECON processing")
        recon_dir = os.path.join(self.output_dir, 'RECON')
        
        self.run_cmd(f"cat ../RepeatScout/consensi.fa ../look4ltrs/consensi.fa > raw.fa", recon_dir)
        
        genome_size = os.path.getsize(self.processed_genome)

        if genome_size > 10_000_000_000:  # HUGE_GENOME_SIZE
            sample_percent = 30
            batch_size = 200000
        elif genome_size > 5_000_000_000:  # BIG_GENOME_SIZE
            sample_percent = 60
            batch_size = 200000
        elif genome_size > 1_000_000_000:  # MEDIUM_GENOME_SIZE
            sample_percent = 70
            batch_size = 100000
        elif genome_size > 500_000_000:  # SMALL_GENOME_SIZE
            sample_percent = 80
            batch_size = 80000
        else:  # TINY_GENOME_SIZE
            sample_percent = 90
            batch_size = 40000
        
        sample_size = min(int(genome_size * sample_percent / 100), 5_000_000_000)
        
        cmd = f"run_RECON 1 {self.threads} {self.processed_genome} {sample_size} {batch_size}"
        self.run_cmd(cmd, recon_dir)
        
        self.create_checkpoint('recon')

    def combine_results(self):
        """Step 5: Combine and classify results."""
        if self.check_checkpoint('combine'):
            logger.info("Results combination already completed, skipping...")
            return
            
        if not self.model_dir:
            logger.warning("No model directory specified, skipping classification")
            return
            
        logger.info("Starting results combination and classification")
        combine_dir = os.path.join(self.output_dir, 'Combine')
        
        # Combine results
        self.run_cmd("cat ../RECON/round-1/lib.fa ../RECON/round-1/consensi.fa > raw_TEs.fa",
                    combine_dir)
        
        # Run classifier
        self.run_cmd(f"run_Classifier {self.model_dir}", combine_dir)
        
        self.create_checkpoint('combine')
        logger.info("Results combination completed")

    def process_single_genome(self, output_file: str):
        """Process a single genome file."""
        logger.info("Starting single genome processing")
        
        # Process genome file
        tmp_fa = output_file + '.tmp'
        self.run_cmd(f"tr 'a-z' 'A-Z' < {self.genome_file} > {tmp_fa}")
        self.run_cmd(f"clean_seq {tmp_fa} {output_file}")
        
        # Index genome
        self.run_cmd(f"samtools faidx {output_file}")
        self.run_cmd(f"index {output_file}") 

        os.remove(tmp_fa)
        logger.info("Single genome processing completed")

    def combine_genome_results(self, genome_dir: str, input_files: list):
        """Combine and process all genome sequences into final genome.fa."""
        logger.info("Combining genome results")
        
        # Combine all input files
        combined_fa = os.path.join(genome_dir, 'combined.fa')
        with open(combined_fa, 'w') as outfile:
            for infile in input_files:
                with open(infile) as indata:
                    outfile.write(indata.read())
        
        # Process combined file
        tmp_fa = os.path.join(genome_dir, 'tmp.fa')
        self.run_cmd(f"tr 'a-z' 'A-Z' < {combined_fa} > {tmp_fa}")
        self.run_cmd(f"clean_seq {tmp_fa} {os.path.join(genome_dir, 'genome.fa')}")
        self.run_cmd(f"samtools faidx {os.path.join(genome_dir, 'genome.fa')}")
        self.run_cmd(f"index {os.path.join(genome_dir, 'genome')}")
        
        # Cleanup
        os.remove(tmp_fa)
        os.remove(combined_fa)
        for file in input_files:
            os.remove(file)
        
        logger.info("Genome results combined successfully")

    def run(self):
        """Run complete pipeline."""
        try:
            # Process genome first
            if not self.check_checkpoint('genome'):
                self.process_genome()
                
            # Run parallel detection after genome processing
            logger.info("Starting parallel TE detection")
            with ThreadPoolExecutor(max_workers=2) as executor:
                ltr_future = executor.submit(self.process_ltr)
                rs_future = executor.submit(self.process_repeatscout)
                
                # Wait for both to complete
                for future in [ltr_future, rs_future]:
                    try:
                        future.result()
                    except Exception as e:
                        logger.error(f"Detection task failed: {str(e)}")
                        raise
            
            # Run remaining steps
            if not self.check_checkpoint('recon'):
                self.process_recon()
                
            if not self.check_checkpoint('combine'):
                self.combine_results()
                
        except Exception as e:
            logger.error(f"Pipeline failed: {str(e)}")
            raise

def main():
    """Main execution function."""
    try:
        # Parse arguments
        args = parse_arguments()
        
        # Create pipeline instance
        pipeline = PanTEPipeline(
            genome_file=args.genome,
            output_dir=args.out,
            threads=args.threads,
            model_dir=args.model_dir,
            vcf_dir=args.vcf_dir
        )

        # Set memory limit if specified
        if args.M > 0:
            import resource
            resource.setrlimit(resource.RLIMIT_AS, (args.M * 1024 * 1024, -1))
            logger.info(f"Set memory limit to {args.M} MB")

        # Set fragment size
        if hasattr(pipeline, 'fragment_size'):
            pipeline.fragment_size = args.fragment_size
            logger.info(f"Set fragment size to {args.fragment_size}")

        # Run pipeline
        pipeline.run()
        logger.info("Pipeline completed successfully")
        
    except KeyboardInterrupt:
        logger.error("Pipeline interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Pipeline failed: {str(e)}")
        if logging.getLogger().isEnabledFor(logging.DEBUG):
            logger.exception("Detailed error traceback:")
        sys.exit(1)

if __name__ == "__main__":
    main()
