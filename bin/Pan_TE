#!/usr/bin/env python3

import os
import sys
import logging
import argparse
import subprocess
import time
import gzip
import bz2
import tempfile
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from typing import Dict, Optional
from vcf_processor import VCFProcessor  

VERSION = '1.0.0'

# Usage and help message
USAGE = f'''Usage:
perl $0 [options]

Example:
    perl $0 --genome genome.fasta --threads 80 --model-dir Your_Path_To_ClassifyTE 

Parameters:
[General]
    --genome <string>         Required. Genome file in FASTA format (.fa, .fasta, .fna, .fas), optionally compressed (.gz, .bz2).
    --model-dir <string>      Provide path to ClassifyTE for classification.

[Other]
    --vcf-dir <string>        Default: NA. Path for VCF, see gfa.list for format.
    --out <string>           Default: current directory. The work directory.
    -M <int>                 Memory limit (in MB), default: 0 (unlimited).
    --threads <int>          Default: 4. Number of threads, preferably in multiples of 4.
    --fragment_size <int>    Default: 40000. Length for fragment.
    --help|-h                Display this help information.

Version: {VERSION}
'''

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    handlers=[
        logging.FileHandler('pan_te.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def detect_compression_format(file_path: str) -> str:
    """
    Detect the compression format of a file.
    
    Returns:
        'gzip' for .gz files
        'bzip2' for .bz2 files  
        'uncompressed' for regular files
    """
    file_path_lower = file_path.lower()
    
    if file_path_lower.endswith('.gz'):
        return 'gzip'
    elif file_path_lower.endswith('.bz2'):
        return 'bzip2'
    else:
        return 'uncompressed'

def is_fasta_file(file_path: str) -> bool:
    """
    Check if a file is a FASTA file based on extension.
    Supports both compressed and uncompressed formats.
    """
    file_path_lower = file_path.lower()
    
    # Remove compression extensions first
    if file_path_lower.endswith('.gz'):
        file_path_lower = file_path_lower[:-3]
    elif file_path_lower.endswith('.bz2'):
        file_path_lower = file_path_lower[:-4]
    
    # Check for FASTA extensions
    return file_path_lower.endswith(('.fa', '.fasta', '.fna', '.fas'))

def decompress_file(input_file: str, output_file: str = None) -> str:
    """
    Decompress a file if it's compressed, otherwise return original path.
    
    Args:
        input_file: Path to potentially compressed file
        output_file: Optional output path for decompressed file
        
    Returns:
        Path to decompressed file (or original if not compressed)
    """
    compression_format = detect_compression_format(input_file)
    
    if compression_format == 'uncompressed':
        return input_file
    
    # Create temp file if no output specified
    if output_file is None:
        temp_fd, output_file = tempfile.mkstemp(suffix='.fa', prefix='pan_te_genome_')
        os.close(temp_fd)
    
    logger.info(f"Decompressing {compression_format} file: {input_file}")
    
    try:
        if compression_format == 'gzip':
            with gzip.open(input_file, 'rt') as infile, open(output_file, 'w') as outfile:
                outfile.write(infile.read())
        elif compression_format == 'bzip2':
            with bz2.open(input_file, 'rt') as infile, open(output_file, 'w') as outfile:
                outfile.write(infile.read())
        
        logger.info(f"Decompression completed: {output_file}")
        return output_file
        
    except Exception as e:
        # Clean up temp file on error
        if output_file and os.path.exists(output_file):
            os.remove(output_file)
        logger.error(f"Failed to decompress {input_file}: {str(e)}")
        raise

class CustomArgumentParser(argparse.ArgumentParser):
    def format_help(self):
        return USAGE

    def error(self, message):
        sys.stderr.write(f'error: {message}\n\n')
        self.print_help()
        sys.exit(2)

def parse_arguments():
    """Parse command line arguments with custom help format."""
    parser = CustomArgumentParser(add_help=False)
    
    # General options
    parser.add_argument('--genome', 
                       type=str,
                       help='Genome file in FASTA format')
    parser.add_argument('--model-dir',
                       type=str,
                       help='Path to ClassifyTE for classification')
    
    # Other options
    parser.add_argument('--vcf-dir',
                       type=str,
                       help='Path for VCF files')
    parser.add_argument('--out',
                       type=str,
                       default=os.getcwd(),
                       help='Work directory')
    parser.add_argument('-M',
                       type=int,
                       default=0,
                       help='Memory limit in MB')
    parser.add_argument('--threads',
                       type=int,
                       default=4,
                       help='Number of threads')
    parser.add_argument('--fragment_size',
                       type=int,
                       default=40000,
                       help='Length for fragment')
    parser.add_argument('--help', '-h',
                       action='help',
                       help='Display help information')
    
    args = parser.parse_args()
    
    # Validate required arguments
    if not args.genome:
        parser.error('--genome is required')
    
    # Validate genome file exists and is FASTA format
    if args.genome:
        if not os.path.exists(args.genome):
            parser.error(f'Genome file not found: {args.genome}')
        if not is_fasta_file(args.genome):
            parser.error(f'Genome file must be in FASTA format (.fa, .fasta, .fna, .fas), optionally compressed (.gz, .bz2): {args.genome}')
        
    return args

class PanTEPipeline:
    """Implementation of the Pan TE Pipeline with step tracking and checkpoints."""
    
    def __init__(self, genome_file: Optional[str], output_dir: str, threads: int = 4,
                 model_dir: Optional[str] = None, vcf_dir: Optional[str] = None):
        """Initialize the pipeline with configuration parameters."""
        self.original_genome_file = os.path.abspath(genome_file) if genome_file else None
        self.genome_file = None  # Will be set after decompression if needed
        self.decompressed_genome_file = None  # Track decompressed temp file for cleanup
        self.vcf_dir = os.path.abspath(vcf_dir) if vcf_dir else None
        self.output_dir = os.path.abspath(output_dir)
        self.threads = threads
        self.model_dir = os.path.abspath(model_dir) if model_dir else None
        self.processed_genome = os.path.join(self.output_dir, 'genome', 'genome.fa')

        # Set genome_file initially to original file path for validation
        # Actual decompression will happen in process_genome() only if needed
        if self.original_genome_file:
            self.genome_file = self.original_genome_file

        # Verify inputs
        if not self.genome_file and not self.vcf_dir:
            raise ValueError("Either genome_file or vcf_dir must be provided")
        
        # Calculate chunk count for dynamic thread allocation
        self.chunk_count = self.calculate_chunk_count()
        self.ltr_threads = max(1, threads - (self.chunk_count * 3))  # Each RS chunk uses 3 threads
        self.rs_threads = max(1, threads)
        
        logger.info(f"Thread allocation: Total={threads}, Chunks={self.chunk_count}, LTR={self.ltr_threads}, RS={self.rs_threads}")

        # Pipeline steps and their checkpoints
        self.steps = {
            'genome': 'genome.ok',
            'ltr': 'LTR.ok',
            'repeatscout': 'RepeatScout.ok',
            'recon': 'RECON.ok',
            'combine': 'Combine.ok'
        }
        self.processes = {
            'genome': self.process_genome,
            'ltr': self.process_ltr,
            'repeatscout': self.process_repeatscout,
            'recon': self.process_recon,
            'combine': self.combine_results
        }
        # Create output directory structure
        self.setup_directories()

    def prepare_genome_file(self):
        """Handle decompression of genome file if needed."""
        if not self.original_genome_file:
            return
            
        compression_format = detect_compression_format(self.original_genome_file)
        
        if compression_format == 'uncompressed':
            self.genome_file = self.original_genome_file
            logger.info(f"Using uncompressed genome file: {self.genome_file}")
        else:
            logger.info(f"Detected {compression_format} compressed genome file: {self.original_genome_file}")
            # Decompress to a temporary file in the output directory
            temp_genome = os.path.join(self.output_dir, 'genome', 'decompressed_genome.fa')
            os.makedirs(os.path.dirname(temp_genome), exist_ok=True)
            
            self.genome_file = decompress_file(self.original_genome_file, temp_genome)
            self.decompressed_genome_file = temp_genome
            logger.info(f"Decompressed genome file ready: {self.genome_file}")

    def cleanup_temp_files(self):
        """Clean up temporary files created during processing."""
        if self.decompressed_genome_file and os.path.exists(self.decompressed_genome_file):
            try:
                os.remove(self.decompressed_genome_file)
                logger.info(f"Cleaned up decompressed genome file: {self.decompressed_genome_file}")
            except Exception as e:
                logger.warning(f"Failed to clean up decompressed genome file: {e}")

    def calculate_chunk_count(self):
        """Calculate the number of chunks that will be used by build_RS based on genome size."""
        if not self.genome_file or not os.path.exists(self.genome_file):
            return 1  # Default to 1 chunk if genome file doesn't exist yet
            
        genome_size = os.path.getsize(self.genome_file)
        
        # Constants from build_RS determine_sampling_strategy
        mb_500 = 500 * 1024 * 1024    # 500MB
        gb_1 = 1024 * 1024 * 1024     # 1GB
        gb_3 = 3 * 1024 * 1024 * 1024 # 3GB
        gb_5 = 5 * 1024 * 1024 * 1024 # 5GB
        
        if genome_size < mb_500:
            # Small genome (<500Mb): single run
            return 1
        elif genome_size >= mb_500 and genome_size < gb_1:
            # Medium genome (500Mb-1Gb): single run
            return 1
        elif genome_size >= gb_1 and genome_size < gb_3:
            # Large genome (1Gb-3Gb): 2 parallel chunks
            return 2
        elif genome_size >= gb_3 and genome_size < gb_5:
            # Very large genome (3Gb-5Gb): 3 parallel chunks
            return 3
        else:
            # Massive genome (>5Gb): 4 parallel chunks
            return 4

    def setup_directories(self):
        """Create the necessary directory structure."""
        directories = [
            self.output_dir,
            os.path.join(self.output_dir, 'genome'),
            os.path.join(self.output_dir, 'RepeatScout'),
            os.path.join(self.output_dir, 'RECON'),
            os.path.join(self.output_dir, 'Combine')
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
            logger.info(f"Created directory: {directory}")

    def create_checkpoint(self, step: str):
        """Create a checkpoint file for a completed step."""
        checkpoint_file = os.path.join(self.output_dir, self.steps[step])
        Path(checkpoint_file).touch()
        logger.info(f"Created checkpoint for {step}: {checkpoint_file}")

    def check_checkpoint(self, step: str) -> bool:
        """Check if a step's checkpoint exists."""
        return os.path.exists(os.path.join(self.output_dir, self.steps[step]))

    def run_cmd(self, cmd: str, workdir: Optional[str] = None) -> subprocess.CompletedProcess:
        """Run shell command with error handling and logging."""
        start_time = time.time()
        current_dir = os.getcwd()
        
        try:
            if workdir:
                os.chdir(workdir)
                
            logger.info(f"Running command: {cmd}")
            result = subprocess.run(
                cmd,
                shell=True,
                check=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                encoding='utf-8'
            )
            
            duration = time.time() - start_time
            logger.info(f"Command completed in {duration:.2f} seconds")
            return result
            
        except subprocess.CalledProcessError as e:
            duration = time.time() - start_time
            logger.error(f"Command failed after {duration:.2f} seconds")
            logger.error(f"Error output: {e.stderr}")
            raise
            
        finally:
            if workdir:
                os.chdir(current_dir)

    def process_genome(self):
        """Step 1: Process genome and VCF files if provided."""
        if self.check_checkpoint('genome'):
            logger.info("Genome processing already completed, skipping...")
            # When genome.ok exists, use the processed genome directly
            self.genome_file = self.processed_genome
            return

        genome_dir = os.path.join(self.output_dir, 'genome')
        processed_genome = os.path.join(genome_dir, 'genome.fa')
        os.makedirs(genome_dir, exist_ok=True)

        # Decompress genome file if needed (only when checkpoint doesn't exist)
        if self.original_genome_file:
            self.prepare_genome_file()
            
        # Step 1: Process VCF files if provided
        combined_input = None
        if self.vcf_dir:
            logger.info("Processing VCF files...")
            vcf_processor = VCFProcessor(
                vcf_dir=self.vcf_dir,
                output_dir=genome_dir,
                total_threads=self.threads
            )
            try:
                vcf_sequences = vcf_processor.process_all_vcfs()
                logger.info(f"VCF processing completed: {vcf_sequences}")
                
                # Step 2: Combine VCF results with input genome
                logger.info("Combining VCF results with input genome...")
                combined_input = os.path.join(genome_dir, 'combined_input.fa')
                with open(combined_input, 'w') as outfile:
                    # First write VCF sequences
                    with open(vcf_sequences) as infile:
                        outfile.write(infile.read())
                    # Then write genome sequences
                    with open(self.genome_file) as infile:
                        outfile.write(infile.read())
                
                # Clean up VCF results file (DISABLED - keeping temp files)
                # os.remove(vcf_sequences)
                
                # Update genome file path to combined file for processing
                self.genome_file = combined_input
                
            except Exception as e:
                logger.error(f"VCF processing failed: {str(e)}")
                raise

        # Step 3: Process the genome file (either combined or original)
        logger.info("Processing genome file...")
        self.process_single_genome(processed_genome)

        # Clean up temporary files (DISABLED - keeping temp files)
        # if combined_input and os.path.exists(combined_input):
        #     os.remove(combined_input)

        # Update genome file path to processed version
        self.genome_file = processed_genome
        self.create_checkpoint('genome')
        logger.info("Genome processing completed")

    def process_single_genome(self, output_file: str):
        """Process a single genome file."""
        logger.info("Starting single genome processing")
        
        # Process genome file
        tmp_fa = output_file + '.tmp'
        self.run_cmd(f"tr 'a-z' 'A-Z' < {self.genome_file} > {tmp_fa}")
        self.run_cmd(f"clean_seq {tmp_fa} {output_file}")
        
        # Index genome
        self.run_cmd(f"samtools faidx {output_file}")
        self.run_cmd(f"index {output_file}") 

        # os.remove(tmp_fa)  # DISABLED - keeping temp files
        logger.info("Single genome processing completed")

    def process_ltr(self):
        """Process LTRs using allocated threads."""
        if self.check_checkpoint('ltr'):
            logger.info("LTR processing already completed")
            return
            
        logger.info(f"Starting LTR processing with {self.ltr_threads} threads")
        
        # Use the processed genome file from genome directory
        genome_file = os.path.join(self.output_dir, 'genome', 'genome.fa')
        if not os.path.exists(genome_file):
            raise FileNotFoundError(f"Processed genome file not found: {genome_file}")
        
        # First create genome index if it doesn't exist
        if not os.path.exists(f"{genome_file}.fai"):
            self.run_cmd(f"samtools faidx {genome_file}")
        
        # Run LTR_detect with the processed genome file
        cmd = f"LTR_detect {genome_file} {self.ltr_threads} {self.output_dir}"
        self.run_cmd(cmd)
        
        self.create_checkpoint('ltr')
        logger.info("LTR processing completed")

    def process_repeatscout(self):
        """Process RepeatScout with allocated threads."""
        if self.check_checkpoint('repeatscout'):
            logger.info("RepeatScout processing already completed")
            return
            
        logger.info(f"Starting RepeatScout with {self.rs_threads} threads")
        rs_dir = os.path.join(self.output_dir, 'RepeatScout')
        tmp_dir = os.path.join(rs_dir, 'tmp')
        os.makedirs(tmp_dir, exist_ok=True)
        processed_genome = os.path.join(self.output_dir, 'genome', 'genome.fa')
        cmd = (f"build_RS --genome {processed_genome} --threads {self.rs_threads} "
               f"--tmp {tmp_dir} --workdir {rs_dir} ")
        
        if hasattr(self, 'rmblast_dir'):
            cmd += f" --rmblast {self.rmblast_dir}"
            
        self.run_cmd(cmd, rs_dir)
        self.create_checkpoint('repeatscout')
        logger.info("RepeatScout processing completed")

    def process_recon(self):
        """Step 4: Process RECON."""
        if self.check_checkpoint('recon'):
            logger.info("RECON processing already completed")
            return
            
        logger.info("Starting RECON processing")
        recon_dir = os.path.join(self.output_dir, 'RECON')
        
        # Always use Refiner-generated masked genome (simplifies workflow)  
        masked_genome = os.path.join(self.output_dir, 'RepeatScout', 'refiner_output', 'genome_final_masked.fa')
        
        if not os.path.exists(masked_genome):
            raise FileNotFoundError(f"Expected masked genome not found: {masked_genome}")
            
        logger.info(f"Using Refiner-generated masked genome: {masked_genome}")
        
        # Use original genome size for batch size calculation
        original_genome_size = os.path.getsize(self.processed_genome)
        
        # Use masked genome size as sample size (full masked genome)
        sample_size = os.path.getsize(masked_genome)
        logger.info(f"Using full masked genome (size: {sample_size:,} bp) for RECON")
        
        cmd = f"run_RECON_advanced {self.threads} {masked_genome} {sample_size}"
        self.run_cmd(cmd, recon_dir)
        
        self.create_checkpoint('recon')

    def combine_results(self):
        """Step 5: Combine and classify results."""
        if self.check_checkpoint('combine'):
            logger.info("Results combination already completed, skipping...")
            return
            
        if not self.model_dir:
            logger.warning("No model directory specified, skipping classification step")
            # Continue with combination but skip classification
            
        logger.info("Starting results combination and classification")
        combine_dir = os.path.join(self.output_dir, 'Combine')
        
        # Step 1: Combine raw results from LTR, RepeatScout and RECON
        # RepeatScout now uses Refiner output with high_quality_consensus.fasta
        rs_consensus = "../RepeatScout/refiner_output/phase3_analysis_library.fa"
        if not os.path.exists(os.path.join(combine_dir, rs_consensus)):
            rs_consensus = "../RepeatScout/consensi.fa"  # fallback
        
        # Find all RECON consensi files from different tracks and rounds
        recon_dir = os.path.join(self.output_dir, 'RECON')
        recon_consensi = []
        
        # Check for direct consensi.fa (single_track mode for small genomes)
        direct_consensi = os.path.join(recon_dir, 'consensi.fa')
        if os.path.exists(direct_consensi) and os.path.getsize(direct_consensi) > 0:
            recon_consensi.append("../RECON/consensi.fa")
            logger.info("Found RECON single_track result: consensi.fa")
        
        # Check for masked track consensi (dual_track mode)
        masked_track_consensi = os.path.join(recon_dir, 'masked_track', 'consensi.fa')
        if os.path.exists(masked_track_consensi) and os.path.getsize(masked_track_consensi) > 0:
            recon_consensi.append("../RECON/masked_track/consensi.fa")
            logger.info("Found RECON masked_track result: masked_track/consensi.fa")
        
        # Check for sampling track rounds (dual_track mode)
        sampling_track_dir = os.path.join(recon_dir, 'sampling_track')
        if os.path.isdir(sampling_track_dir):
            # Get all round directories and sort them numerically
            round_dirs = [d for d in os.listdir(sampling_track_dir) if d.startswith('round_')]
            round_dirs.sort(key=lambda x: int(x.split('_')[1]) if x.split('_')[1].isdigit() else 0)
            
            for round_dir in round_dirs:
                round_consensi = os.path.join(sampling_track_dir, round_dir, 'consensi.fa')
                if os.path.exists(round_consensi) and os.path.getsize(round_consensi) > 0:
                    recon_consensi.append(f"../RECON/sampling_track/{round_dir}/consensi.fa")
                    logger.info(f"Found RECON sampling_track result: sampling_track/{round_dir}/consensi.fa")
        
        # Build the cat command with all available consensi files
        input_files = ["../Look4LTRs/consensi.fa", rs_consensus] + recon_consensi
        cat_cmd = f"cat {' '.join(input_files)} > raw_TEs_combined.fa"
        
        if len(recon_consensi) == 0:
            logger.warning("No RECON consensus files found - check if RECON step completed successfully")
        
        logger.info(f"Combining consensus libraries from {len(input_files)} sources: LTR, RepeatScout, and {len(recon_consensi)} RECON result(s)")
        self.run_cmd(cat_cmd, combine_dir)
        
        # Step 2: TE-optimized deduplication using CD-HIT-EST
        logger.info("Removing redundant TEs with TE-optimized parameters...")
        
        # CD-HIT parameters optimized for TE characteristics:
        # -c 0.8: 80% identity - balance between removing redundancy and preserving TE diversity
        # -aS 0.8: 80% shorter sequence coverage - important for TE fragments and deletions
        # -aL 0.8: 80% longer sequence coverage - handles partial TEs
        # -G 0: local alignment - better for TEs with divergent ends/TIRs
        # -n 8: word size 8 for 80% identity (CD-HIT recommendation)
        # -r 1: compare both strands - TEs can insert in both orientations
        # -mask NX: mask low-complexity regions - common in TEs
        # -M 0: no memory limit
        # -d 0: full sequence names in output
        
        self.run_cmd(
            f"cd-hit-est "
            f"-i raw_TEs_combined.fa "
            f"-o raw_TEs.fa "
            f"-c 0.8 "           # 80% sequence identity threshold
            f"-aS 0.8 "          # 80% coverage of shorter sequence
            f"-aL 0.8 "          # 80% coverage of longer sequence  
            f"-G 0 "             # local alignment mode
            f"-n 8 "             # word size for 80% identity
            f"-r 1 "             # compare both strands
            f"-mask NX "         # mask low-complexity regions
            f"-M 0 "             # no memory limit
            f"-T {self.threads} " # use all available threads
            f"-d 0",             # keep full sequence names
            combine_dir
        )
        
        # Log deduplication results
        raw_count = int(subprocess.check_output(f"grep -c '^>' raw_TEs_combined.fa", 
                                               shell=True, cwd=combine_dir).decode().strip())
        final_count = int(subprocess.check_output(f"grep -c '^>' raw_TEs.fa", 
                                                 shell=True, cwd=combine_dir).decode().strip())
        logger.info(f"Deduplication: {raw_count} -> {final_count} sequences "
                   f"({raw_count - final_count} duplicates removed)")
        
        # Step 3: Run classifier on deduplicated TEs (if model directory available)
        if self.model_dir:
            self.run_cmd(f"run_Classifier {self.model_dir} {self.threads}", combine_dir)
        else:
            logger.info("Skipping classification step (no model directory specified)")
        
        self.create_checkpoint('combine')
        logger.info("Results combination completed")

    def process_single_genome(self, output_file: str):
        """Process a single genome file."""
        logger.info("Starting single genome processing")
        
        # Process genome file
        tmp_fa = output_file + '.tmp'
        self.run_cmd(f"tr 'a-z' 'A-Z' < {self.genome_file} > {tmp_fa}")
        self.run_cmd(f"clean_seq {tmp_fa} {output_file}")
        
        # Index genome
        self.run_cmd(f"samtools faidx {output_file}")
        self.run_cmd(f"index {output_file}") 

        # os.remove(tmp_fa)  # DISABLED - keeping temp files
        logger.info("Single genome processing completed")

    def combine_genome_results(self, genome_dir: str, input_files: list):
        """Combine and process all genome sequences into final genome.fa."""
        logger.info("Combining genome results")
        
        # Combine all input files
        combined_fa = os.path.join(genome_dir, 'combined.fa')
        with open(combined_fa, 'w') as outfile:
            for infile in input_files:
                with open(infile) as indata:
                    outfile.write(indata.read())
        
        # Process combined file
        tmp_fa = os.path.join(genome_dir, 'tmp.fa')
        self.run_cmd(f"tr 'a-z' 'A-Z' < {combined_fa} > {tmp_fa}")
        self.run_cmd(f"clean_seq {tmp_fa} {os.path.join(genome_dir, 'genome.fa')}")
        self.run_cmd(f"samtools faidx {os.path.join(genome_dir, 'genome.fa')}")
        self.run_cmd(f"index {os.path.join(genome_dir, 'genome')}")
        
        # Cleanup (DISABLED - keeping temp files)
        # os.remove(tmp_fa)  # DISABLED - keeping temp files
        # os.remove(combined_fa)
        # for file in input_files:
        #     os.remove(file)
        
        logger.info("Genome results combined successfully")

    def run(self):
        """Run complete pipeline."""
        try:
            # Process genome first
            if not self.check_checkpoint('genome'):
                self.process_genome()
                
            # Run parallel detection after genome processing
            logger.info("Starting parallel TE detection")
            with ThreadPoolExecutor(max_workers=2) as executor:
                ltr_future = executor.submit(self.process_ltr)
                rs_future = executor.submit(self.process_repeatscout)
                
                # Wait for both to complete
                for future in [ltr_future, rs_future]:
                    try:
                        future.result()
                    except Exception as e:
                        logger.error(f"Detection task failed: {str(e)}")
                        raise
            
            # Run remaining steps
            if not self.check_checkpoint('recon'):
                self.process_recon()
                
            if not self.check_checkpoint('combine'):
                self.combine_results()
                
        except Exception as e:
            logger.error(f"Pipeline failed: {str(e)}")
            raise
        finally:
            # Clean up temporary files
            self.cleanup_temp_files()

def main():
    """Main execution function."""
    try:
        # Parse arguments
        args = parse_arguments()
        
        # Create pipeline instance
        pipeline = PanTEPipeline(
            genome_file=args.genome,
            output_dir=args.out,
            threads=args.threads,
            model_dir=args.model_dir,
            vcf_dir=args.vcf_dir
        )

        # Set memory limit if specified
        if args.M > 0:
            import resource
            resource.setrlimit(resource.RLIMIT_AS, (args.M * 1024 * 1024, -1))
            logger.info(f"Set memory limit to {args.M} MB")

        # Set fragment size
        if hasattr(pipeline, 'fragment_size'):
            pipeline.fragment_size = args.fragment_size
            logger.info(f"Set fragment size to {args.fragment_size}")

        # Run pipeline
        pipeline.run()
        logger.info("Pipeline completed successfully")
        
    except KeyboardInterrupt:
        logger.error("Pipeline interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Pipeline failed: {str(e)}")
        if logging.getLogger().isEnabledFor(logging.DEBUG):
            logger.exception("Detailed error traceback:")
        sys.exit(1)

if __name__ == "__main__":
    main()
