"""
Phase 4: 高质量基因组硬屏蔽 (High-Quality Genome Hard Masking)
新策略：
1. 只使用高质量的共识序列(consensus)运行RepeatMasker
2. 采用硬屏蔽(hard masking)模式，将重复序列替换为N字符
3. 不再依赖Phase 1结果，完全基于Phase 3的高质量共识序列
目标：基于最终高质量consensus的精准硬屏蔽
"""

import logging
import os
import tempfile
import subprocess
from typing import Dict, List, Set, Tuple, Optional
from pathlib import Path
import numpy as np
from Bio import SeqIO
from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord

from config import PipelineConfig
from utils.robust_runner import RobustRunner
from utils.genome_renamer import GenomeRenamer, integrate_with_repeatmasker_workflow, restore_after_phase4

logger = logging.getLogger(__name__)

class GenomeMasker:
    """Phase 4: 高质量基因组硬屏蔽"""
    
    def __init__(self, config: PipelineConfig):
        self.config = config
        self.runner = RobustRunner(config)
        
        # 硬屏蔽参数 (优化用于高质量consensus)
        self.min_rm_score = 200  # RepeatMasker最小分数（适中，避免过于严格）
        self.min_identity = 70.0  # 较低阈值，捕获更多diverged copies
        self.min_length = 50     # 最小mask长度
        self.hard_mask = True    # 强制硬屏蔽模式
        
        # 高质量consensus参数（更合理的默认值）
        self.min_consensus_quality = 0.3  # 降低质量阈值，因为Phase 1已经筛选过
        self.min_consensus_copies = 2     # 至少基于2个拷贝的consensus
        self.exclude_low_complexity = True  # 排除低复杂度sequences
        self.min_consensus_length = 80    # 最小序列长度
        
        # ID重命名和还原
        self.use_id_renaming = True  # 启用ID重命名预防RepeatMasker ID问题
        self.genome_renamer = None
        self.renamed_genome_file = None
        self.mapping_file = None
        
        logger.info(f"Phase 4 initialized (Consensus-only hard masking): "
                   f"min_score={self.min_rm_score}, min_identity={self.min_identity}%")
        logger.info(f"ID renaming enabled: {self.use_id_renaming}")
    
    def run(self, phase1_output: Dict, phase3_output: Dict) -> Dict[str, any]:
        """执行Phase 4完整流程（仅基于高质量consensus的硬屏蔽）"""
        logger.info("="*60)
        logger.info("Phase 4: High-Quality Consensus Hard Masking")
        logger.info("="*60)
        
        # 步骤0：重命名基因组ID以避免RepeatMasker问题
        genome_to_use = self.config.genome_file
        if self.use_id_renaming:
            genome_to_use, self.mapping_file = self._prepare_renamed_genome()
            logger.info(f"Using renamed genome: {genome_to_use}")
        
        # 步骤1：从Phase 3获取并过滤高质量consensus序列
        high_quality_consensus = self._extract_high_quality_consensus(phase3_output)
        logger.info(f"Extracted {len(high_quality_consensus)} high-quality consensus sequences")
        
        # 步骤2：创建高质量consensus库文件
        consensus_library_file = self._create_consensus_library(high_quality_consensus)
        logger.info(f"Created consensus library: {consensus_library_file}")
        
        # 步骤3：使用高质量consensus运行RepeatMasker硬屏蔽
        hard_masked_genome = self._run_repeatmasker_soft_masking(
            genome_to_use,  # 使用重命名后的基因组
            consensus_library_file
        )
        logger.info(f"Generated hard-masked genome: {hard_masked_genome}")
        
        # 步骤4：验证硬屏蔽质量
        masking_stats = self._analyze_masking_quality(
            self.config.genome_file,
            hard_masked_genome,
            high_quality_consensus
        )
        logger.info(f"Masking quality analysis completed")
        
        # 步骤5：计算masking统计和报告
        final_stats = self._calculate_final_stats(
            self.config.genome_file,
            hard_masked_genome,
            masking_stats,
            high_quality_consensus
        )
        
        # 步骤6：生成输出文件
        output_files = self._generate_output_files(
            hard_masked_genome,  # hard_masked_genome
            final_stats,         # stats
            consensus_library_file  # consensus_library
        )
        
        # 计算注释比例并写入文件供RECON使用
        annotation_ratio = final_stats['hard_masked_percent'] / 100.0  # 转换为0-1范围
        annotation_file = Path(self.config.output_dir) / "annotation_ratio.txt"
        
        with open(annotation_file, 'w') as f:
            f.write(f"{annotation_ratio:.6f}\n")
            f.write(f"# Annotation ratio: {final_stats['hard_masked_percent']:.2f}%\n")
            f.write(f"# Masked bases: {final_stats['hard_masked_bp']:,}\n")
            f.write(f"# Total bases: {final_stats['genome_size']:,}\n")
            f.write(f"# Generated by Phase 4 Consensus-only Hard Masking\n")
        
        logger.info(f"Annotation ratio ({annotation_ratio:.4f}) written to {annotation_file}")
        
        # 步骤7：还原ID到原始格式（如果使用了重命名）
        final_output_files = output_files
        if self.use_id_renaming and self.mapping_file:
            final_output_files = self._restore_original_ids(output_files)
            logger.info(f"Restored original IDs for all output files")
        
        return {
            'hard_masked_genome': final_output_files.get('hard_masked_genome', hard_masked_genome),
            'consensus_library': consensus_library_file,
            'masking_stats': masking_stats,
            'final_stats': final_stats,
            'annotation_ratio': annotation_ratio,
            'annotation_file': str(annotation_file),
            'output_files': final_output_files,
            'high_quality_consensus_count': len(high_quality_consensus),
            'id_mapping_file': self.mapping_file,
            'renamed_genome_file': self.renamed_genome_file,
            'id_renaming_used': self.use_id_renaming,
            'summary': f"Hard-masked {final_stats['hard_masked_percent']:.2f}% of genome using {len(high_quality_consensus)} high-quality consensus sequences (annotation ratio: {annotation_ratio:.4f})"
        }
    
    def _extract_high_quality_consensus(self, phase3_output: Dict) -> List[Dict]:
        """从Phase 3输出中提取高质量consensus序列"""
        high_quality_consensus = []
        
        # Phase 3输出结构：masking_library, analysis_library
        # 优先使用masking_library，它已经针对基因组屏蔽进行了优化
        consensus_library = phase3_output.get('masking_library', [])
        if not consensus_library:
            # 后备选择：使用analysis_library
            consensus_library = phase3_output.get('analysis_library', [])
        
        if not consensus_library:
            logger.warning("No consensus sequences found in Phase 3 output")
            return high_quality_consensus
        
        logger.info(f"Processing {len(consensus_library)} consensus sequences from Phase 3")
        
        for consensus_item in consensus_library:
            # consensus_item是字典，包含id, sequence, quality_score, copy_number等
            if not isinstance(consensus_item, dict):
                logger.warning(f"Invalid consensus item: {type(consensus_item)}")
                continue
            
            # 获取基本信息
            consensus_id = consensus_item.get('id', 'unknown')
            consensus_seq = consensus_item.get('sequence', '')
            if not consensus_seq:
                logger.debug(f"No sequence for consensus {consensus_id}")
                continue
            
            # 获取质量评分（兼容多种字段名）
            quality_score = consensus_item.get('quality_score', 
                                              consensus_item.get('final', 0.5))  # 更合理的默认值
            # 获取拷贝数（兼容Phase 2和Phase 3的字段名）
            copy_number = consensus_item.get('copy_number', 
                                            consensus_item.get('num_copies', 1))  # 默认至少1个拷贝
            
            # 序列长度检查
            seq_length = len(consensus_seq)
            if seq_length < self.min_consensus_length:
                logger.debug(f"Excluded short consensus {consensus_id}: length={seq_length} < {self.min_consensus_length}")
                continue
            
            # 应用高质量过滤条件（更宽松的条件）
            # 优先考虑：1)序列长度 2)拷贝数 3)质量分数
            if copy_number >= self.min_consensus_copies or \
               (copy_number >= 1 and quality_score >= self.min_consensus_quality) or \
               (seq_length >= 200 and copy_number >= 1):  # 长序列放宽要求
                
                # 检查低复杂度（如果启用）
                if self.exclude_low_complexity:
                    complexity_score = self._calculate_sequence_complexity(consensus_seq)
                    if complexity_score < 0.2:  # 只排除极低复杂度序列（如AAAAA...）
                        logger.debug(f"Excluded low complexity consensus {consensus_id} (complexity: {complexity_score:.3f})")
                        continue
                
                high_quality_consensus.append({
                    'id': consensus_id,
                    'sequence': consensus_seq,
                    'quality_score': quality_score,
                    'copy_number': copy_number,
                    'length': len(consensus_seq),
                    'description': f"High-quality consensus from {copy_number} copies, quality={quality_score:.3f}"
                })
                
                logger.debug(f"Included consensus {consensus_id}: "
                           f"quality={quality_score:.3f}, copies={copy_number}, length={seq_length}")
            else:
                logger.debug(f"Excluded consensus {consensus_id}: "
                           f"quality={quality_score:.3f}, copies={copy_number}, length={seq_length} "
                           f"(min_copies={self.min_consensus_copies}, min_quality={self.min_consensus_quality})")
        
        logger.info(f"Selected {len(high_quality_consensus)} high-quality consensus sequences")
        return high_quality_consensus
    
    def _calculate_sequence_complexity(self, sequence: str) -> float:
        """计算序列复杂度（基于DUST算法的简化版本）"""
        if len(sequence) < 10:
            return 0.0
        
        # 统计k-mer频率 (k=3)
        kmer_counts = {}
        k = 3
        total_kmers = len(sequence) - k + 1
        
        if total_kmers <= 0:
            return 0.0
        
        for i in range(total_kmers):
            kmer = sequence[i:i+k].upper()
            if 'N' not in kmer:  # 排除含N的kmer
                kmer_counts[kmer] = kmer_counts.get(kmer, 0) + 1
        
        if not kmer_counts:
            return 0.0
        
        # 计算Shannon熵
        entropy = 0.0
        for count in kmer_counts.values():
            freq = count / total_kmers
            entropy -= freq * np.log2(freq)
        
        # 标准化到0-1范围（3-mer的最大熵约为6）
        max_entropy = np.log2(min(64, total_kmers))  # 4^3 = 64种可能的3-mer
        complexity = entropy / max_entropy if max_entropy > 0 else 0.0
        
        return min(1.0, complexity)
    
    def _create_consensus_library(self, high_quality_consensus: List[Dict]) -> str:
        """创建高质量consensus库文件"""
        library_file = Path(self.config.output_dir) / "high_quality_consensus.fasta"
        
        with open(library_file, 'w') as f:
            for consensus in high_quality_consensus:
                header = f">{consensus['id']} {consensus['description']}"
                f.write(header + '\n')
                f.write(consensus['sequence'] + '\n')
        
        logger.info(f"Created consensus library with {len(high_quality_consensus)} sequences: {library_file}")
        return str(library_file)
    
    def _run_repeatmasker_soft_masking(self, genome_file: str, consensus_library: str) -> str:
        """使用高质量consensus运行RepeatMasker硬屏蔽"""
        output_dir = Path(self.config.output_dir)
        hard_masked_file = output_dir / "genome_hard_masked.fa"
        
        # RepeatMasker硬屏蔽参数
        cmd = [
            'RepeatMasker',
            '-lib', consensus_library,    # 使用自定义consensus库
            # 移除 -xsmall 参数以启用硬屏蔽模式（N字符）
            '-nolow',                     # 不屏蔽低复杂度区域
            '-no_is',                     # 不搜索interspersed repeats
            '-norna',                     # 不搜索RNA genes
            '-parallel', str(self.config.threads),  # 并行线程数
            '-dir', str(output_dir),      # 输出目录
            '-cutoff', str(int(self.min_rm_score)),  # 最小分数阈值
            genome_file
        ]
        
        logger.info(f"Running RepeatMasker hard masking with command: {' '.join(cmd)}")
        
        try:
            result = subprocess.run(
                cmd,
                cwd=output_dir,
                capture_output=True,
                text=True,
                timeout=7200  # 2小时超时
            )
            
            if result.returncode != 0:
                logger.error(f"RepeatMasker failed with return code {result.returncode}")
                logger.error(f"STDERR: {result.stderr}")
                raise RuntimeError(f"RepeatMasker execution failed")
            
            # RepeatMasker生成的文件名 (完整输入文件名 + .masked)
            genome_filename = Path(genome_file).name
            rm_output = output_dir / f"{genome_filename}.masked"
            
            if rm_output.exists():
                # 重命名为标准名称
                import shutil
                shutil.move(str(rm_output), str(hard_masked_file))
                logger.info(f"RepeatMasker hard masking completed: {hard_masked_file}")
            else:
                raise RuntimeError(f"RepeatMasker output not found: {rm_output}")
            
        except subprocess.TimeoutExpired:
            logger.error("RepeatMasker timeout (2 hours)")
            raise RuntimeError("RepeatMasker execution timeout")
        except Exception as e:
            logger.error(f"RepeatMasker execution error: {e}")
            raise
        
        return str(hard_masked_file)
    
    def _analyze_masking_quality(self, original_genome: str, hard_masked_genome: str, 
                                high_quality_consensus: List[Dict]) -> Dict:
        """分析硬屏蔽质量 - 使用RepeatMasker .tbl文件的准确统计"""
        logger.info("Analyzing hard masking quality...")
        
        # 首先尝试从RepeatMasker .tbl文件读取准确的masking统计
        rm_stats = self._parse_repeatmasker_tbl()
        
        if rm_stats:
            # 使用RepeatMasker .tbl文件的准确统计
            masked_bases = rm_stats['bases_masked']
            total_bases = rm_stats['total_length']
            masked_percent = rm_stats['masked_percent']
            
            logger.info(f"Using RepeatMasker .tbl statistics: {masked_percent:.2f}% masking")
        else:
            # 后备方案：手动计算（可能不准确）
            logger.warning("RepeatMasker .tbl file not found, using manual calculation")
            masked_stats = self._get_sequence_stats(hard_masked_genome)
            masked_bases = masked_stats['n_count']
            total_bases = masked_stats['total_bases']
            masked_percent = (masked_bases / total_bases * 100) if total_bases > 0 else 0
        
        quality_stats = {
            'total_consensus_used': len(high_quality_consensus),
            'avg_consensus_quality': np.mean([c['quality_score'] for c in high_quality_consensus]) if high_quality_consensus else 0,
            'avg_consensus_copies': np.mean([c['copy_number'] for c in high_quality_consensus]) if high_quality_consensus else 0,
            'avg_consensus_length': np.mean([c['length'] for c in high_quality_consensus]) if high_quality_consensus else 0,
            'masked_bases': masked_bases,
            'total_bases': total_bases,
            'masked_percent': masked_percent,
            'repeatmasker_stats': rm_stats,
            'stats_source': 'repeatmasker_tbl' if rm_stats else 'manual_calculation'
        }
        
        logger.info(f"Hard masking quality: {masked_percent:.2f}% of genome masked using {len(high_quality_consensus)} consensus sequences")
        if rm_stats:
            logger.info(f"RepeatMasker statistics: {rm_stats['total_elements']} elements, {rm_stats['total_length']:,} bp genome")
        
        return quality_stats
    
    def _parse_repeatmasker_tbl(self) -> Dict:
        """解析RepeatMasker .tbl文件获取准确的masking统计"""
        # 查找RepeatMasker .tbl文件
        output_dir = Path(self.config.output_dir)
        possible_tbl_files = [
            output_dir / f"{Path(self.renamed_genome_file).name}.tbl" if self.renamed_genome_file else None,
            output_dir / "renamed_genome_merged_for_refiner.fa.tbl",
            output_dir / "genome_hard_masked.fa.tbl",
            # 在子目录中查找
            output_dir / "genome_renaming" / "renamed_genome_merged_for_refiner.fa.tbl"
        ]
        
        tbl_file = None
        for possible_file in possible_tbl_files:
            if possible_file and possible_file.exists():
                tbl_file = possible_file
                break
        
        if not tbl_file:
            logger.warning("RepeatMasker .tbl file not found in expected locations")
            return None
        
        logger.info(f"Parsing RepeatMasker statistics from: {tbl_file}")
        
        try:
            with open(tbl_file, 'r') as f:
                lines = f.readlines()
            
            stats = {}
            
            # 解析文件头信息
            for line in lines[:10]:  # 前10行通常包含基本统计
                line = line.strip()
                
                if line.startswith('file name:'):
                    stats['file_name'] = line.split(':', 1)[1].strip()
                elif line.startswith('sequences:'):
                    stats['num_sequences'] = int(line.split()[1])
                elif line.startswith('total length:'):
                    # 格式: "total length:  838000000 bp  (759994272 bp excl N/X-runs)"
                    parts = line.split()
                    stats['total_length'] = int(parts[2])
                elif line.startswith('bases masked:'):
                    # 格式: "bases masked:  283523120 bp ( 33.83 %)"
                    parts = line.split()
                    stats['bases_masked'] = int(parts[2])
                    # 提取百分比，格式可能是 "( 33.83 %)" 
                    pct_str = line.split('(')[1].split('%')[0].strip()
                    stats['masked_percent'] = float(pct_str)
            
            # 解析总的interspersed repeats统计
            for line in lines:
                if line.startswith('Total interspersed repeats:'):
                    parts = line.split()
                    if len(parts) >= 4:
                        # 可能与bases masked不同，使用更准确的数字
                        interspersed_masked = int(parts[3])
                        if 'bases_masked' not in stats or interspersed_masked > 0:
                            stats['bases_masked'] = interspersed_masked
                    break
            
            # 统计分类的repeat数量
            stats['total_elements'] = 0
            for line in lines:
                if line.startswith('Unclassified:'):
                    parts = line.split()
                    if len(parts) >= 2:
                        stats['total_elements'] = int(parts[1])
                    break
            
            # 验证必要字段
            required_fields = ['total_length', 'bases_masked', 'masked_percent']
            if not all(field in stats for field in required_fields):
                logger.error(f"Failed to parse required fields from .tbl file: missing {[f for f in required_fields if f not in stats]}")
                return None
            
            logger.info(f"RepeatMasker .tbl statistics:")
            logger.info(f"  Genome length: {stats['total_length']:,} bp")
            logger.info(f"  Bases masked: {stats['bases_masked']:,} bp")
            logger.info(f"  Masked percent: {stats['masked_percent']:.2f}%")
            logger.info(f"  Total elements: {stats.get('total_elements', 'unknown')}")
            
            return stats
            
        except Exception as e:
            logger.error(f"Failed to parse RepeatMasker .tbl file {tbl_file}: {e}")
            return None
    
    def _get_sequence_stats(self, fasta_file: str) -> Dict:
        """获取FASTA文件的序列统计信息"""
        stats = {
            'total_bases': 0,
            'uppercase_count': 0,
            'lowercase_count': 0,
            'n_count': 0,
            'gc_count': 0
        }
        
        for record in SeqIO.parse(fasta_file, 'fasta'):
            seq_str = str(record.seq)
            stats['total_bases'] += len(seq_str)
            
            for char in seq_str:
                if char.isupper():
                    stats['uppercase_count'] += 1
                elif char.islower():
                    stats['lowercase_count'] += 1
                
                char_upper = char.upper()
                if char_upper == 'N':
                    stats['n_count'] += 1
                elif char_upper in 'GC':
                    stats['gc_count'] += 1
        
        # 计算比例
        if stats['total_bases'] > 0:
            stats['gc_percent'] = stats['gc_count'] / stats['total_bases'] * 100
            stats['n_percent'] = stats['n_count'] / stats['total_bases'] * 100
            stats['masked_percent'] = stats['lowercase_count'] / stats['total_bases'] * 100
        
        return stats
    
    def _calculate_final_stats(self, original_genome: str, hard_masked_genome: str,
                             masking_stats: Dict, consensus_list: List[Dict]) -> Dict:
        """计算最终统计信息"""
        
        stats = {
            'genome_size': masking_stats['total_bases'],
            'hard_masked_bp': masking_stats['masked_bases'],
            'hard_masked_percent': masking_stats['masked_percent'],
            'consensus_used': len(consensus_list),
            'avg_consensus_quality': masking_stats['avg_consensus_quality'],
            'avg_consensus_copies': masking_stats['avg_consensus_copies'],
            'avg_consensus_length': masking_stats['avg_consensus_length'],
            'masking_method': 'hard_masking',
            'masking_tool': 'RepeatMasker_with_consensus',
            'stats_source': masking_stats.get('stats_source', 'unknown'),
            'repeatmasker_stats': masking_stats.get('repeatmasker_stats', None)
        }
        
        return stats
    
    def _generate_output_files(self, hard_masked_genome: str, stats: Dict, consensus_library: str) -> Dict:
        """生成输出文件"""
        output_dir = Path(self.config.output_dir)
        
        # 创建最终输出文件（genome_final_masked.fa用于RECON）
        final_genome_file = output_dir / "genome_final_masked.fa"
        import shutil
        shutil.copy2(hard_masked_genome, final_genome_file)
        
        # 生成统计报告
        stats_file = output_dir / "phase4_masking_stats.txt"
        with open(stats_file, 'w') as f:
            f.write("# Phase 4: High-Quality Consensus Hard Masking Statistics\n")
            f.write(f"# Generated: {Path().absolute()}\n")
            f.write(f"# Statistics source: {stats.get('stats_source', 'unknown')}\n\n")
            
            f.write("## Masking Overview\n")
            f.write(f"Genome size: {stats['genome_size']:,} bp\n")
            f.write(f"Masked bases: {stats['hard_masked_bp']:,} bp\n")
            f.write(f"Masked percentage: {stats['hard_masked_percent']:.2f}%\n")
            f.write(f"Masking method: {stats['masking_method']}\n\n")
            
            # 添加RepeatMasker原始统计信息
            if stats.get('repeatmasker_stats'):
                rm_stats = stats['repeatmasker_stats']
                f.write("## RepeatMasker Original Statistics\n")
                f.write(f"File analyzed: {rm_stats.get('file_name', 'unknown')}\n")
                f.write(f"Total sequences: {rm_stats.get('num_sequences', 'unknown')}\n")
                f.write(f"Total length: {rm_stats.get('total_length', 'unknown'):,} bp\n")
                f.write(f"Bases masked: {rm_stats.get('bases_masked', 'unknown'):,} bp\n")
                f.write(f"Masked percentage: {rm_stats.get('masked_percent', 'unknown'):.2f}%\n")
                f.write(f"Total elements: {rm_stats.get('total_elements', 'unknown')}\n\n")
            
            f.write("## Consensus Library Used\n")
            f.write(f"Total consensus sequences: {stats['consensus_used']}\n")
            f.write(f"Average quality score: {stats['avg_consensus_quality']:.3f}\n")
            f.write(f"Average copy number: {stats['avg_consensus_copies']:.1f}\n")
            f.write(f"Average consensus length: {stats['avg_consensus_length']:.0f} bp\n\n")
            
            f.write("## Output Files\n")
            f.write(f"Hard-masked genome: {hard_masked_genome}\n")
            f.write(f"Final masked genome: {final_genome_file}\n")
            f.write(f"Consensus library: {consensus_library}\n\n")
            
            if stats.get('stats_source') == 'repeatmasker_tbl':
                f.write("## Notes\n")
                f.write("Statistics are based on RepeatMasker .tbl file analysis.\n")
                f.write("This provides the most accurate masking statistics.\n")
            else:
                f.write("## Notes\n")
                f.write("Statistics are based on manual sequence analysis.\n")
                f.write("RepeatMasker .tbl file was not available for accurate statistics.\n")
        
        output_files = {
            'hard_masked_genome': hard_masked_genome,
            'final_masked_genome': str(final_genome_file),
            'consensus_library': consensus_library,
            'stats_file': str(stats_file)
        }
        
        logger.info(f"Generated output files: {list(output_files.keys())}")
        return output_files

# ==============================================================================
# END OF UPDATED PHASE 4: CONSENSUS-ONLY SOFT MASKING
# ==============================================================================

# The GenomeMasker class has been completely rewritten to use only high-quality
# consensus sequences from Phase 3, with soft masking via RepeatMasker.
# 
# Key changes:
# 1. No longer depends on Phase 1 RepeatMasker results
# 2. Uses only high-quality consensus sequences (quality >= 0.8, copies >= 3)  
# 3. Performs soft masking (-xsmall) instead of hard masking
# 4. Filters consensus by complexity to exclude low-complexity sequences
# 5. Generates genome_final_masked.fa for downstream RECON usage
# 
# All methods below this line are DEPRECATED and should not be used.

# ==============================================================================
# DEPRECATED METHODS (Phase 1 dependent - no longer used in consensus-only mode)
# ==============================================================================

# The following methods are deprecated and not used in the new consensus-only
# soft masking approach. They are retained for reference but should not be called.

    def _collect_high_quality_ids_deprecated(self, phase1_output: Dict, phase3_output: Dict) -> Set[str]:
        """收集高质量序列的ID"""
        high_quality_ids = set()
        
        # 添加所有A类序列
        for seq in phase1_output.get('a_sequences', []):
            high_quality_ids.add(seq['id'])
        
        # 添加高分B类序列（分数>0.65）
        scores = phase1_output.get('scores', {})
        for seq in phase1_output.get('b_sequences', []):
            if scores.get(seq['id'], {}).get('final', 0) > 0.65:
                high_quality_ids.add(seq['id'])
        
        # 添加最终consensus序列的原始ID
        for seq in phase3_output.get('masking_library', []):
            if 'seed_id' in seq:
                high_quality_ids.add(seq['seed_id'])
        
        return high_quality_ids
    
    def _filter_repeatmasker_results(self, rm_results: Dict, high_quality_ids: Set[str]) -> Dict:
        """过滤RepeatMasker结果，只保留高可信度的hits"""
        filtered = {}
        total_hits = 0
        filtered_hits = 0
        
        for seq_id, result in rm_results.items():
            # 只处理高质量序列
            if seq_id not in high_quality_ids:
                continue
            
            filtered_result = {
                'hits': [],
                'total_coverage': 0,
                'hit_count': 0
            }
            
            for hit in result.get('hits', []):
                total_hits += 1
                
                # 过滤条件
                if (hit.get('score', 0) >= self.min_rm_score and
                    hit.get('identity', 0) >= self.min_identity and
                    (hit.get('end', 0) - hit.get('start', 0)) >= self.min_length):
                    
                    filtered_result['hits'].append(hit)
                    filtered_result['hit_count'] += 1
                    filtered_result['total_coverage'] += hit.get('end', 0) - hit.get('start', 0)
                    filtered_hits += 1
            
            if filtered_result['hits']:
                filtered[seq_id] = filtered_result
        
        logger.info(f"Filtered RepeatMasker hits: {filtered_hits}/{total_hits} "
                   f"({filtered_hits/total_hits*100:.1f}%) passed quality threshold")
        
        return filtered
    
    def _count_total_hits(self, rm_results: Dict) -> int:
        """统计总hits数"""
        return sum(len(result.get('hits', [])) for result in rm_results.values())
    
    def _generate_preliminary_mask(self, genome_file: str, filtered_rm_results: Dict) -> str:
        """使用过滤后的RepeatMasker结果生成初步masked genome"""
        output_file = Path(self.config.output_dir) / "genome_preliminary_masked.fa"
        
        # 读取基因组
        genome_dict = {}
        for record in SeqIO.parse(genome_file, "fasta"):
            genome_dict[record.id] = list(str(record.seq))
        
        # 收集所有需要mask的区域
        mask_regions = []
        for seq_id, result in filtered_rm_results.items():
            for hit in result['hits']:
                mask_regions.append({
                    'chrom': hit['chrom'],
                    'start': hit['start'] - 1,  # 转换为0-based
                    'end': hit['end'],
                    'score': hit.get('score', 0),
                    'identity': hit.get('identity', 0)
                })
        
        # 按染色体和位置排序
        mask_regions.sort(key=lambda x: (x['chrom'], x['start']))
        
        # 合并重叠区域
        merged_regions = self._merge_overlapping_regions(mask_regions)
        
        # 应用hard mask (将重复区域替换为N)
        masked_bp = 0
        for region in merged_regions:
            chrom = region['chrom']
            if chrom in genome_dict:
                for i in range(region['start'], min(region['end'], len(genome_dict[chrom]))):
                    if genome_dict[chrom][i] not in 'Nn':
                        genome_dict[chrom][i] = 'N'  # Hard mask: 用N替换
                        masked_bp += 1
        
        # 写出masked genome
        masked_records = []
        for chrom_id, seq_list in genome_dict.items():
            masked_seq = ''.join(seq_list)
            record = SeqRecord(
                Seq(masked_seq),
                id=chrom_id,
                description=f"preliminary masked"
            )
            masked_records.append(record)
        
        with open(output_file, 'w') as f:
            SeqIO.write(masked_records, f, "fasta")
        
        total_bp = sum(len(seq) for seq in genome_dict.values())
        logger.info(f"Preliminary masking: {masked_bp:,} bp ({masked_bp/total_bp*100:.2f}%)")
        
        return str(output_file)
    
    def _merge_overlapping_regions(self, regions: List[Dict]) -> List[Dict]:
        """合并重叠的mask区域"""
        if not regions:
            return []
        
        merged = []
        current = regions[0].copy()
        
        for region in regions[1:]:
            if region['chrom'] == current['chrom'] and region['start'] <= current['end']:
                # 重叠，合并
                current['end'] = max(current['end'], region['end'])
                current['score'] = max(current['score'], region['score'])
                current['identity'] = max(current['identity'], region['identity'])
            else:
                # 不重叠，保存当前并开始新的
                merged.append(current)
                current = region.copy()
        
        merged.append(current)
        return merged
    
    def _prepare_consensus_library(self, phase3_output: Dict) -> List[Dict]:
        """准备用于第二轮masking的consensus库"""
        consensus_library = []
        
        # 使用masking库（95%去冗余）
        for seq_data in phase3_output.get('masking_library', []):
            consensus_library.append({
                'id': seq_data['id'],
                'sequence': seq_data['sequence'],
                'num_copies': seq_data.get('num_copies', 1)
            })
        
        return consensus_library
    
    def _perform_second_round_masking(self, preliminary_masked: str, 
                                     consensus_library: List[Dict]) -> str:
        """使用最终consensus序列进行第二轮精细masking"""
        output_file = Path(self.config.output_dir) / "genome_final_masked.fa"
        
        # 创建临时consensus库文件
        with tempfile.NamedTemporaryFile(mode='w', suffix='.fa', delete=False) as lib_file:
            for seq_data in consensus_library:
                lib_file.write(f">{seq_data['id']}\n{seq_data['sequence']}\n")
            lib_file_path = lib_file.name
        
        # 运行RepeatMasker进行第二轮masking
        try:
            # 构建RepeatMasker命令（为RECON优化的参数）
            cmd = [
                self.config.repeatmasker_exe,
                '-lib', lib_file_path,
                '-no_is',  # 不搜索细菌插入序列
                '-nolow',  # 不mask低复杂度区域，让RECON处理
                '-cutoff', '200',  # 降低cutoff，保留更多候选
                '-s',      # 慢速但更敏感的搜索
                '-small',  # 返回较短的匹配，保留RECON发现机会
                '-pa', str(max(1, self.config.threads // 2)),
                '-dir', str(Path(self.config.output_dir) / 'round2_rm'),
                preliminary_masked
            ]
            
            # 如果启用RECON友好模式，添加额外参数
            if self.round2_strict_mode == False:
                cmd.extend([
                    '-frag', '20'  # 允许片段化匹配
                    # Note: removed invalid -maxsize option that doesn't exist in RepeatMasker
                ])
            
            logger.info("Using RECON-optimized parameters for second round masking:")
            logger.info(f"  - Identity threshold: {self.round2_min_identity}%")
            logger.info(f"  - Sensitive mode: enabled")
            logger.info(f"  - Fragment matching: enabled")
            
            logger.info(f"Running second round RepeatMasker with {len(consensus_library)} consensus sequences")
            
            # 创建输出目录
            os.makedirs(Path(self.config.output_dir) / 'round2_rm', exist_ok=True)
            
            # 运行RepeatMasker
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=7200)  # 2小时超时
            
            if result.returncode != 0:
                logger.warning(f"Second round RepeatMasker failed: {result.stderr}")
                # 如果失败，返回初步masked结果
                return preliminary_masked
            
            # 找到masked输出文件
            masked_file = Path(self.config.output_dir) / 'round2_rm' / f"{Path(preliminary_masked).name}.masked"
            
            if masked_file.exists():
                # 移动到最终位置
                import shutil
                shutil.move(str(masked_file), str(output_file))
                logger.info(f"Second round masking completed: {output_file}")
                return str(output_file)
            else:
                logger.warning("Second round masked file not found, using preliminary result")
                return preliminary_masked
                
        except Exception as e:
            logger.error(f"Second round masking failed: {e}")
            return preliminary_masked
        finally:
            # 清理临时文件
            if os.path.exists(lib_file_path):
                os.unlink(lib_file_path)
    
    def _calculate_masking_stats(self, original_genome: str, 
                                preliminary_masked: str, 
                                final_masked: str) -> Dict:
        """计算masking统计信息"""
        stats = {
            'genome_size': 0,
            'preliminary_masked_bp': 0,
            'final_masked_bp': 0,
            'preliminary_masked_percent': 0,
            'final_masked_percent': 0,
            'additional_masked_bp': 0,
            'chromosomes': {}
        }
        
        # 分析原始基因组
        for record in SeqIO.parse(original_genome, "fasta"):
            seq_str = str(record.seq)
            stats['genome_size'] += len(seq_str)
        
        # 分析初步masked (现在是hard mask，只计算N)
        if os.path.exists(preliminary_masked):
            for record in SeqIO.parse(preliminary_masked, "fasta"):
                seq_str = str(record.seq)
                masked_count = sum(1 for c in seq_str if c in 'Nn')  # 只计算N
                stats['preliminary_masked_bp'] += masked_count
                stats['chromosomes'][record.id] = {
                    'size': len(seq_str),
                    'preliminary_masked': masked_count
                }
        
        # 分析最终masked (现在是hard mask，只计算N)
        if os.path.exists(final_masked):
            for record in SeqIO.parse(final_masked, "fasta"):
                seq_str = str(record.seq)
                masked_count = sum(1 for c in seq_str if c in 'Nn')  # 只计算N
                stats['final_masked_bp'] += masked_count
                if record.id in stats['chromosomes']:
                    stats['chromosomes'][record.id]['final_masked'] = masked_count
        
        # 计算百分比
        if stats['genome_size'] > 0:
            stats['preliminary_masked_percent'] = stats['preliminary_masked_bp'] / stats['genome_size'] * 100
            stats['final_masked_percent'] = stats['final_masked_bp'] / stats['genome_size'] * 100
            stats['additional_masked_bp'] = stats['final_masked_bp'] - stats['preliminary_masked_bp']
        
        return stats
    
    def _generate_output_files(self, preliminary_masked: str, 
                              final_masked: str, 
                              stats: Dict) -> Dict:
        """生成输出文件和报告"""
        output_files = {
            'soft_masked_genome': preliminary_masked,  # 主要输出文件
            'preliminary_masked': preliminary_masked,   # 兼容性
            'final_masked': final_masked,               # 兼容性
            'stats_file': str(Path(self.config.output_dir) / 'masking_statistics.txt'),
            'summary_file': str(Path(self.config.output_dir) / 'masking_summary.txt')
        }
        
        # 调试：检查stats的类型
        logger.debug(f"Stats type: {type(stats)}, content: {stats}")
        
        # 写统计文件
        with open(output_files['stats_file'], 'w') as f:
            f.write("Genome Masking Statistics\n")
            f.write("="*60 + "\n\n")
            
            # 安全地访问字典
            if isinstance(stats, dict):
                f.write(f"Genome size: {stats.get('genome_size', 'N/A'):,} bp\n")
            else:
                f.write(f"Error: stats is not a dict, type: {type(stats)}\n")
                return output_files
            f.write(f"Soft masked: {stats['soft_masked_bp']:,} bp ({stats['soft_masked_percent']:.2f}%)\n")
            f.write(f"Consensus used: {stats['consensus_used']}\n")
            f.write(f"Average consensus quality: {stats.get('avg_consensus_quality', 'N/A')}\n")
            f.write(f"Average consensus copies: {stats.get('avg_consensus_copies', 'N/A')}\n")
            f.write(f"Average consensus length: {stats.get('avg_consensus_length', 'N/A')}\n")
            f.write(f"Masking method: {stats.get('masking_method', 'soft_masking')}\n")
            f.write(f"Masking tool: {stats.get('masking_tool', 'RepeatMasker')}\n")
        
        # 写摘要文件
        with open(output_files['summary_file'], 'w') as f:
            f.write("Phase 4: Genome Masking Summary\n")
            f.write("="*60 + "\n\n")
            f.write("High-quality consensus soft masking strategy:\n")
            f.write("- Uses high-quality consensus sequences from Phase 3\n")
            f.write("- RepeatMasker soft masking with strict parameters\n\n")
            f.write(f"Results:\n")
            f.write(f"- Total genome soft-masked: {stats['soft_masked_percent']:.2f}%\n")
            f.write(f"- High-quality consensus sequences used: {stats['consensus_used']}\n")
            f.write(f"- Balanced specificity and sensitivity achieved\n")
        
        logger.info(f"Output files written to {self.config.output_dir}")
        
        return output_files
    
    def _prepare_renamed_genome(self) -> Tuple[str, str]:
        """
        准备重命名的基因组文件以避免RepeatMasker ID问题
        
        Returns:
            (重命名后的基因组文件路径, 映射文件路径)
        """
        logger.info("Preparing renamed genome to avoid RepeatMasker ID conflicts")
        
        # 创建基因组重命名器
        work_dir = Path(self.config.output_dir) / "genome_renaming"
        work_dir.mkdir(exist_ok=True)
        
        self.genome_renamer = GenomeRenamer(str(work_dir))
        
        # 执行重命名
        self.renamed_genome_file, mapping_file = self.genome_renamer.rename_genome_for_repeatmasker(
            self.config.genome_file
        )
        
        # 记录统计信息
        stats = self.genome_renamer.get_mapping_stats()
        logger.info(f"Renamed {stats['total_sequences']} sequences")
        logger.info(f"Original: {stats['original_genome']}")
        logger.info(f"Renamed: {stats['renamed_genome']}")
        logger.info(f"Mapping file: {mapping_file}")
        
        return self.renamed_genome_file, mapping_file
    
    def _restore_original_ids(self, output_files: Dict) -> Dict:
        """
        还原所有输出文件的ID到原始格式
        
        Args:
            output_files: 原始输出文件字典
            
        Returns:
            还原后的输出文件字典
        """
        logger.info("Restoring original sequence IDs in all output files")
        
        if not self.genome_renamer or not self.mapping_file:
            logger.error("No genome renamer or mapping file available")
            return output_files
        
        restored_files = {}
        restored_dir = Path(self.config.output_dir) / "restored_output"
        restored_dir.mkdir(exist_ok=True)
        
        # 需要还原ID的文件类型
        files_to_restore = [
            'soft_masked_genome',
            'preliminary_masked', 
            'final_masked'
        ]
        
        for file_key in files_to_restore:
            if file_key in output_files:
                original_file = output_files[file_key]
                if original_file and Path(original_file).exists():
                    try:
                        restored_file = restored_dir / f"restored_{Path(original_file).name}"
                        
                        # 还原ID
                        final_restored = self.genome_renamer.restore_genome_ids(
                            original_file, str(restored_file)
                        )
                        
                        restored_files[file_key] = final_restored
                        logger.info(f"Restored {file_key}: {Path(original_file).name} → {Path(final_restored).name}")
                        
                    except Exception as e:
                        logger.error(f"Failed to restore {file_key}: {e}")
                        restored_files[file_key] = original_file  # 使用原始文件
                else:
                    restored_files[file_key] = original_file
            else:
                if file_key in output_files:
                    restored_files[file_key] = output_files[file_key]
        
        # 复制不需要还原的文件
        for key, value in output_files.items():
            if key not in restored_files:
                restored_files[key] = value
        
        # 创建一个特殊的主输出文件链接
        if 'soft_masked_genome' in restored_files:
            main_output = Path(self.config.output_dir) / "genome_final_masked.fa"
            main_restored = Path(restored_files['soft_masked_genome'])
            
            # 创建符号链接或复制文件
            try:
                if main_output.exists():
                    main_output.unlink()
                main_output.symlink_to(main_restored.resolve())
                logger.info(f"Created main output link: {main_output} → {main_restored.name}")
            except Exception:
                # 如果符号链接失败，复制文件
                import shutil
                shutil.copy2(str(main_restored), str(main_output))
                logger.info(f"Copied main output: {main_output}")
        
        logger.info(f"ID restoration completed. Restored files in: {restored_dir}")
        return restored_files
