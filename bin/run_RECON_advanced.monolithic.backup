#!/usr/bin/env perl
use strict;
use warnings;
use File::Path qw(make_path remove_tree);
use File::Spec;
use File::Basename qw(dirname basename);
use Cwd qw(getcwd abs_path);
use Parallel::ForkManager;
use POSIX qw(ceil strftime);
use List::Util qw(min max sum);  # Import min/max functions from List::Util
use Time::HiRes qw(time gettimeofday tv_interval);
use constant {
    MIN_THREADS => 1,
    DEFAULT_EVALUE => '1e-5',
    BLAST_DB_TYPE => 'nucl',
    CHUNK_SIZE => 500000,      # 500kb chunks
    OVERLAP_SIZE => 0,         # No overlap between chunks
    FRAGMENT_SIZE => 40000,    # 40kb fragments for sampling
    INITIAL_SAMPLE => 30,      # Initial 30Mb sampling
    SAMPLE_MULTIPLIER => 3,    # X3 increment for each round
};

# ===================================================================
# LOGGING SYSTEM
# ===================================================================
my $LOG_FILE = "recon_advanced.log";
my $LOG_LEVEL = "INFO";  # DEBUG, INFO, WARN, ERROR
my $PIPELINE_START_TIME = time();
my $CURRENT_STEP_START = time();

sub log_message {
    my ($level, $message, $details) = @_;
    $details ||= "";
    
    # Check log level
    my %levels = (DEBUG => 0, INFO => 1, WARN => 2, ERROR => 3);
    return if $levels{$level} < $levels{$LOG_LEVEL};
    
    my $timestamp = strftime("%Y-%m-%d %H:%M:%S", localtime());
    my $elapsed = sprintf("%.1f", time() - $PIPELINE_START_TIME);
    my $step_elapsed = sprintf("%.1f", time() - $CURRENT_STEP_START);
    
    my $log_entry = sprintf("[%s] [%s] [+%ss] [step:%ss] %s", 
        $timestamp, $level, $elapsed, $step_elapsed, $message);
    
    if ($details) {
        $log_entry .= "\n    Details: $details";
    }
    
    # Print to both STDOUT and log file
    print "$log_entry\n";
    
    open(my $fh, '>>', $LOG_FILE) or warn "Cannot open log file: $!";
    print $fh "$log_entry\n";
    close($fh);
}

sub log_step_start {
    my ($step_name, $description) = @_;
    $CURRENT_STEP_START = time();
    log_message("INFO", "=== STARTING: $step_name ===", $description);
}

sub log_step_end {
    my ($step_name, $status, $details) = @_;
    my $step_time = sprintf("%.1f", time() - $CURRENT_STEP_START);
    my $status_msg = $status ? "COMPLETED" : "FAILED";
    log_message("INFO", "=== $status_msg: $step_name (${step_time}s) ===", $details);
}

sub log_progress {
    my ($current, $total, $description) = @_;
    my $percent = sprintf("%.1f", ($current / $total) * 100);
    log_message("INFO", "Progress: $current/$total ($percent%) - $description");
}

sub log_resource_usage {
    my ($context) = @_;
    
    # Get memory usage
    my $memory_info = `free -m 2>/dev/null | grep Mem:` || "";
    chomp $memory_info;
    
    # Get disk usage of current directory
    my $disk_usage = `du -sh . 2>/dev/null` || "";
    chomp $disk_usage;
    
    # Get load average
    my $load_avg = `uptime 2>/dev/null | grep -o 'load average:.*'` || "";
    chomp $load_avg;
    
    my $details = join(" | ", 
        $memory_info ? "Memory: $memory_info" : "",
        $disk_usage ? "Disk: $disk_usage" : "",
        $load_avg ? "Load: $load_avg" : ""
    );
    
    log_message("DEBUG", "Resource usage [$context]", $details);
}

# Forward declaration of functions that are called before their definition
sub run_sampling_track_independent {
    my ($genome_file, $bed_files_ref, $masked_consensi, $output_dir, 
        $threads, $cpu_threads, $genome_size) = @_;
    
    my @bed_files = @$bed_files_ref;
    my $current_dir = getcwd();
    chdir $output_dir or die "Cannot change to $output_dir: $!\n";
    
    print "Starting independent sampling track with progressive masking...\n";
    
    # Initialize cumulative exclusion mask (will be created in first round)
    my $exclusion_mask;
    
    # Initialize cumulative consensi list for progressive masking
    my @cumulative_consensi;
    push @cumulative_consensi, $masked_consensi if -s $masked_consensi;
    
    # Sampling parameters
    my $round = 1;
    my @sample_sizes = (30, 90, 270, 540);  # MB
    my $accumulated_mask = $exclusion_mask;
    my %previous_metrics;
    
    for my $sample_size_mb (@sample_sizes) {
        last if $round > 4;  # Max 4 rounds
        
        print "\n=== Sampling Round $round (${sample_size_mb}MB) ===\n";
        
        my $round_dir = "round_$round";
        make_path($round_dir);
        
        # Change to round directory for all work
        chdir $round_dir or die "Cannot change to $round_dir: $!\n";
        
        # Create initial exclusion mask from BED files (in round directory)
        if ($round == 1) {
            $exclusion_mask = "initial_exclusion.bed";
            if (@bed_files) {
                my $bed_list = join(" ", @bed_files);
                run_cmd("cat $bed_list | bedtools sort | bedtools merge > $exclusion_mask");
            } else {
                open(my $fh, '>', $exclusion_mask);
                close($fh);
            }
            $accumulated_mask = $exclusion_mask;
        } else {
            # For subsequent rounds, copy accumulated mask to this round
            $exclusion_mask = "accumulated_mask.bed";
            if (defined $accumulated_mask && -f "../$accumulated_mask") {
                run_cmd("cp ../$accumulated_mask $exclusion_mask");
            } else {
                # Fallback to empty mask
                open(my $fh, '>', $exclusion_mask);
                close($fh);
            }
        }
        
        # Check available genome space
        my $available_size = calculate_available_size($genome_file, $accumulated_mask);
        my $required_size = $sample_size_mb * 1000000;
        
        if ($available_size < $required_size * 1.5) {
            print "Insufficient genome space for ${sample_size_mb}MB sampling\n";
            chdir "..";
            last;
        }
        
        # Sample genome (in round directory)
        my $sample_file = "sample.fa";
        perform_adaptive_sampling($genome_file, $exclusion_mask, $accumulated_mask, 
                                 $sample_file, $sample_size_mb, FRAGMENT_SIZE);
        
        # Apply progressive masking with cumulative consensi (in round directory)
        my $masked_sample = "sample_masked.fa";
        run_trf_masking($sample_file, $masked_sample);
        
        for my $consensi (@cumulative_consensi) {
            next unless -s $consensi;
            print "Applying progressive masking with: $consensi\n";
            my $temp_out = "${masked_sample}.tmp";
            run_repeatmasker($masked_sample, $consensi, $temp_out, $cpu_threads);
            run_cmd("mv $temp_out $masked_sample") if -s $temp_out;
        }
        
        # Run BLAST and create MSP (in round directory)
        my $blast_output = "self_alignment.blast";
        create_blast_database($masked_sample, "sample_db");
        run_rmblastn_self_alignment($masked_sample, $blast_output, $threads, "sample_db");
        run_cmd("MSPCollect.pl $blast_output > msp.out");
        
        if (!-s "msp.out") {
            print "No repeats found in round $round\n";
            chdir "..";
            $round++;
            next;
        }
        
        # Run RECON (already in round directory)
        create_seq_name_list($masked_sample);
        run_cmd("cp msp.out msp.out.backup");  # Keep original
        my $k_param = determine_k_parameter("msp.out");
        run_recon_pipeline($k_param);
        
        # Build consensi for this round
        if (-d "summary" && -s "summary/families") {
            # Use the local masked sample file for this round
            my $local_genome = abs_path($masked_sample);
            run_cmd("build_for_RECON ./ $local_genome $cpu_threads");
            
            if (-s "consensi.fa") {
                push @cumulative_consensi, abs_path("consensi.fa");
            }
        }
        
        # Generate sampled regions BED file for this round (in round directory)
        # This will be used to update the accumulated mask for next round
        my $sampled_regions_file = "sampled_regions.bed";
        # TODO: This should extract sampled regions from the sampling step
        # For now, create empty file as placeholder
        unless (-f $sampled_regions_file) {
            open(my $fh, '>', $sampled_regions_file);
            close($fh);
        }
        
        # Return to sampling_track directory
        chdir "..";
        
        # Calculate metrics for stopping decision
        my %current_metrics = analyze_round_metrics($round, $round_dir, \%previous_metrics);
        
        # Stopping criteria evaluation (from round 2 onwards)
        if ($round >= 2) {
            my $should_stop = evaluate_stopping_criteria(\%previous_metrics, \%current_metrics, 
                                                        $round, $sample_size_mb);
            if ($should_stop) {
                print "Stopping criteria met. Ending sampling at round $round\n";
                last;
            }
        }
        
        # Update for next round
        %previous_metrics = %current_metrics;
        if (-s "$round_dir/$sampled_regions_file") {
            update_accumulated_mask(\$accumulated_mask, "$round_dir/$sampled_regions_file");
        }
        $round++;
    }
    
    chdir $current_dir;
}

sub analyze_round_metrics {
    my ($round, $round_dir, $prev_metrics_ref) = @_;
    my %metrics;
    
    # Count families
    if (-f "$round_dir/summary/families") {
        $metrics{total_families} = `wc -l < $round_dir/summary/families`;
        chomp $metrics{total_families};
        
        # Count singletons
        open(my $fh, '<', "$round_dir/summary/eles");
        my %family_counts;
        while (<$fh>) {
            next if /^#/;
            my @fields = split;
            $family_counts{$fields[0]}++;
        }
        close($fh);
        
        $metrics{singletons} = scalar(grep { $family_counts{$_} == 1 } keys %family_counts);
        
        # P95 family size
        my @sizes = sort { $b <=> $a } values %family_counts;
        $metrics{p95_family_size} = $sizes[int(@sizes * 0.05)] || 0;
    }
    
    # MSP density
    if (-f "$round_dir/msp.out") {
        $metrics{msp_lines} = `wc -l < $round_dir/msp.out`;
        chomp $metrics{msp_lines};
    }
    
    # Calculate derived metrics
    if ($prev_metrics_ref && %$prev_metrics_ref) {
        $metrics{new_families} = ($metrics{total_families} || 0) - 
                                 ($prev_metrics_ref->{total_families} || 0);
    }
    
    return %metrics;
}

sub evaluate_stopping_criteria {
    my ($prev_metrics, $curr_metrics, $round, $sample_mb) = @_;
    
    # Calculate rates
    my $new_families_per_100mb = 0;
    if ($round == 2) {  # 30->90MB transition
        $new_families_per_100mb = ($curr_metrics->{new_families} || 0) * 100 / 60;
    } elsif ($round == 3) {  # 90->270MB transition  
        $new_families_per_100mb = ($curr_metrics->{new_families} || 0) * 100 / 180;
    } elsif ($round == 4) {  # 270->540MB transition
        $new_families_per_100mb = ($curr_metrics->{new_families} || 0) * 100 / 270;
    }
    
    my $singleton_ratio = 0;
    if ($curr_metrics->{total_families} && $curr_metrics->{total_families} > 0) {
        $singleton_ratio = ($curr_metrics->{singletons} || 0) / $curr_metrics->{total_families};
    }
    
    my $p95_change = 0;
    if ($prev_metrics->{p95_family_size} && $prev_metrics->{p95_family_size} > 0) {
        $p95_change = (($curr_metrics->{p95_family_size} || 0) - $prev_metrics->{p95_family_size}) 
                     / $prev_metrics->{p95_family_size};
    }
    
    my $msp_density_change = 0;
    if ($prev_metrics->{msp_lines} && $prev_metrics->{msp_lines} > 0) {
        $msp_density_change = (($curr_metrics->{msp_lines} || 0) - $prev_metrics->{msp_lines})
                             / $prev_metrics->{msp_lines};
    }
    
    print "Round $round metrics:\n";
    print "  New families per 100MB: $new_families_per_100mb\n";
    print "  Singleton ratio: " . sprintf("%.2f", $singleton_ratio) . "\n";
    print "  P95 family size change: " . sprintf("%.2f%%", $p95_change * 100) . "\n";
    print "  MSP density change: " . sprintf("%.2f%%", $msp_density_change * 100) . "\n";
    
    # Continue conditions (any one is sufficient)
    if ($new_families_per_100mb >= 30) {
        print "Continuing: High new family discovery rate\n";
        return 0;
    }
    
    if ($singleton_ratio <= 0.7) {
        print "Continuing: Singleton ratio acceptable\n";
        return 0;
    }
    
    if ($p95_change >= 0.2) {
        print "Continuing: Family size still growing\n";
        return 0;
    }
    
    if ($msp_density_change >= 0.3) {
        print "Continuing: MSP density increasing\n";
        return 0;
    }
    
    # Stop if all thresholds suggest diminishing returns
    print "Stopping: All metrics suggest diminishing returns\n";
    return 1;
}

sub update_accumulated_mask {
    my ($mask_ref, $new_regions) = @_;
    
    return unless -s $new_regions;
    
    my $temp_mask = "temp_accumulated.bed";
    run_cmd("cat $$mask_ref $new_regions | bedtools sort | bedtools merge > $temp_mask");
    $$mask_ref = $temp_mask;
}

die "Usage: $0 threads genome_file genome_size\n"
    unless @ARGV == 3;

my ($threads, $genome_file, $genome_size) = @ARGV;

# Initialize logging
log_step_start("PIPELINE INITIALIZATION", "Advanced RECON pipeline with dual-track processing");
log_message("INFO", "Command line arguments", "threads=$threads, genome=$genome_file, size=$genome_size");

# Validate arguments
die "Threads must be at least " . MIN_THREADS . "\n" unless $threads >= MIN_THREADS;
die "Genome file $genome_file not found\n" unless -f $genome_file;
die "Invalid genome size\n" unless $genome_size =~ /^\d+$/;

# Convert genome_file to absolute path before any operations
my $abs_genome_file = File::Spec->rel2abs($genome_file);
log_message("INFO", "Using absolute path", "genome_file=$abs_genome_file");

my $cpu_threads = int($threads/4);
$cpu_threads = MIN_THREADS if $cpu_threads < MIN_THREADS;

log_message("INFO", "Resource allocation", "threads=$threads, cpu_threads=$cpu_threads");
log_resource_usage("initialization");
print "Genome file: $abs_genome_file\n";
print "Using $threads threads total, $cpu_threads per BLAST job\n";

# Working directory is already RECON, no need to create round-X

# Main pipeline steps
my $start_time = time();

# Check for Refiner masked genome and bed files
my ($masked_genome, $bed_files_ref, $use_full_genome) = find_input_files($abs_genome_file);
my @bed_files = @$bed_files_ref;

log_step_end("PIPELINE INITIALIZATION", 1, "Ready to start RECON processing");

if ($use_full_genome) {
    log_step_start("SINGLE TRACK MODE", "Small genome - full masked genome processing");
    log_message("INFO", "Genome mode", "Single-track (small genome)");
    log_message("INFO", "Input files", "masked_genome=$masked_genome");
    
    # For small genomes, run directly on masked genome
    run_full_genome_track($masked_genome, $threads, $cpu_threads);
    
    log_step_end("SINGLE TRACK MODE", 1, "Single-track processing completed");
} else {
    log_step_start("DUAL TRACK MODE", "Large genome - independent dual-track processing");
    log_message("INFO", "Genome mode", "Dual-track (large genome)");
    log_message("INFO", "Input files", "masked_genome=$masked_genome, original=$abs_genome_file");
    log_message("INFO", "BED exclusion files", scalar(@bed_files) . " files: " . join(", ", @bed_files));
    
    # Track 1: Process masked genome
    log_step_start("TRACK 1 - MASKED GENOME", "Processing masked genome for high-quality consensus");
    my $masked_track_dir = "masked_track";
    make_path($masked_track_dir) unless -d $masked_track_dir;
    
    run_masked_track_independent($masked_genome, $masked_track_dir, $threads, $cpu_threads);
    
    log_step_end("TRACK 1 - MASKED GENOME", 1, "Masked genome processing completed");
    
    # Track 2: Multi-round sampling with progressive masking
    log_step_start("TRACK 2 - SAMPLING", "Multi-round sampling with progressive masking");
    my $sampling_track_dir = "sampling_track";
    make_path($sampling_track_dir) unless -d $sampling_track_dir;
    
    eval {
        # Pass masked track consensi for initial masking
        my $masked_consensi = "$masked_track_dir/consensi.fa";
        log_message("INFO", "Starting sampling track", "initial_consensi=$masked_consensi");
        
        run_sampling_track_independent($abs_genome_file, \@bed_files, 
                                      $masked_consensi, $sampling_track_dir, 
                                      $threads, $cpu_threads, $genome_size);
        
        log_step_end("TRACK 2 - SAMPLING", 1, "Sampling track completed successfully");
    };
    if ($@) {
        log_message("WARN", "Sampling track failed", "error=$@");
        log_message("INFO", "Fallback strategy", "Masked track results are still available");
        log_step_end("TRACK 2 - SAMPLING", 0, "Sampling track failed, using masked track only");
    }
    
    log_step_end("DUAL TRACK MODE", 1, "Both tracks completed");
}

# Summary report
log_step_start("PIPELINE SUMMARY", "Generating final statistics and report");

my $total_elapsed = time() - $PIPELINE_START_TIME;

if ($use_full_genome) {
    log_message("INFO", "Pipeline mode", "Single-track (small genome)");
    
    if (-s "consensi.fa") {
        my $count = `grep -c '^>' consensi.fa`;
        chomp $count;
        my $size = -s "consensi.fa";
        log_message("INFO", "Final results", "sequences=$count, file_size=${size} bytes");
        print "Generated $count consensus sequences\n";
    } else {
        log_message("WARN", "No consensus sequences generated", "consensi.fa missing or empty");
        print "Warning: No consensus sequences generated\n";
    }
} else {
    log_message("INFO", "Pipeline mode", "Dual-track (large genome)");
    
    # Report masked track results
    my $masked_count = 0;
    my $masked_size = 0;
    if (-s "masked_track/consensi.fa") {
        $masked_count = `grep -c '^>' masked_track/consensi.fa`;
        chomp $masked_count;
        $masked_size = -s "masked_track/consensi.fa";
        log_message("INFO", "Masked track results", "sequences=$masked_count, size=${masked_size} bytes");
        print "Masked track: $masked_count consensus sequences\n";
    } else {
        log_message("WARN", "Masked track failed", "no consensus sequences generated");
        print "Masked track: 0 consensus sequences (failed)\n";
    }
    
    # Report sampling track results
    my $sampling_rounds = 0;
    my $sampling_consensi = 0;
    my $sampling_total_size = 0;
    
    if (-d "sampling_track") {
        for my $round_dir (glob "sampling_track/round_*") {
            if (-s "$round_dir/consensi.fa") {
                $sampling_rounds++;
                my $count = `grep -c '^>' '$round_dir/consensi.fa'`;
                chomp $count;
                $sampling_consensi += $count;
                $sampling_total_size += -s "$round_dir/consensi.fa";
            }
        }
        
        if ($sampling_rounds > 0) {
            log_message("INFO", "Sampling track results", "rounds=$sampling_rounds, total_sequences=$sampling_consensi, total_size=${sampling_total_size} bytes");
        } else {
            log_message("WARN", "Sampling track failed", "no rounds completed successfully");
        }
    } else {
        log_message("WARN", "Sampling track not started", "directory not found");
    }
    
    print "Sampling track: $sampling_rounds rounds, $sampling_consensi total consensus sequences\n";
    
    my $total_sequences = $masked_count + $sampling_consensi;
    my $total_size = $masked_size + $sampling_total_size;
    log_message("INFO", "Combined results", "total_sequences=$total_sequences, total_size=${total_size} bytes");
}

log_message("INFO", "Pipeline completed", sprintf("total_time=%.1f seconds (%.1f minutes)", $total_elapsed, $total_elapsed/60));
log_resource_usage("pipeline_end");

print sprintf("\nRECON pipeline completed in %.1f seconds (%.1f minutes)\n", $total_elapsed, $total_elapsed/60);

# Create completion marker
open(my $fh_ok, '>', "RECON.ok") or die "Cannot create RECON.ok: $!\n";
print $fh_ok "Completed at " . localtime() . "\n";
close $fh_ok;

log_message("INFO", "Pipeline completion marker created", "file=RECON.ok");
log_step_end("PIPELINE SUMMARY", 1, "Advanced RECON pipeline finished successfully");

exit 0;

# ============================================================================
# Subroutines
# ============================================================================

sub find_input_files {
    my ($original_genome) = @_;
    
    # Determine base_dir based on the genome path
    # If the genome is already in refiner_output, adjust the base_dir accordingly
    my $base_dir;
    if ($original_genome =~ m{/RepeatScout/refiner_output/}) {
        # Genome is already the masked genome from Refiner
        $base_dir = $original_genome;
        $base_dir =~ s{/RepeatScout/refiner_output/.*$}{};
    } else {
        # Original logic for regular genome files
        $base_dir = dirname(dirname($original_genome));
    }
    
    # Get Refiner masked genome (guaranteed to exist from RepeatScout phase)
    my @possible_masked = (
        "$base_dir/RepeatScout/refiner_output/genome_final_masked.fa",
        "$base_dir/RepeatScout/refiner_output/genome_soft_masked.fa",
    );
    
    my $masked_genome;
    for my $masked_file (@possible_masked) {
        if (-s $masked_file) {
            print "Found Refiner masked genome: $masked_file\n";
            $masked_genome = $masked_file;
            last;
        }
    }
    
    die "ERROR: Refiner masked genome not found! RepeatScout phase may have failed.\n" unless $masked_genome;
    
    # Check for BED files to determine processing mode
    my $repeatscout_dir = "$base_dir/RepeatScout";
    my @bed_files = glob("$repeatscout_dir/tmp/*.bed $repeatscout_dir/refiner_output/*.bed");
    
    # Filter to include sampling bed files from RepeatScout
    @bed_files = grep { 
        $_ =~ /sample_\d+\.bed(\.validated)?$/ ||  # Sample regions (validated or not)
        $_ =~ /progressive_mask\.bed$/ ||         # Progressive masking regions
        $_ =~ /sampling_regions\.bed$/ ||         # General sampling regions
        $_ =~ /temp_merged_mask_\d+\.bed$/        # Temporary merged masks
    } @bed_files;
    
    print "Found sampling BED files from RepeatScout: " . scalar(@bed_files) . "\n";
    for my $bed_file (@bed_files) {
        print "  - $bed_file\n";
    }
    
    # Decision logic: BED files determine the processing mode
    if (!@bed_files) {
        # No BED files: use full masked genome mode
        print "No sampling BED files found, using full masked genome mode\n";
        return ($masked_genome, [], 1);  # Use full genome mode
    } else {
        # BED files found: use dual-track mode
        print "Using dual-track mode with " . scalar(@bed_files) . " exclusion BED files\n";
        return ($masked_genome, \@bed_files, 0);  # Use dual-track mode
    }
}

sub run_full_genome_track {
    my ($masked_genome, $threads, $cpu_threads) = @_;
    
    print "Processing full masked genome track (extracting unmasked regions)...\n";
    
    # Extract unmasked regions from the masked genome
    my $genome_file = "unmasked_regions.fa";
    if (!-f $genome_file) {
        print "Extracting unmasked regions from masked genome...\n";
        extract_unmasked_regions($masked_genome, $genome_file, 30);
        
        if (!-s $genome_file) {
            die "ERROR: Unmasked regions extraction failed or produced empty file\n";
        }
    } else {
        print "Unmasked regions file already exists, skipping extraction...\n";
    }
    
    # Create BLAST database from the unmasked regions
    if (!-f "genome_db.nhr") {
        print "Creating BLAST database from unmasked regions...\n";
        run_cmd("makeblastdb -in $genome_file -dbtype nucl -parse_seqids -out genome_db");
    } else {
        print "BLAST database already exists, skipping creation...\n";
    }
    
    # Run self-alignment with rmblastn on the unmasked regions
    my $blast_output = "unmasked_self_alignment.blast";
    my $blast_checkpoint = "rmblastn.ok";
    
    if (!-f $blast_checkpoint || !-f $blast_output) {
        print "Running rmblastn self-alignment on unmasked regions...\n";
        run_rmblastn_self_alignment($genome_file, $blast_output, $threads, "genome_db");
        # Create checkpoint after successful BLAST
        open(my $fh, '>', $blast_checkpoint) or die "Cannot create $blast_checkpoint: $!\n";
        print $fh "rmblastn completed at " . localtime() . "\n";
        close($fh);
    } else {
        print "rmblastn already completed (checkpoint found), skipping...\n";
    }
    
    # The blast output is already in MSP format, use it directly for RECON
    if (!-f "msp.out") {
        print "Setting up MSP file for RECON...\n";
        run_cmd("ln -sf $blast_output msp.out");
    }
    
    # Determine K parameter based on MSP size
    my $k_param = determine_k_parameter("msp.out");
    
    # Run RECON pipeline
    print "Running RECON pipeline with K=$k_param...\n";
    run_recon_pipeline_with_k("msp.out", $genome_file, $threads, $k_param);
    
    # Build consensi if families found
    if (-d "summary" && -s "summary/families") {
        print "Building consensi for full genome track...\n";
        my $local_genome = abs_path($genome_file);
        run_cmd("build_for_RECON ./ $local_genome $cpu_threads");
    }
}

sub run_masked_track_independent {
    my ($masked_genome, $output_dir, $threads, $cpu_threads) = @_;
    
    my $current_dir = getcwd();
    chdir $output_dir or die "Cannot change to $output_dir: $!\n";
    
    log_message("INFO", "Starting masked track processing", "dir=$output_dir, threads=$threads");
    log_resource_usage("masked_track_start");
    
    # Check if the entire process is already completed
    if (-f "masked_track_completed.ok") {
        log_message("INFO", "Masked track already completed", "checkpoint found: masked_track_completed.ok");
        chdir $current_dir;
        return;
    }
    
    # Check RECON pipeline checkpoints and resume from the last completed step
    my @recon_steps = qw(imagespread eledef eleredef edgeredef famdef);
    my $last_completed = -1;
    
    for my $i (0..$#recon_steps) {
        if (-f "$recon_steps[$i].ok") {
            $last_completed = $i;
        }
    }
    
    # Initialize variables for checkpoint logging and consensus building
    my $genome_file = "unmasked_regions.fa";
    my $msp_output = "msp.out";
    my $k_param = "unknown";
    
    if ($last_completed >= 0) {
        my $completed_steps = join(", ", @recon_steps[0..$last_completed]);
        log_message("INFO", "Found completed RECON steps", "completed: $completed_steps");
    } else {
        log_message("INFO", "No completed RECON steps found", "starting from beginning");
    }
    
    if ($last_completed >= 4) {  # famdef completed
        log_message("INFO", "RECON pipeline already completed", "proceeding to consensus building");
        
        # But check if summary/eles exists - it's crucial for build_for_RECON
        if (!-f "summary/eles") {
            log_message("WARN", "summary/eles missing despite famdef.ok", "RECON output may be incomplete");
            
            # Try to check what files actually exist
            if (-d "summary") {
                my $summary_files = `ls -la summary/ 2>/dev/null`;
                log_message("DEBUG", "Files in summary/", $summary_files);
            } else {
                log_message("ERROR", "summary/ directory missing", "RECON output is incomplete");
            }
        }
        
        goto BUILD_CONSENSI;
    }
    
    # Extract unmasked regions
    if (!-f $genome_file) {
        log_message("INFO", "Extracting unmasked regions", "input=$masked_genome, min_gap=30bp");
        extract_unmasked_regions($masked_genome, $genome_file, 30);
        
        if (!-s $genome_file) {
            log_message("WARN", "No unmasked regions found", "extraction failed or produced empty file");
            chdir $current_dir;
            return;
        }
        
        my $size = -s $genome_file;
        log_message("INFO", "Unmasked regions extracted", "file=$genome_file, size=${size} bytes");
    } else {
        my $size = -s $genome_file;
        log_message("INFO", "Unmasked regions file exists", "file=$genome_file, size=${size} bytes, skipping extraction");
    }
    
    # Check if MSP processing is already done
    if (!-s $msp_output) {
        log_message("INFO", "Starting BLAST self-alignment", "target=$genome_file, threads=$threads");
        
        # Run BLAST self-alignment
        my $blast_output = "self_alignment.blast";
        if (!-f "rmblastn_completed.ok") {
            log_message("INFO", "Creating BLAST database", "input=$genome_file, db=genome_db");
            create_blast_database($genome_file, "genome_db");
            
            log_message("INFO", "Running rmblastn self-alignment", "threads=$threads, output=$blast_output");
            run_rmblastn_self_alignment($genome_file, $blast_output, $threads, "genome_db");
        } else {
            log_message("INFO", "BLAST already completed", "found rmblastn_completed.ok checkpoint");
        }
        
        # Check if the output is already in MSP format (from parallel processing)
        if (-s $blast_output) {
            my $blast_size = -s $blast_output;
            log_message("INFO", "BLAST output generated", "file=$blast_output, size=${blast_size} bytes");
            
            # Check first line to determine format
            open(my $check_fh, '<', $blast_output) or die "Cannot read $blast_output: $!\n";
            my $first_line = <$check_fh>;
            close($check_fh);
            
            if ($first_line && $first_line =~ /^\d{6}\s+\d+\s+\d+\s+\d+\s+/) {
                # Already in MSP format (from parallel processing)
                log_message("INFO", "BLAST output in MSP format", "copying directly to msp.out");
                run_cmd("cp $blast_output $msp_output");
            } else {
                # Traditional BLAST format, need to convert
                log_message("INFO", "Converting BLAST to MSP format", "using MSPCollect.pl");
                run_cmd("MSPCollect.pl $blast_output > $msp_output");
            }
        }
        
        if (!-s $msp_output) {
            log_message("WARN", "No repeats found", "MSP file is empty, cannot proceed");
            chdir $current_dir;
            return;
        }
        
        my $msp_size = -s $msp_output;
        log_message("INFO", "MSP file generated", "file=$msp_output, size=${msp_size} bytes");
    } else {
        my $msp_size = -s $msp_output;
        log_message("INFO", "MSP file exists", "file=$msp_output, size=${msp_size} bytes, skipping BLAST processing");
    }
    
    # Run RECON (only if not already completed)
    if ($last_completed < 4) {  # famdef not yet completed
        log_message("INFO", "Starting RECON pipeline", "continuing from step " . ($last_completed + 1));
        
        create_seq_name_list($genome_file);
        $k_param = determine_k_parameter($msp_output);
        
        log_message("INFO", "RECON parameters", "K=$k_param, MSP_size=" . (-s $msp_output) . " bytes");
        
        run_recon_pipeline($k_param);
        
        log_message("INFO", "RECON pipeline completed", "all steps finished successfully");
    } else {
        log_message("INFO", "RECON pipeline already completed", "skipping to consensus building");
        
        # Try to get k_param from existing MSP file for logging
        if (-s $msp_output) {
            $k_param = determine_k_parameter($msp_output);
        }
    }
    
    BUILD_CONSENSI:
    log_message("INFO", "Starting consensus building", "extracting family sequences");
    
    # Ensure we have the genome file for consensus building
    if (!-f $genome_file) {
        log_message("INFO", "Extracting unmasked regions for consensus", "input=$masked_genome");
        extract_unmasked_regions($masked_genome, $genome_file, 30);
    }
    
    # Build consensi
    if (-d "summary" && -s "summary/families") {
        my $families_size = -s "summary/families";
        log_message("INFO", "Building consensi from families", "families_file_size=${families_size} bytes");
        
        # Debug: Check if summary/eles exists (required by build_for_RECON)
        if (!-f "summary/eles") {
            log_message("ERROR", "summary/eles missing", "build_for_RECON requires summary/eles file!");
            
            # Try to create a dummy eles file from families if possible
            if (-f "summary/families") {
                log_message("WARN", "Attempting workaround", "Will skip build_for_RECON due to missing eles file");
                # Don't run build_for_RECON without eles file
                log_message("ERROR", "Cannot build consensi", "summary/eles is required but missing");
                goto SKIP_CONSENSUS;
            }
        }
        
        # Check if tmp.fa exists and is valid
        if (!-f "tmp.fa") {
            log_message("ERROR", "tmp.fa missing", "build_for_RECON requires tmp.fa!");
            
            # Try to recreate it
            if (-f $genome_file) {
                log_message("INFO", "Recreating tmp.fa", "from $genome_file");
                my $abs_genome = abs_path($genome_file);
                unlink "tmp.fa" if -l "tmp.fa";
                run_cmd("ln -sf $abs_genome tmp.fa");
            }
        }
        
        # Use the local unmasked_regions.fa file instead of original genome
        my $local_genome = abs_path($genome_file);
        log_message("INFO", "Running build_for_RECON", "genome=$local_genome, threads=$cpu_threads");
        
        # Debug: List current directory contents
        log_message("DEBUG", "Current directory contents before build_for_RECON", "");
        system("ls -la | head -20");
        
        run_cmd("build_for_RECON ./ $local_genome $cpu_threads");
        
        # Check consensus output
        if (-s "consensi.fa") {
            my $consensi_size = -s "consensi.fa";
            my $consensi_count = `grep -c '^>' consensi.fa 2>/dev/null` || 0;
            chomp $consensi_count;
            log_message("INFO", "Consensi built successfully", "file=consensi.fa, sequences=$consensi_count, size=${consensi_size} bytes");
        } else {
            log_message("WARN", "No consensus sequences generated", "consensi.fa is empty or missing");
        }
    } else {
        log_message("WARN", "No families found", "summary/families is missing or empty, cannot build consensi");
    }
    
    SKIP_CONSENSUS:
    
    # Create masked track completion checkpoint
    my $masked_completion_checkpoint = "masked_track_completed.ok";
    open(my $masked_fh, '>', $masked_completion_checkpoint) or die "Cannot create $masked_completion_checkpoint: $!\n";
    print $masked_fh "Masked track processing completed at " . localtime() . "\n";
    print $masked_fh "MSP output: $msp_output\n";
    print $masked_fh "K parameter: $k_param\n";
    if (-s "consensi.fa") {
        my $consensi_count = `grep -c '^>' consensi.fa`;
        chomp $consensi_count;
        print $masked_fh "Consensi sequences: $consensi_count\n";
    }
    close($masked_fh);
    
    log_message("INFO", "Masked track completed", "checkpoint created: $masked_completion_checkpoint");
    log_resource_usage("masked_track_end");
    
    chdir $current_dir;
}

sub run_masked_track {
    my ($masked_genome, $output_dir, $threads, $cpu_threads) = @_;
    
    my $current_dir = getcwd();
    chdir $output_dir or die "Cannot change to $output_dir: $!\n";
    
    # Check if masked track already completed
    my $checkpoint_file = "masked_track.ok";
    my $msp_file = "masked_track.msp";
    
    if (-f $checkpoint_file && -f $msp_file) {
        print "Masked track already completed (checkpoint found), skipping...\n";
        my $abs_msp = abs_path($msp_file);
        chdir $current_dir;
        return $abs_msp;
    }
    
    # Extract unmasked regions from the masked genome with 30bp gap filtering
    my $genome_file = "unmasked_regions.fa";
    if (!-f $genome_file) {
        print "Processing masked genome track (extracting unmasked regions)...\n";
        extract_unmasked_regions($masked_genome, $genome_file, 30);
        
        if (!-s $genome_file) {
            print "Warning: Unmasked regions extraction failed or produced empty file\n";
            chdir $current_dir;
            return undef;
        }
    } else {
        print "Unmasked regions file already exists, skipping extraction...\n";
    }
    
    # Create BLAST database from the unmasked regions
    if (!-f "genome_db.nhr") {
        print "Creating BLAST database from unmasked regions...\n";
        run_cmd("makeblastdb -in $genome_file -dbtype nucl -parse_seqids -out genome_db");
    } else {
        print "BLAST database already exists, skipping creation...\n";
    }
    
    # Run self-alignment with rmblastn on the unmasked regions
    my $blast_output = "unmasked_self_alignment.blast";
    my $blast_checkpoint = "rmblastn.ok";
    
    if (!-f $blast_checkpoint || !-f $blast_output) {
        print "Running rmblastn self-alignment on unmasked regions...\n";
        run_rmblastn_self_alignment($genome_file, $blast_output, $threads, "genome_db");
        # Create checkpoint after successful BLAST
        open(my $fh, '>', $blast_checkpoint) or die "Cannot create $blast_checkpoint: $!\n";
        print $fh "rmblastn completed at " . localtime() . "\n";
        close($fh);
    } else {
        print "rmblastn already completed (checkpoint found), skipping...\n";
    }
    
    # Convert BLAST output to MSP format with self-hit filtering
    if (!-f $msp_file) {
        print "Converting BLAST output to MSP format with self-hit filtering...\n";
        my $msp_temp = "masked_self_alignment_temp.msp";
        
        # Check if the BLAST output is already in MSP format (from parallel processing)
        if (-s $blast_output) {
            open(my $check_fh, '<', $blast_output) or die "Cannot read $blast_output: $!\n";
            my $first_line = <$check_fh>;
            close($check_fh);
            
            if ($first_line && $first_line =~ /^\d{6}\s+\d+\s+\d+\s+\d+\s+/) {
                # Already in MSP format (from parallel processing)
                print "BLAST output is already in MSP format, copying directly\n";
                run_cmd("cp $blast_output $msp_file");
            } else {
                # Traditional BLAST format, need to convert
                print "BLAST output is in traditional format, converting to MSP\n";
                # First convert to MSP
                run_cmd("MSPCollect.pl $blast_output > $msp_temp");
                
                # Filter out self-hits
                if (-f $msp_temp && -s $msp_temp) {
                    print "MSP temp file size: " . (-s $msp_temp) . " bytes\n";
                    filter_self_hits($msp_temp, $msp_file);
                    
                    # Check if filtering removed all content
                    if (-s $msp_file) {
                        print "Filtered MSP file size: " . (-s $msp_file) . " bytes\n";
                        unlink $msp_temp;
                    } else {
                        print "Warning: Self-hit filtering removed all content, using unfiltered MSP\n";
                        run_cmd("mv $msp_temp $msp_file");
                    }
                } else {
                    # Fallback - just copy if MSPCollect failed or produced empty file
                    print "Warning: MSPCollect.pl failed or produced empty output, checking if input is already MSP\n";
                    run_cmd("cp $blast_output $msp_file");
                }
            }
        }
    }
    
    # Check if we have valid MSP data to proceed with RECON
    if (!-s $msp_file) {
        print "Warning: No valid MSP data generated. Skipping RECON for masked track.\n";
        chdir $current_dir;
        return undef;
    }
    
    # Run RECON on masked track MSP
    print "Running RECON on masked track MSP...\n";
    
    # Create symlink for RECON
    run_cmd("ln -sf $msp_file msp.out");
    
    # Determine K parameter
    my $k_param = determine_k_parameter("msp.out");
    
    # Create seq_name.list
    create_seq_name_list($genome_file);
    
    # Run RECON pipeline
    eval {
        run_recon_pipeline($k_param);
    };
    if ($@) {
        print "ERROR: RECON failed on masked track: $@\n";
        chdir $current_dir;
        die "Masked track RECON failure. Cannot proceed.\n";
    } else {
        print "Masked track RECON completed successfully.\n";
        
        # Build consensi if families found
        if (-d "summary" && -s "summary/families") {
            print "Building consensi for masked track...\n";
            # Use the local unmasked_regions.fa file instead of masked genome
            my $local_genome = abs_path($genome_file);
            run_cmd("build_for_RECON ./ $local_genome $cpu_threads");
        }
    }
    
    # Create masked track completion checkpoint
    open(my $fh, '>', $checkpoint_file) or die "Cannot create $checkpoint_file: $!\n";
    print $fh "Masked track completed at " . localtime() . "\n";
    print $fh "MSP file: $msp_file\n";
    print $fh "K parameter: $k_param\n" if defined $k_param;
    close($fh);
    
    my $abs_msp = abs_path($msp_file);
    chdir $current_dir;
    
    return $abs_msp;
}

sub run_sampling_track_progressive {
    my ($genome_file, $bed_files_ref, $masked_msp, $output_dir, $threads, $cpu_threads) = @_;
    my @bed_files = @$bed_files_ref;
    
    my $current_dir = getcwd();
    chdir $output_dir or die "Cannot change to $output_dir: $!\n";
    
    # Initialize progressive masking list with masked_track results
    my @progressive_msp_files;
    push @progressive_msp_files, $masked_msp if $masked_msp && -s $masked_msp;
    
    # Convert MSP to consensi for masking if available
    my $initial_consensi;
    if ($masked_msp && -s $masked_msp) {
        print "Converting masked_track MSP to consensi for progressive masking...\n";
        $initial_consensi = "masked_track_consensi.fa";
        # Extract unique sequences from MSP and build consensi
        convert_msp_to_consensi($masked_msp, $initial_consensi, $genome_file);
    }
    
    # Merge all bed files to create initial exclusion mask
    my $exclusion_mask = "initial_exclusion_mask.bed";
    if (@bed_files) {
        print "Creating initial exclusion mask from " . scalar(@bed_files) . " BED files...\n";
        for my $bed_file (@bed_files) {
            my $count = `wc -l < '$bed_file'`;
            chomp $count;
            print "  - $bed_file: $count regions\n";
        }
        my $bed_list = join(" ", @bed_files);
        run_cmd("cat $bed_list | bedtools sort | bedtools merge > $exclusion_mask");
        
        # Show merged result
        my $merged_count = `wc -l < '$exclusion_mask'`;
        chomp $merged_count;
        print "  - Merged exclusion mask: $merged_count regions\n";
    } else {
        open(my $fh, '>', $exclusion_mask) or die "Cannot create $exclusion_mask: $!\n";
        close($fh);
        print "No BED files found, created empty exclusion mask\n";
    }
    
    # Multi-round sampling with progressive masking
    my @round_msps;
    my @round_consensi;
    my $round = 1;
    my $sample_size_mb = INITIAL_SAMPLE;  # Start with 30Mb
    my $previous_families = 0;
    my $continue_sampling = 1;
    my $accumulated_mask = $exclusion_mask;
    
    # Create cumulative consensi file for progressive masking
    my $cumulative_consensi = "cumulative_consensi.fa";
    if ($initial_consensi && -s $initial_consensi) {
        run_cmd("cp $initial_consensi $cumulative_consensi");
    }
    
    while ($continue_sampling && $round <= 5) {  # Max 5 rounds for safety
        print "\n=== Progressive Sampling Round $round (${sample_size_mb}Mb) ===\n";
        
        my $round_dir = "round_$round";
        make_path($round_dir) unless -d $round_dir;
        
        # Check available genome space
        my $available_size = calculate_available_size($genome_file, $accumulated_mask);
        my $required_size = $sample_size_mb * 1000000;
        
        if ($available_size < $required_size * 2) {
            print "Available genome space (${available_size} bp) less than 2X required (${required_size} bp)\n";
            print "Insufficient genome space for next round. Stopping sampling.\n";
            last;
        }
        
        # Sample genome (prefer unsampled regions)
        my $sample_file = "$round_dir/sample.fa";
        perform_adaptive_sampling($genome_file, $exclusion_mask, $accumulated_mask, $sample_file, $sample_size_mb, FRAGMENT_SIZE);
        
        # Progressive masking: TRF + cumulative consensi from all previous rounds
        my $masked_sample = "$round_dir/sample_trf_masked.fa";
        run_trf_masking($sample_file, $masked_sample);
        
        # Apply cumulative masking with all previous consensi
        if (-s $cumulative_consensi) {
            print "Applying progressive masking with cumulative consensi...\n";
            my $progressive_masked = "$round_dir/sample_progressive_masked.fa";
            run_repeatmasker($masked_sample, $cumulative_consensi, $progressive_masked, $cpu_threads);
            $masked_sample = $progressive_masked if -s $progressive_masked;
        }
        
        # Run BLAST self-alignment
        print "Running BLAST self-alignment for round $round...\n";
        my $blast_output = "$round_dir/self_alignment.blast";
        create_blast_database($masked_sample, "$round_dir/sample_db");
        run_rmblastn_self_alignment($masked_sample, $blast_output, $threads, "$round_dir/sample_db");
        
        # Convert to MSP
        my $round_msp = "$round_dir/round.msp";
        run_cmd("MSPCollect.pl $blast_output > $round_msp");
        
        # Check if we found anything new
        if (!-s $round_msp) {
            print "No new repeats found in round $round. Stopping sampling.\n";
            last;
        }
        
        # Determine K parameter
        my $k_param = determine_k_parameter($round_msp);
        
        # Run RECON
        print "Running RECON with K=$k_param for round $round...\n";
        chdir $round_dir;
        create_seq_name_list($masked_sample);
        run_cmd("cp round.msp msp.out");
        run_recon_pipeline($k_param);
        
        # Build consensi for this round
        my $round_consensi_file = "consensi.fa";
        if (-d "summary" && -s "summary/families") {
            # Use the local masked sample file for this round
            my $local_genome = abs_path($masked_sample);
            run_cmd("build_for_RECON ./ $local_genome $cpu_threads");
        }
        
        chdir "..";
        
        # Add this round's consensi to cumulative file
        if (-s "$round_dir/$round_consensi_file") {
            print "Adding round $round consensi to cumulative library...\n";
            run_cmd("cat $round_dir/$round_consensi_file >> $cumulative_consensi");
            push @round_consensi, "$round_dir/$round_consensi_file";
        }
        
        # Create round-specific checkpoint
        my $round_checkpoint = "$round_dir/round_completed.ok";
        open(my $round_fh, '>', $round_checkpoint) or die "Cannot create round checkpoint $round_checkpoint: $!\n";
        print $round_fh "Sampling round $round completed at " . localtime() . "\n";
        print $round_fh "Sample size: ${sample_size_mb}Mb\n";
        print $round_fh "Round directory: $round_dir\n";
        print $round_fh "Consensi file: $round_consensi_file\n";
        if (-s "$round_dir/$round_consensi_file") {
            my $consensi_count = `grep -c '^>' $round_dir/$round_consensi_file`;
            chomp $consensi_count;
            print $round_fh "Consensi sequences: $consensi_count\n";
        }
        close($round_fh);
        print "Round $round checkpoint created: $round_checkpoint\n";
        
        # Analyze results to decide continuation
        my ($family_count, $should_continue) = analyze_round_results(
            "$round_dir/summary/families", 
            $previous_families
        );
        
        if (!$should_continue) {
            print "Convergence detected (minimal new families). Stopping sampling.\n";
            $continue_sampling = 0;
        } else {
            $previous_families = $family_count;
            push @round_msps, abs_path("$round_dir/round.msp") if -s "$round_dir/round.msp";
            
            # Update accumulated mask with sampled regions
            my $round_bed = "$round_dir/sampled_regions.bed";
            if (-f $round_bed) {
                my $new_mask = "accumulated_mask_round_$round.bed";
                run_cmd("cat $accumulated_mask $round_bed | bedtools sort | bedtools merge > $new_mask");
                $accumulated_mask = $new_mask;
            }
            
            # Increase sample size for next round
            $sample_size_mb *= SAMPLE_MULTIPLIER;
            $round++;
        }
    }
    
    # Merge all round MSPs
    my $final_msp;
    if (@round_msps) {
        $final_msp = "sampling_track_final.msp";
        print "Merging " . scalar(@round_msps) . " round MSP files...\n";
        merge_msp_files(\@round_msps, $final_msp);
    }
    
    chdir $current_dir;
    return $final_msp ? abs_path("$output_dir/$final_msp") : undef;
}

sub run_sampling_track {
    my ($genome_file, $bed_files_ref, $output_dir, $threads, $cpu_threads) = @_;
    my @bed_files = @$bed_files_ref;
    
    my $current_dir = getcwd();
    chdir $output_dir or die "Cannot change to $output_dir: $!\n";
    
    # Merge all bed files to create exclusion mask
    my $exclusion_mask = "exclusion_mask.bed";
    if (@bed_files) {
        print "Creating exclusion mask from " . scalar(@bed_files) . " BED files...\n";
        my $bed_list = join(" ", @bed_files);
        run_cmd("cat $bed_list | bedtools sort | bedtools merge > $exclusion_mask");
    } else {
        # Create empty bed file if none provided
        open(my $fh, '>', $exclusion_mask) or die "Cannot create $exclusion_mask: $!\n";
        close($fh);
    }
    
    # Multi-round sampling
    my @round_msps;
    my $round = 1;
    my $sample_size_mb = INITIAL_SAMPLE;  # Start with 30Mb
    my $previous_families = 0;
    my $continue_sampling = 1;
    my $accumulated_mask = $exclusion_mask;
    
    while ($continue_sampling && $round <= 5) {  # Max 5 rounds for safety
        print "\n=== Sampling Round $round (${sample_size_mb}Mb) ===\n";
        
        my $round_dir = "round_$round";
        make_path($round_dir) unless -d $round_dir;
        
        # Check available genome space
        my $available_size = calculate_available_size($genome_file, $accumulated_mask);
        my $required_size = $sample_size_mb * 1000000;
        
        if ($available_size < $required_size * 2) {
            print "Available genome space (${available_size} bp) less than 2X required (${required_size} bp)\n";
            print "Insufficient genome space for next round. Stopping sampling.\n";
            last;
        }
        
        # Sample genome (prefer unsampled regions, allow resampling if needed)
        my $sample_file = "$round_dir/sample.fa";
        perform_adaptive_sampling($genome_file, $exclusion_mask, $accumulated_mask, $sample_file, $sample_size_mb, FRAGMENT_SIZE);
        
        # TRF soft-mask the sample
        my $trf_masked = "$round_dir/sample_trf_masked.fa";
        run_trf_masking($sample_file, $trf_masked);
        
        # If round > 1, also mask with previous round's consensi
        if ($round > 1 && -s "round_" . ($round-1) . "/consensi.fa") {
            my $prev_consensi = "round_" . ($round-1) . "/consensi.fa";
            my $double_masked = "$round_dir/sample_double_masked.fa";
            run_repeatmasker($trf_masked, $prev_consensi, $double_masked, $cpu_threads);
            $trf_masked = $double_masked;
        }
        
        # Run BLAST self-alignment
        print "Running BLAST self-alignment for round $round...\n";
        my $blast_output = "$round_dir/self_alignment.blast";
        create_blast_database($trf_masked, "$round_dir/sample_db");
        run_rmblastn_self_alignment($trf_masked, $blast_output, $threads, "$round_dir/sample_db");
        
        # Convert to MSP
        my $round_msp = "$round_dir/round.msp";
        run_cmd("MSPCollect.pl $blast_output > $round_msp");
        
        # Determine K parameter
        my $k_param = determine_k_parameter($round_msp);
        
        # Run RECON
        print "Running RECON with K=$k_param for round $round...\n";
        chdir $round_dir;
        create_seq_name_list($sample_file);
        # Copy MSP file to standard name for RECON pipeline
        run_cmd("cp round.msp msp.out");
        run_recon_pipeline($k_param);
        
        # Build consensi for this round
        if (-d "summary" && -s "summary/families") {
            # Use the local sample file for this round
            my $local_genome = abs_path($sample_file);
            run_cmd("build_for_RECON ./ $local_genome $cpu_threads");
        }
        
        chdir "..";
        
        # Analyze results to decide continuation
        my ($should_continue, $metrics) = analyze_round_results($round, $round_dir, $previous_families);
        
        print "Round $round metrics:\n";
        print "  New families: $metrics->{new_families}\n";
        print "  New families per 100Mb: $metrics->{new_per_100mb}\n";
        print "  Total families: $metrics->{total_families}\n";
        
        if (!$should_continue) {
            print "Stopping criteria met. Ending sampling.\n";
            $continue_sampling = 0;
        } else {
            # Update for next round
            $sample_size_mb *= SAMPLE_MULTIPLIER;
            $previous_families = $metrics->{total_families};
            
            # Update accumulated mask with current round's sample regions
            my $round_sample_bed = "$round_dir/sampled_regions.bed";
            if (-s $round_sample_bed) {
                my $new_mask = "accumulated_mask_round_$round.bed";
                run_cmd("cat $accumulated_mask $round_sample_bed | bedtools sort | bedtools merge > $new_mask");
                $accumulated_mask = $new_mask;
            }
        }
        
        push @round_msps, abs_path($round_msp);
        
        # Create round completion checkpoint
        my $completion_checkpoint = "$round_dir/round_complete.ok";
        open(my $fh, '>', $completion_checkpoint) or die "Cannot create $completion_checkpoint: $!\n";
        print $fh "Round $round completed at " . localtime() . "\n";
        print $fh "New families: $metrics->{new_families}\n";
        print $fh "Total families: $metrics->{total_families}\n";
        close($fh);
        
        $round++;
    }
    
    # Merge all round MSPs
    my $final_msp = "sampling_track_merged.msp";
    if (@round_msps) {
        print "Merging MSP files from all sampling rounds...\n";
        merge_msp_files(\@round_msps, $final_msp);
    }
    
    my $abs_msp = abs_path($final_msp);
    chdir $current_dir;
    
    return $abs_msp;
}

sub determine_k_parameter {
    my ($msp_file) = @_;
    
    return 1 unless -s $msp_file;
    
    my $file_size = -s $msp_file;
    my $size_gb = $file_size / (1024 * 1024 * 1024);
    
    my $k;
    if ($size_gb < 1) {
        $k = 1;
    } elsif ($size_gb < 5) {
        $k = 4;
    } elsif ($size_gb < 20) {
        $k = 16;
    } else {
        $k = 64;
    }
    
    print "MSP file size: " . sprintf("%.2f GB", $size_gb) . ", using K=$k\n";
    return $k;
}

sub run_recon_pipeline_with_k {
    my ($msp_file, $genome_file, $threads, $k_param) = @_;
    
    # Create sequence name list from genome
    create_seq_name_list($genome_file);
    
    # Run RECON pipeline manually with specified K parameter
    run_recon_pipeline($k_param);
}

sub run_rmblastn_self_alignment {
    my ($query_file, $output_file, $threads, $db_name) = @_;
    
    $db_name ||= "genome_db";
    
    # Check file size to decide if parallel processing is needed
    my $file_size = -s $query_file;
    my $size_mb = $file_size / (1024 * 1024);
    
    if ($size_mb <= 1 || $threads < 4) {
        # Small file or few threads - run directly
        print "Running single rmblastn (file size: " . sprintf("%.1f", $size_mb) . " MB)...\n";
        run_single_rmblastn($query_file, $output_file, $threads, $db_name);
    } else {
        # Large file - use parallel processing
        # Use threads/2 for parallel jobs
        my $num_parallel = int($threads / 2);
        $num_parallel = 2 if $num_parallel < 2;
        
        print "Running parallel rmblastn (file size: " . sprintf("%.1f", $size_mb) . " MB, $num_parallel parallel jobs)...\n";
        run_parallel_rmblastn_with_msp($query_file, $output_file, $threads, $num_parallel, $db_name);
    }
    
    # Create checkpoint file after successful rmblastn completion
    my $checkpoint_file = "rmblastn_completed.ok";
    if (-s $output_file) {
        open(my $checkpoint_fh, '>', $checkpoint_file) or die "Cannot create checkpoint $checkpoint_file: $!\n";
        print $checkpoint_fh "rmblastn completed at " . localtime() . "\n";
        print $checkpoint_fh "Query file: $query_file\n";
        print $checkpoint_fh "Output file: $output_file\n";
        print $checkpoint_fh "Database: $db_name\n";
        print $checkpoint_fh "Threads: $threads\n";
        print $checkpoint_fh "File size: " . sprintf("%.1f", $size_mb) . " MB\n";
        close($checkpoint_fh);
        print "rmblastn checkpoint created: $checkpoint_file\n";
    }
}

sub run_single_rmblastn {
    my ($query_file, $output_file, $threads, $db_name) = @_;
    
    # Run rmblastn with outfmt 0 for RECON MSPCollect.pl compatibility
    my $cmd = "rmblastn " .
              "-query $query_file " .
              "-db $db_name " .
              "-out $output_file " .
              "-outfmt 0 " .  # Changed from 6 to 0 for MSPCollect.pl
              "-evalue 1e-5 " .
              "-num_threads $threads " .
              "-word_size 11 " .
              "-gapopen 5 " .
              "-gapextend 2 " .
              "-penalty -3 " .
              "-reward 2 " .
              "-dust yes " .
              "-soft_masking true " .
              "-no_greedy";
    
    run_cmd($cmd);
}

sub run_parallel_rmblastn_with_msp {
    my ($query_file, $output_file, $total_threads, $num_parallel, $db_name) = @_;
    
    # Create temp directory for chunks
    my $chunk_dir = "blast_chunks_$$";
    make_path($chunk_dir) unless -d $chunk_dir;
    
    # Split query file into 1MB chunks
    my @chunk_files = split_fasta_by_size($query_file, $chunk_dir, 1024 * 1024);  # 1MB chunks
    
    if (@chunk_files == 0) {
        die "Failed to split query file for parallel processing\n";
    }
    
    print "Split query into " . scalar(@chunk_files) . " chunks (1MB each)\n";
    
    # Calculate threads per job
    my $threads_per_job = int($total_threads / $num_parallel);
    $threads_per_job = 2 if $threads_per_job < 2;
    
    print "Running up to $num_parallel parallel jobs, $threads_per_job threads each\n";
    
    # Use Parallel::ForkManager for better job management
    use Parallel::ForkManager;
    my $pm = Parallel::ForkManager->new($num_parallel);
    
    my @msp_files;
    my $completed = 0;
    
    # Setup completion handler
    $pm->run_on_finish(sub {
        my ($pid, $exit_code, $ident, $exit_signal, $core_dump, $data_ref) = @_;
        if ($exit_code == 0 && $data_ref) {
            my $msp_file = $$data_ref;
            push @msp_files, $msp_file if -f $msp_file;
            $completed++;
            print "Completed $completed/" . scalar(@chunk_files) . " chunks\n";
        }
    });
    
    # Process all chunks - ForkManager will manage the parallelism
    for (my $i = 0; $i < @chunk_files; $i++) {
        my $chunk_file = $chunk_files[$i];
        my $chunk_num = $i;
        
        # This will block if we've reached max processes
        my $pid = $pm->start;
        if ($pid) {
            # Parent - continue to next iteration
            next;
        }
        
        # Child process
        local $| = 1;  # Autoflush
        print "Starting chunk " . ($chunk_num + 1) . " of " . scalar(@chunk_files) . "\n";
        
        # Run BLAST
        my $blast_output = "$chunk_dir/blast_chunk_$chunk_num.out";
        run_single_rmblastn($chunk_file, $blast_output, $threads_per_job, $db_name);
        
        # Convert to MSP immediately with self-hit filtering
        my $msp_output = "$chunk_dir/msp_chunk_$chunk_num.msp";
        my $msp_temp = "$chunk_dir/msp_chunk_$chunk_num.tmp";
        
        # First convert to MSP
        system("MSPCollect.pl $blast_output > $msp_temp 2>/dev/null");
        
        # Filter out self-hits (where query and subject are identical)
        if (-f $msp_temp) {
            filter_self_hits($msp_temp, $msp_output);
            unlink $msp_temp;
        }
        
        # Delete BLAST output to save space
        unlink $blast_output;
        
        # Return MSP file path to parent
        $pm->finish(0, \$msp_output);
    }
    
    # Wait for all children to complete
    $pm->wait_all_children;
    
    print "All chunks processed, merging " . scalar(@msp_files) . " MSP files...\n";
    
    # Sort MSP files by chunk number for consistent ordering
    @msp_files = sort {
        my ($a_num) = $a =~ /chunk_(\d+)\.msp/;
        my ($b_num) = $b =~ /chunk_(\d+)\.msp/;
        $a_num <=> $b_num;
    } @msp_files;
    
    # Merge all MSP files
    merge_msp_files(\@msp_files, $output_file);
    
    # Create checkpoint after successful merge
    my $checkpoint_file = "rmblastn_msp_merge.ok";
    open(my $checkpoint_fh, '>', $checkpoint_file) or die "Cannot create checkpoint $checkpoint_file: $!\n";
    print $checkpoint_fh "rmblastn MSP merge completed at " . localtime() . "\n";
    print $checkpoint_fh "Merged " . scalar(@msp_files) . " MSP files\n";
    print $checkpoint_fh "Output file: $output_file\n";
    close($checkpoint_fh);
    
    # Clean up
    remove_tree($chunk_dir);
    
    print "Parallel rmblastn with MSP conversion completed\n";
}

sub filter_self_hits {
    my ($input_msp, $output_msp) = @_;
    
    open(my $in, '<', $input_msp) or return;
    open(my $out, '>', $output_msp) or return;
    
    while (my $line = <$in>) {
        # MSP format: score identity q_start q_end query s_start s_end subject
        if ($line =~ /^(\d+)\s+(\d+)\s+(\d+)\s+(\d+)\s+(\S+)\s+(\d+)\s+(\d+)\s+(\S+)/) {
            my ($score, $iden, $q_start, $q_end, $query, $s_start, $s_end, $subject) = 
               ($1, $2, $3, $4, $5, $6, $7, $8);
            
            # Skip if this is a self-hit (same sequence, same position)
            if ($query eq $subject && $q_start == $s_start && $q_end == $s_end) {
                next;  # Skip trivial self-alignment
            }
            
            # Also skip if the alignment is >95% overlapping with itself
            if ($query eq $subject) {
                my $q_len = abs($q_end - $q_start) + 1;
                my $s_len = abs($s_end - $s_start) + 1;
                my $overlap_start = ($q_start > $s_start) ? $q_start : $s_start;
                my $overlap_end = ($q_end < $s_end) ? $q_end : $s_end;
                
                if ($overlap_end >= $overlap_start) {
                    my $overlap = $overlap_end - $overlap_start + 1;
                    if ($overlap > 0.95 * $q_len || $overlap > 0.95 * $s_len) {
                        next;  # Skip near-complete self-overlaps
                    }
                }
            }
        }
        
        print $out $line;
    }
    
    close($in);
    close($out);
}

sub split_fasta_by_size {
    my ($input_file, $output_dir, $max_size) = @_;
    
    open(my $in, '<', $input_file) or die "Cannot open $input_file: $!\n";
    
    my @chunk_files;
    my $chunk_num = 0;
    my $current_size = 0;
    my $chunk_fh;
    my $current_chunk_file;
    my $in_sequence = 0;
    my $current_seq_header = "";
    my $current_seq = "";
    
    while (my $line = <$in>) {
        if ($line =~ /^>/) {
            # Save previous sequence if exists
            if ($in_sequence && $current_seq_header) {
                my $seq_size = length($current_seq_header) + length($current_seq);
                
                # Check if we need a new chunk
                if ($chunk_fh && ($current_size + $seq_size) > $max_size) {
                    # Close current chunk and start new one
                    close($chunk_fh);
                    $chunk_fh = undef;
                    $current_size = 0;
                }
                
                # Open new chunk if needed
                if (!$chunk_fh) {
                    $current_chunk_file = "$output_dir/chunk_$chunk_num.fa";
                    push @chunk_files, $current_chunk_file;
                    open($chunk_fh, '>', $current_chunk_file) or die "Cannot create $current_chunk_file: $!\n";
                    $chunk_num++;
                }
                
                # Write sequence to chunk
                print $chunk_fh $current_seq_header;
                print $chunk_fh $current_seq;
                $current_size += $seq_size;
            }
            
            # Start new sequence
            $current_seq_header = $line;
            $current_seq = "";
            $in_sequence = 1;
        } else {
            $current_seq .= $line;
        }
    }
    
    # Write last sequence
    if ($in_sequence && $current_seq_header) {
        # Open chunk file if not already open
        if (!$chunk_fh) {
            $current_chunk_file = "$output_dir/chunk_$chunk_num.fa";
            push @chunk_files, $current_chunk_file;
            open($chunk_fh, '>', $current_chunk_file) or die "Cannot create $current_chunk_file: $!\n";
        }
        print $chunk_fh $current_seq_header;
        print $chunk_fh $current_seq;
    }
    
    close($chunk_fh) if $chunk_fh;
    close($in);
    
    return @chunk_files;
}


sub perform_adaptive_sampling {
    my ($genome_file, $exclusion_bed, $accumulated_mask, $output_file, $size_mb, $fragment_size) = @_;
    
    print "Adaptive sampling ${size_mb}Mb from genome (${fragment_size}bp fragments)...\n";
    print "DEBUG: Exclusion BED file: $exclusion_bed\n";
    print "DEBUG: Accumulated mask file: $accumulated_mask\n";
    
    # Get genome index
    unless (-s "$genome_file.fai") {
        run_cmd("samtools faidx $genome_file");
    }
    
    # Create complement of exclusion regions (only original BED exclusions)
    my $genome_bed = "genome.bed";
    run_cmd("awk '{print \$1\"\\t0\\t\"\$2}' $genome_file.fai > $genome_bed");
    
    # Create master exclusion file combining all exclusions
    my $master_exclusion = "master_exclusion.bed";
    my @exclude_files = ();
    push @exclude_files, $exclusion_bed if -s $exclusion_bed;
    push @exclude_files, $accumulated_mask if -s $accumulated_mask && $accumulated_mask ne $exclusion_bed;
    
    if (@exclude_files) {
        print "DEBUG: Creating master exclusion from files: " . join(", ", @exclude_files) . "\n";
        my $exclude_list = join(" ", @exclude_files);
        run_cmd("cat $exclude_list | bedtools sort | bedtools merge > $master_exclusion");
        
        # Debug: show exclusion file size
        my $exclude_count = `wc -l < $master_exclusion`;
        chomp $exclude_count;
        print "DEBUG: Master exclusion file has $exclude_count regions\n";
    } else {
        # Create empty exclusion file
        run_cmd("touch $master_exclusion");
        print "DEBUG: No exclusion files, created empty master exclusion\n";
    }
    
    # Create available regions (genome minus all exclusions)
    my $available_bed = "available_regions.bed";
    if (-s $master_exclusion) {
        run_cmd("bedtools subtract -a $genome_bed -b $master_exclusion > $available_bed");
    } else {
        run_cmd("cp $genome_bed $available_bed");
    }
    
    # Calculate sizes
    my $available_size = calculate_bed_size($available_bed);
    my $target_size = $size_mb * 1000000;
    
    print "Available genome space: " . sprintf("%.1f Mb", $available_size / 1000000) . "\n";
    print "Target sampling size: " . sprintf("%.1f Mb", $target_size / 1000000) . "\n";
    
    # Check if we have enough available space
    if ($available_size < $target_size * 0.1) {
        print "WARNING: Very little available space for sampling!\n";
        print "Available: " . sprintf("%.1f Mb", $available_size / 1000000) . 
              ", Requested: " . sprintf("%.1f Mb", $target_size / 1000000) . "\n";
    }
    
    my $sampled_bed = "sampled_regions.bed";
    my $num_fragments = ceil($target_size / $fragment_size);
    
    # Sample from available regions with proper exclusion
    print "Generating $num_fragments random fragments of ${fragment_size}bp each\n";
    run_cmd("bedtools random -l $fragment_size -n " . ($num_fragments * 3) . " -seed 42 -g $genome_file.fai | " .
            "bedtools intersect -a - -b $available_bed -wa | " .
            "head -n $num_fragments > $sampled_bed");
    
    # Verify no overlaps with exclusions
    if (-s $master_exclusion && -s $sampled_bed) {
        my $overlap_check = "overlap_check.bed";
        run_cmd("bedtools intersect -a $sampled_bed -b $master_exclusion > $overlap_check");
        my $overlap_count = `wc -l < $overlap_check`;
        chomp $overlap_count;
        if ($overlap_count > 0) {
            print "ERROR: Found $overlap_count overlaps with exclusion regions!\n";
            print "This indicates a problem with bedtools subtract logic.\n";
            # Show first few overlaps for debugging
            run_cmd("head -5 $overlap_check");
        } else {
            print "DEBUG: Verified no overlaps with exclusion regions\n";
        }
        unlink $overlap_check;
    }
    
    # Get actual sampled regions count
    my $actual_samples = 0;
    if (-s $sampled_bed) {
        $actual_samples = `wc -l < $sampled_bed`;
        chomp $actual_samples;
    }
    
    print "Successfully sampled $actual_samples regions\n";
    
    if ($actual_samples == 0) {
        print "WARNING: No regions were sampled! Check available space and exclusions.\n";
        return;
    }
    
    # Extract sequences from sampled regions
    print "Extracting sequences from sampled regions...\n";
    
    # Extract sequences with bedtools (this will create complex IDs)
    my $temp_fasta = "$output_file.tmp";
    run_cmd("bedtools getfasta -fi $genome_file -bed $sampled_bed -fo $temp_fasta");
    
    # Rename sequence IDs to simple gi|N format
    rename_fasta_ids($temp_fasta, $output_file);
    unlink $temp_fasta;
    
    my $output_size = -s $output_file || 0;
    print "Extracted " . sprintf("%.1f Mb", $output_size / 1000000) . " of sequence data\n";
    
    # Cleanup temporary files
    unlink $genome_bed, $master_exclusion, $available_bed if $ENV{CLEANUP_TEMP};
    
    return $output_file;
}

sub calculate_bed_size {
    my ($bed_file) = @_;
    return 0 unless -s $bed_file;
    
    my $total_size = 0;
    open(my $fh, '<', $bed_file) or return 0;
    while (<$fh>) {
        chomp;
        my @fields = split /\t/;
        $total_size += ($fields[2] - $fields[1]) if @fields >= 3;
    }
    close($fh);
    
    return $total_size;
}

sub rename_fasta_ids {
    my ($input_file, $output_file) = @_;
    
    open(my $in, '<', $input_file) or die "Cannot read $input_file: $!\n";
    open(my $out, '>', $output_file) or die "Cannot write $output_file: $!\n";
    
    my $seq_counter = 0;
    
    while (my $line = <$in>) {
        chomp $line;
        if ($line =~ /^>/) {
            # Replace any existing ID with simple gi|N format
            $seq_counter++;
            print $out ">gi|$seq_counter\n";
        } else {
            # Copy sequence lines as-is
            print $out "$line\n";
        }
    }
    
    close($in);
    close($out);
    
    print "Renamed $seq_counter sequences to gi|N format\n";
}

sub perform_sampling {
    my ($genome_file, $exclusion_bed, $output_file, $size_mb, $fragment_size) = @_;
    
    print "Sampling ${size_mb}Mb from genome (${fragment_size}bp fragments)...\n";
    
    # Get genome index
    unless (-s "$genome_file.fai") {
        run_cmd("samtools faidx $genome_file");
    }
    
    # Create complement of exclusion regions
    my $genome_bed = "genome.bed";
    run_cmd("awk '{print \$1\"\\t0\\t\"\$2}' $genome_file.fai > $genome_bed");
    
    my $available_bed = "available_regions.bed";
    if (-s $exclusion_bed) {
        run_cmd("bedtools subtract -a $genome_bed -b $exclusion_bed > $available_bed");
    } else {
        run_cmd("cp $genome_bed $available_bed");
    }
    
    # Calculate total available size
    my $available_size = 0;
    open(my $fh, '<', $available_bed) or die "Cannot read $available_bed: $!\n";
    while (<$fh>) {
        chomp;
        my @fields = split /\t/;
        $available_size += ($fields[2] - $fields[1]);
    }
    close($fh);
    
    my $target_size = $size_mb * 1000000;
    if ($available_size < $target_size) {
        print "Warning: Available size ($available_size bp) less than target ($target_size bp)\n";
        $target_size = $available_size * 0.9;  # Use 90% of available
    }
    
    # Sample regions
    my $sampled_bed = "sampled_regions.bed";
    my $num_fragments = ceil($target_size / $fragment_size);
    
    # Use bedtools shuffle for random sampling
    run_cmd("bedtools random -l $fragment_size -n $num_fragments -seed 42 -g $genome_file.fai | " .
            "bedtools intersect -a - -b $available_bed -wa | " .
            "head -n $num_fragments > $sampled_bed");
    
    # Extract sequences with bedtools (this will create complex IDs)
    my $temp_fasta = "$output_file.tmp";
    run_cmd("bedtools getfasta -fi $genome_file -bed $sampled_bed -fo $temp_fasta");
    
    # Rename sequence IDs to simple gi|N format
    rename_fasta_ids($temp_fasta, $output_file);
    unlink $temp_fasta;
    
    print "Sampled " . (-s $output_file) . " bytes of sequence\n";
}

sub run_trf_masking {
    my ($input_file, $output_file) = @_;
    
    print "Running TRF soft-masking...\n";
    
    # Run TRF
    my $trf_output = "$input_file.2.7.7.80.10.50.2000.dat";
    run_cmd("trf $input_file 2 7 7 80 10 50 2000 -d -h 2>/dev/null");
    
    if (!-s $trf_output) {
        print "No tandem repeats found by TRF\n";
        run_cmd("cp $input_file $output_file");
        return;
    }
    
    # Convert TRF output to BED
    my $trf_bed = "$input_file.trf.bed";
    open(my $in_fh, '<', $trf_output) or die "Cannot read TRF output: $!\n";
    open(my $out_fh, '>', $trf_bed) or die "Cannot write BED file: $!\n";
    
    my $current_seq = "";
    while (<$in_fh>) {
        if (/^Sequence: (.+)/) {
            $current_seq = $1;
        } elsif (/^(\d+)\s+(\d+)\s+\d+/) {
            print $out_fh "$current_seq\t" . ($1-1) . "\t$2\n";
        }
    }
    
    close($in_fh);
    close($out_fh);
    
    # Soft-mask using bedtools
    if (-s $trf_bed) {
        run_cmd("bedtools maskfasta -fi $input_file -bed $trf_bed -fo $output_file -soft");
    } else {
        run_cmd("cp $input_file $output_file");
    }
    
    # Clean up
    unlink $trf_output if -f $trf_output;
    unlink $trf_bed if -f $trf_bed;
}

sub run_repeatmasker {
    my ($input_file, $lib_file, $output_file, $threads) = @_;
    
    print "Running RepeatMasker with previous round's library...\n";
    
    my $rm_dir = "repeatmasker_tmp";
    make_path($rm_dir) unless -d $rm_dir;
    
    run_cmd("RepeatMasker -lib $lib_file -dir $rm_dir -pa $threads -xsmall -nolow -norna -no_is $input_file");
    
    my $masked_file = "$rm_dir/" . basename($input_file) . ".masked";
    if (-s $masked_file) {
        run_cmd("mv $masked_file $output_file");
    } else {
        run_cmd("cp $input_file $output_file");
    }
    
    remove_tree($rm_dir);
}

sub analyze_round_results {
    my ($round_num, $round_dir, $previous_families) = @_;
    
    my $metrics = {
        new_families => 0,
        new_per_100mb => 0,
        total_families => 0,
        singletons => 0,
        p95_family_size => 0,
    };
    
    # Check if RECON produced results
    my $families_file = "$round_dir/summary/families";
    return (0, $metrics) unless -s $families_file;
    
    # Count families
    open(my $fh, '<', $families_file) or return (0, $metrics);
    my @family_sizes;
    while (<$fh>) {
        chomp;
        my @fields = split;
        $metrics->{total_families}++;
        push @family_sizes, $fields[1] if defined $fields[1];
        $metrics->{singletons}++ if $fields[1] == 1;
    }
    close($fh);
    
    # Calculate new families
    $metrics->{new_families} = $metrics->{total_families} - $previous_families;
    
    # Calculate per 100Mb rate
    my $sample_mb = INITIAL_SAMPLE * (SAMPLE_MULTIPLIER ** ($round_num - 1));
    $metrics->{new_per_100mb} = ($metrics->{new_families} * 100) / $sample_mb;
    
    # Calculate P95 family size
    if (@family_sizes) {
        @family_sizes = sort { $a <=> $b } @family_sizes;
        my $p95_index = int(0.95 * scalar(@family_sizes));
        $metrics->{p95_family_size} = $family_sizes[$p95_index];
    }
    
    # Decision logic based on iteration_guide.md
    my $should_continue = 0;
    
    # Primary criteria
    if ($metrics->{new_per_100mb} >= 20) {
        $should_continue = 1;
        print "Continue criterion met: new_families_per_100Mb >= 20 ($metrics->{new_per_100mb})\n";
    } elsif ($metrics->{new_per_100mb} < 10) {
        $should_continue = 0;
        print "Stop criterion met: new_families_per_100Mb < 10 ($metrics->{new_per_100mb})\n";
    } else {
        # In the ambiguous zone (10-20), use secondary criteria
        print "In ambiguous zone: new_families_per_100Mb = $metrics->{new_per_100mb} (10-20)\n";
        
        # Check singleton ratio
        my $singleton_ratio = 0;
        if ($metrics->{total_families} > 0) {
            $singleton_ratio = $metrics->{singletons} / $metrics->{total_families};
        }
        
        if ($singleton_ratio >= 0.6) {
            $should_continue = 0;
            print "Stop due to high singleton ratio: " . sprintf("%.1f%%", $singleton_ratio * 100) . "\n";
        } elsif ($metrics->{new_per_100mb} >= 15) {
            # More lenient threshold for ambiguous zone
            $should_continue = 1;
            print "Continue due to moderate discovery rate in ambiguous zone\n";
        } else {
            # Conservative stop for low discovery rates
            $should_continue = 0;
            print "Stop due to low discovery rate in ambiguous zone\n";
        }
    }
    
    # Override stop if singleton ratio is very high
    if ($metrics->{total_families} > 0) {
        my $singleton_ratio = $metrics->{singletons} / $metrics->{total_families};
        if ($singleton_ratio >= 0.6) {
            $should_continue = 0;
            print "Override: Stop due to singleton ratio >= 60% (" . sprintf("%.1f%%", $singleton_ratio * 100) . ")\n";
        }
    }
    
    return ($should_continue, $metrics);
}

sub calculate_available_size {
    my ($genome_file, $exclusion_bed) = @_;
    
    # Get genome index
    unless (-s "$genome_file.fai") {
        run_cmd("samtools faidx $genome_file");
    }
    
    # Calculate total genome size
    my $total_size = 0;
    open(my $fh, '<', "$genome_file.fai") or die "Cannot read genome index: $!\n";
    while (<$fh>) {
        chomp;
        my @fields = split /\t/;
        $total_size += $fields[1];
    }
    close($fh);
    
    # Calculate excluded size (only from BED files, not from previous sampling)
    my $excluded_size = 0;
    if (-s $exclusion_bed && $exclusion_bed eq "exclusion_mask.bed") {
        # Only count original BED exclusions, not accumulated sampling regions
        open($fh, '<', $exclusion_bed) or die "Cannot read exclusion BED: $!\n";
        while (<$fh>) {
            chomp;
            my @fields = split /\t/;
            $excluded_size += ($fields[2] - $fields[1]);
        }
        close($fh);
    }
    
    return $total_size - $excluded_size;
}

sub merge_msp_files {
    my ($msp_files_ref, $output_file) = @_;
    my @msp_files = @$msp_files_ref;
    
    open(my $out_fh, '>', $output_file) or die "Cannot write to $output_file: $!\n";
    
    for my $msp_file (@msp_files) {
        next unless -s $msp_file;
        open(my $in_fh, '<', $msp_file) or die "Cannot read $msp_file: $!\n";
        while (<$in_fh>) {
            print $out_fh $_;
        }
        close($in_fh);
    }
    
    close($out_fh);
}

sub merge_all_track_results {
    my ($msp_files_ref, $genome_file, $threads) = @_;
    my @msp_files = @$msp_files_ref;
    
    if (@msp_files) {
        print "Available MSP tracks: " . scalar(@msp_files) . "\n";
        for my $msp_file (@msp_files) {
            my $size = -s $msp_file;
            print "  - $msp_file (size: $size bytes)\n";
        }
        
        if (@msp_files == 1) {
            # Only one track - use symlink to avoid copying
            print "Only one MSP track available, using symlink...\n";
            unlink "msp.out" if -e "msp.out";
            run_cmd("ln -sf " . File::Spec->abs2rel($msp_files[0]) . " msp.out");
        } else {
            # Multiple tracks - need to merge
            print "Merging " . scalar(@msp_files) . " MSP tracks...\n";
            merge_msp_files(\@msp_files, "msp.out");
        }
        
        # Determine K parameter
        my $k_param = determine_k_parameter("msp.out");
        
        # Run RECON on merged results
        print "Running RECON with K=$k_param on merged tracks...\n";
        create_seq_name_list($genome_file);
        run_recon_pipeline($k_param);
        
        # Create RECON completion checkpoint
        open(my $recon_fh, '>', "recon_completed.ok") or die "Cannot create recon_completed.ok: $!\n";
        print $recon_fh "RECON pipeline completed at " . localtime() . "\n";
        print $recon_fh "MSP files used: " . join(", ", @msp_files) . "\n";
        print $recon_fh "K parameter: $k_param\n";
        close($recon_fh);
        
        print "RECON pipeline completed successfully.\n";
    } else {
        print "ERROR: No valid MSP files found from any track!\n";
        die "Cannot proceed without MSP data.\n";
    }
}

sub convert_msp_to_consensi {
    my ($msp_file, $output_consensi, $genome_file) = @_;
    
    print "Converting MSP file to consensi sequences...\n";
    
    # Create temporary directory for RECON processing
    my $temp_dir = "temp_msp_conversion_$$";
    make_path($temp_dir);
    
    my $current_dir = getcwd();
    chdir $temp_dir;
    
    # Copy MSP file
    run_cmd("cp $msp_file msp.out");
    
    # Create seq name list
    create_seq_name_list($genome_file);
    
    # Run minimal RECON to get families
    my $k_param = determine_k_parameter("msp.out");
    run_recon_pipeline($k_param);
    
    # Build consensi if families found
    if (-d "summary" && -s "summary/families") {
        run_cmd("build_for_RECON ./ $genome_file 1");
        
        if (-s "consensi.fa") {
            run_cmd("cp consensi.fa ../$output_consensi");
            print "Consensi sequences generated successfully\n";
        }
    }
    
    chdir $current_dir;
    
    # Clean up
    remove_tree($temp_dir);
}

sub merge_dual_track_results {
    my ($masked_msp, $sampling_msp, $genome_file, $threads) = @_;
    
    my @msp_files;
    push @msp_files, $masked_msp if $masked_msp && -s $masked_msp;
    push @msp_files, $sampling_msp if $sampling_msp && -s $sampling_msp;
    
    if (@msp_files) {
        print "Available MSP tracks: " . scalar(@msp_files) . "\n";
        for my $msp_file (@msp_files) {
            my $size = -s $msp_file;
            print "  - $msp_file (size: $size bytes)\n";
        }
        
        if (@msp_files == 1) {
            # Only one track - use symlink to avoid copying
            print "Only one MSP track available, using symlink...\n";
            unlink "msp.out" if -e "msp.out";
            run_cmd("ln -sf " . File::Spec->abs2rel($msp_files[0]) . " msp.out");
        } else {
            # Multiple tracks - need to merge
            print "Merging " . scalar(@msp_files) . " MSP tracks...\n";
            merge_msp_files(\@msp_files, "msp.out");
        }
        
        # Determine K parameter
        my $k_param = determine_k_parameter("msp.out");
        
        # Run RECON on results
        print "Running RECON with K=$k_param...\n";
        create_seq_name_list($genome_file);
        run_recon_pipeline($k_param);
    } else {
        print "ERROR: No valid MSP files found from either track!\n";
        print "Masked track MSP: " . (($masked_msp && -s $masked_msp) ? "$masked_msp (valid)" : "invalid or empty") . "\n";
        print "Sampling track MSP: " . (($sampling_msp && -s $sampling_msp) ? "$sampling_msp (valid)" : "invalid or empty") . "\n";
        die "Cannot proceed without MSP data.\n";
    }
}

sub create_blast_database {
    my ($fasta_file, $db_name) = @_;
    
    run_cmd("makeblastdb -in $fasta_file -dbtype nucl -parse_seqids -out $db_name");
}

sub extract_unmasked_regions {
    my ($masked_genome, $output_file, $min_gap_size) = @_;
    $min_gap_size ||= 30;  # Default 30bp gap threshold
    
    open(my $in, '<', $masked_genome) or die "Cannot open $masked_genome: $!\n";
    open(my $out, '>', $output_file) or die "Cannot write to $output_file: $!\n";
    
    my $current_id = "";
    my $current_seq = "";
    my $position = 0;
    my $unmasked_count = 0;
    
    while (my $line = <$in>) {
        chomp $line;
        
        if ($line =~ /^>(.*)/) {
            # Process previous sequence if exists
            if ($current_id && $current_seq) {
                process_sequence_for_unmasked($current_id, $current_seq, $out, \$unmasked_count, $min_gap_size);
            }
            
            $current_id = (split /\s+/, $1)[0];
            $current_seq = "";
            $position = 0;
        } else {
            $current_seq .= $line;
        }
    }
    
    # Process last sequence
    if ($current_id && $current_seq) {
        process_sequence_for_unmasked($current_id, $current_seq, $out, \$unmasked_count, $min_gap_size);
    }
    
    close($in);
    close($out);
    
    print "Extracted $unmasked_count unmasked regions (min gap: ${min_gap_size}bp)\n";
    
    return $output_file;
}

sub process_sequence_for_unmasked {
    my ($seq_id, $sequence, $out_fh, $count_ref, $min_gap_size) = @_;
    $min_gap_size ||= 30;
    
    my $seq_length = length($sequence);
    my @unmasked_regions = ();
    my $position = 0;
    
    # First pass: identify all unmasked regions
    while ($position < $seq_length) {
        # Find start of unmasked region (skip hard-masked bases)
        while ($position < $seq_length && substr($sequence, $position, 1) =~ /[NnXx]/) {
            $position++;
        }
        
        last if $position >= $seq_length;
        
        my $start = $position;
        
        # Find end of unmasked region (valid nucleotides)
        while ($position < $seq_length && substr($sequence, $position, 1) =~ /[ATCGatcg]/) {
            $position++;
        }
        
        my $end = $position - 1;
        my $length = $end - $start + 1;
        
        # Save region if significant length (≥100bp)
        if ($length >= 100) {
            push @unmasked_regions, { start => $start, end => $end, length => $length };
        }
    }
    
    # Second pass: merge regions with gaps < min_gap_size
    my @merged_regions = ();
    for my $region (@unmasked_regions) {
        if (@merged_regions == 0) {
            push @merged_regions, { %$region };  # Create a copy of the region
        } else {
            my $last_region = $merged_regions[-1];
            my $gap_size = $region->{start} - $last_region->{end} - 1;
            
            # If gap is smaller than threshold, merge regions
            if ($gap_size < $min_gap_size) {
                $last_region->{end} = $region->{end};
                $last_region->{length} = $last_region->{end} - $last_region->{start} + 1;
            } else {
                push @merged_regions, { %$region };  # Create a copy of the region
            }
        }
    }
    
    # Output merged regions with simple gi| IDs (extract only unmasked characters)
    for my $region (@merged_regions) {
        my $start = $region->{start};
        my $end = $region->{end};
        
        # Extract only unmasked characters in this range (valid nucleotides)
        my $unmasked_seq = "";
        for my $pos ($start..$end) {
            my $char = substr($sequence, $pos, 1);
            if ($char =~ /[ATCGatcg]/) {
                $unmasked_seq .= $char;
            }
        }
        
        # Only output if we have sufficient unmasked sequence
        if (length($unmasked_seq) >= 100) {
            # Use simple gi|N format for sequence ID
            $$count_ref++;
            print $out_fh ">gi|$$count_ref\n";
            print $out_fh "$unmasked_seq\n";
        }
    }
}

sub split_unmasked_into_chunks {
    my ($unmasked_file, $output_dir) = @_;
    
    make_path($output_dir) unless -d $output_dir;
    
    my %sequences = read_fasta($unmasked_file);
    
    my @chunk_files;
    my $chunk_id = 0;
    my $current_chunk_size = 0;
    my $current_chunk_seqs = [];
    
    foreach my $seq_id (sort keys %sequences) {
        my $sequence = $sequences{$seq_id};
        my $seq_length = length($sequence);
        
        if ($current_chunk_size + $seq_length > CHUNK_SIZE && @$current_chunk_seqs > 0) {
            my $chunk_file = save_chunk($output_dir, $chunk_id, $current_chunk_seqs, \%sequences);
            push @chunk_files, $chunk_file if $chunk_file;
            $chunk_id++;
            
            $current_chunk_seqs = [];
            $current_chunk_size = 0;
        }
        
        push @$current_chunk_seqs, $seq_id;
        $current_chunk_size += $seq_length;
    }
    
    if (@$current_chunk_seqs > 0) {
        my $chunk_file = save_chunk($output_dir, $chunk_id, $current_chunk_seqs, \%sequences);
        push @chunk_files, $chunk_file if $chunk_file;
    }
    
    return @chunk_files;
}

sub save_chunk {
    my ($output_dir, $chunk_id, $sequences_ref, $seq_hash_ref) = @_;
    my @sequences = @$sequences_ref;
    my %seq_hash = %$seq_hash_ref;
    
    return undef unless @sequences;
    
    my $chunk_file = "$output_dir/chunk_${chunk_id}.fa";
    open(my $fh, '>', $chunk_file) or die "Cannot write to $chunk_file: $!\n";
    
    for my $seq_id (@sequences) {
        print $fh ">$seq_id\n";
        print $fh "$seq_hash{$seq_id}\n";
    }
    
    close($fh);
    
    my $chunk_size = -s $chunk_file;
    print "Created chunk $chunk_id: " . sprintf("%.1f kb", $chunk_size/1024) . " (" . scalar(@sequences) . " sequences)\n";
    
    return $chunk_file;
}

sub create_blast_database_from_unmasked {
    my ($unmasked_file) = @_;
    
    run_cmd("cp $unmasked_file genome_db.fa");
    run_cmd("makeblastdb -in genome_db.fa -dbtype nucl -parse_seqids -out genome_db");
    
    print "BLAST database created from unmasked regions\n";
}

sub run_parallel_rmblastn {
    my ($chunk_files_ref, $total_threads, $threads_per_job, $use_outfmt_0) = @_;
    my @chunk_files = @$chunk_files_ref;
    
    $use_outfmt_0 //= 0;  # Default to tabular format
    
    my $max_parallel = int($total_threads / $threads_per_job);
    $max_parallel = 1 if $max_parallel < 1;
    
    print "Running " . scalar(@chunk_files) . " BLAST jobs with up to $max_parallel in parallel\n";
    
    my $pm = Parallel::ForkManager->new($max_parallel);
    my @blast_outputs;
    
    $pm->run_on_finish(sub {
        my ($pid, $exit_code, $ident, $exit_signal, $core_dump, $data_ref) = @_;
        if ($exit_code == 0 && defined $data_ref) {
            push @blast_outputs, $$data_ref;
        }
    });
    
    foreach my $chunk_file (@chunk_files) {
        my $output_file = $chunk_file;
        $output_file =~ s/\.fa$/.blast/;
        
        $pm->start and next;
        
        my $outfmt = $use_outfmt_0 ? "0" : "6";
        
        my $cmd = "rmblastn " .
                  "-query $chunk_file " .
                  "-db genome_db " .
                  "-out $output_file " .
                  "-outfmt $outfmt " .
                  "-evalue 1e-5 " .
                  "-num_threads $threads_per_job " .
                  "-word_size 11 " .
                  "-gapopen 5 " .
                  "-gapextend 2 " .
                  "-penalty -3 " .
                  "-reward 2 " .
                  "-dust yes " .
                  "-soft_masking true " .
                  "-no_greedy";
        
        system($cmd);
        
        $pm->finish(0, \$output_file);
    }
    
    $pm->wait_all_children;
    
    print "Completed " . scalar(@blast_outputs) . " BLAST jobs\n";
    return @blast_outputs;
}

sub merge_blast_to_msp {
    my ($blast_outputs_ref) = @_;
    my @blast_outputs = @$blast_outputs_ref;
    
    my $merged_tbl = "merged.tbl";
    my $merged_msp = "msp.out";
    
    open(my $out_fh, '>', $merged_tbl) or die "Cannot write to $merged_tbl: $!\n";
    
    foreach my $blast_file (@blast_outputs) {
        open(my $in_fh, '<', $blast_file) or die "Cannot read $blast_file: $!\n";
        
        while (my $line = <$in_fh>) {
            chomp $line;
            my @fields = split /\t/, $line;
            
            my $query_id = $fields[0];
            my $subject_id = $fields[1];
            
            my ($query_base) = $query_id =~ /^([^_]+(?:_[^_]+)*?)_\d+_\d+$/;
            my ($subject_base) = $subject_id =~ /^([^_]+(?:_[^_]+)*?)(?:_\d+_\d+)?$/;
            
            if (!$query_base || !$subject_base || $query_base ne $subject_base) {
                print $out_fh "$line\n";
            }
        }
        
        close($in_fh);
    }
    
    close($out_fh);
    
    run_cmd("Tbl2Msp $merged_tbl > $merged_msp");
    
    unlink $merged_tbl;
    foreach my $blast_file (@blast_outputs) {
        unlink $blast_file if -f $blast_file;
    }
    
    print "MSP file created: $merged_msp\n";
    return $merged_msp;
}

sub create_seq_name_list {
    my ($genome_file) = @_;
    
    print "Creating seq_name.list from $genome_file...\n";
    
    # Read sequence headers
    open(my $in, '<', $genome_file) or die "Cannot open $genome_file: $!\n";
    
    my @headers;
    while (my $line = <$in>) {
        if ($line =~ /^>(.*)/) {
            my $header = (split /\s+/, $1)[0];
            push @headers, $header;
        }
    }
    close($in);
    
    # Check if we have any sequences
    if (@headers == 0) {
        die "ERROR: No sequences found in $genome_file\n";
    }
    
    # Sort headers for consistency
    @headers = sort @headers;
    
    # Write seq_name.list in correct format
    open(my $out, '>', 'seq_name.list') or die "Cannot create seq_name.list: $!\n";
    print $out scalar(@headers) . "\n";  # First line: sequence count
    print $out "$_\n" for @headers;      # Remaining lines: sequence names
    close($out);
    
    # Create tmp.fa symlink for RECON - use absolute path
    my $abs_genome = abs_path($genome_file);
    if (!$abs_genome || !-f $abs_genome) {
        die "ERROR: Cannot find genome file: $genome_file\n";
    }
    
    # Remove old symlink if exists
    unlink "tmp.fa" if -l "tmp.fa" || -f "tmp.fa";
    
    # Create new symlink with absolute path
    run_cmd("ln -sf $abs_genome tmp.fa");
    
    # Verify symlink is valid
    if (!-f "tmp.fa") {
        die "ERROR: Failed to create valid tmp.fa symlink to $abs_genome\n";
    }
    
    print "Created seq_name.list with " . scalar(@headers) . " sequences\n";
    print "Created tmp.fa symlink to: $abs_genome\n";
    
    # Debug: Check file sizes
    my $tmp_size = -s "tmp.fa";
    print "DEBUG: tmp.fa size: $tmp_size bytes\n";
}

sub build_final_output {
    my ($threads) = @_;
    
    my $main_output_dir = File::Spec->rel2abs("../..");
    my $genome_file = File::Spec->catfile($main_output_dir, "genome", "genome.fa");
    run_cmd("build_for_RECON ./ $genome_file $threads");
}

sub read_fasta {
    my ($file) = @_;
    my %sequences;
    
    open(my $fh, '<', $file) or die "Cannot open $file: $!\n";
    
    my $current_id = "";
    my $current_seq = "";
    
    while (my $line = <$fh>) {
        chomp $line;
        
        if ($line =~ /^>(.*)/) {
            if ($current_id) {
                $sequences{$current_id} = $current_seq;
            }
            $current_id = (split /\s+/, $1)[0];
            $current_seq = "";
        } else {
            $current_seq .= $line;
        }
    }
    
    if ($current_id) {
        $sequences{$current_id} = $current_seq;
    }
    
    close($fh);
    return %sequences;
}

sub run_cmd {
    my $cmd = shift;
    print "  Running: $cmd\n" if $ENV{DEBUG};
    system($cmd) == 0 or die "Failed to execute: $cmd\n";
}

sub run_recon_pipeline {
    my ($k_param) = @_;
    
    print "Starting RECON pipeline with K=$k_param...\n";
    
    # Validate input data
    if (!-f "seq_name.list") {
        die "ERROR: seq_name.list not found. Cannot proceed with RECON.\n";
    }
    
    if (!-f "msp.out") {
        die "ERROR: msp.out not found. Cannot proceed with RECON.\n";
    }
    
    # Check if MSP file has content
    if (!-s "msp.out") {
        die "ERROR: msp.out is empty. No similarity data available for RECON.\n";
    }
    
    # Check seq_name.list format
    open(my $fh, '<', "seq_name.list") or die "Cannot read seq_name.list: $!\n";
    my $first_line = <$fh>;
    close($fh);
    
    if (!$first_line || $first_line !~ /^\d+$/) {
        die "ERROR: seq_name.list format is invalid. First line should be sequence count.\n";
    }
    
    chomp $first_line;
    if ($first_line == 0) {
        die "ERROR: seq_name.list reports 0 sequences. Cannot proceed with RECON.\n";
    }
    
    print "Input validation passed: $first_line sequences, MSP file size: " . (-s "msp.out") . " bytes\n";
    
    # Create necessary directories
    for my $dir ('images', 'summary') {
        make_path($dir) unless -d $dir;
    }
    
    # Run imagespread with K parameter
    print "Running imagespread with K=$k_param...\n";
    eval {
        run_cmd("imagespread seq_name.list msp.out $k_param > imagespread.log 2>&1");
    };
    if ($@) {
        print "ERROR: imagespread failed. Check imagespread.log for details.\n";
        if (-f "imagespread.log") {
            system("tail -20 imagespread.log");
        }
        die "RECON pipeline failed at imagespread step.\n";
    }
    move_gmon("imagespread");
    
    # Check if imagespread produced results
    if (!-f "images/spread1" || !-s "images/spread1") {
        print "WARNING: imagespread did not produce results. This may indicate insufficient similarity data.\n";
        return;  # Exit gracefully instead of crashing
    }
    
    # Sort images
    run_cmd("sort -k 3,3 -k 4n,4n -k 5nr,5nr images/spread1 > images/images_sorted");
    unlink glob "images/spread*";
    
    # Process element definition
    process_element_definition();
    
    # Process element redefinition
    process_element_redefinition();
    
    # Process edge redefinition
    process_edge_redefinition();
    
    # Run famdef with improved error handling
    my $famdef_checkpoint = "famdef.ok";
    
    if (-f $famdef_checkpoint) {
        print "famdef already completed (checkpoint found), skipping...\n";
    } else {
        print "Running famdef...\n";
        
        # Check if seq_name.list still exists and is valid
        if (!-f "seq_name.list" || !-s "seq_name.list") {
            die "ERROR: seq_name.list missing or empty before famdef step.\n";
        }
        
        eval {
            run_cmd("famdef seq_name.list > famdef.log 2>&1");
        };
        if ($@) {
            print "ERROR: famdef failed. Check famdef.log for details.\n";
            if (-f "famdef.log") {
                system("tail -20 famdef.log");
            }
            die "RECON pipeline failed at famdef step: $@\n";
        }
        
        move_gmon("famdef");
        
        # Create checkpoint
        open(my $fh, '>', $famdef_checkpoint) or die "Cannot create $famdef_checkpoint: $!\n";
        print $fh "famdef completed at " . localtime() . "\n";
        close($fh);
    }
    
    # Debug: Check what RECON produced
    print "\nDEBUG: Checking RECON output files:\n";
    
    if (-d "summary") {
        print "  summary/ directory exists\n";
        
        # Check for families file
        if (-f "summary/families") {
            my $size = -s "summary/families";
            my $lines = `wc -l < summary/families 2>/dev/null` || 0;
            chomp $lines;
            print "  summary/families: $size bytes, $lines lines\n";
            
            if ($lines == 0) {
                print "  WARNING: summary/families exists but is empty!\n";
            }
        } else {
            print "  WARNING: summary/families does not exist!\n";
        }
        
        # Check for eles file
        if (-f "summary/eles") {
            my $size = -s "summary/eles";
            my $lines = `wc -l < summary/eles 2>/dev/null` || 0;
            chomp $lines;
            print "  summary/eles: $size bytes, $lines lines\n";
            
            if ($lines == 0) {
                print "  WARNING: summary/eles exists but is empty!\n";
            }
            
            # Check first few lines for valid data
            if ($lines > 0) {
                print "  First 5 lines of summary/eles:\n";
                system("head -5 summary/eles | sed 's/^/    /'");
            }
        } else {
            print "  WARNING: summary/eles does not exist!\n";
        }
        
        # List all files in summary directory
        print "  All files in summary/:\n";
        system("ls -la summary/ | sed 's/^/    /'");
    } else {
        print "  ERROR: summary/ directory does not exist!\n";
    }
    
    print "\n";
}

sub process_element_definition {
    my $checkpoint = "eledef.ok";
    
    if (-f $checkpoint && -d 'ele_def_res') {
        print "Element definition already completed (checkpoint found), skipping...\n";
        return;
    }
    
    print "Processing element definition...\n";
    remove_tree('ele_def_res') if -d 'ele_def_res';
    make_path('ele_def_res');
    
    run_cmd("eledef seq_name.list msp.out single > eledef.log 2>&1");
    move_gmon("eledef");
    
    # Create checkpoint
    open(my $fh, '>', $checkpoint) or die "Cannot create $checkpoint: $!\n";
    print $fh "eledef completed at " . localtime() . "\n";
    close($fh);
}

sub process_element_redefinition {
    my $checkpoint = "eleredef.ok";
    
    if (-f $checkpoint && -d 'ele_redef_res') {
        print "Element redefinition already completed (checkpoint found), skipping...\n";
        return;
    }
    
    print "Processing element redefinition...\n";
    remove_tree('ele_redef_res') if -d 'ele_redef_res';
    make_path('ele_redef_res');
    
    create_symlinks('ele_def_res', 'ele_redef_res');
    run_cmd("eleredef seq_name.list > eleredef.log 2>&1");
    move_gmon("eleredef");
    remove_symlinks();
    
    # Create checkpoint
    open(my $fh, '>', $checkpoint) or die "Cannot create $checkpoint: $!\n";
    print $fh "eleredef completed at " . localtime() . "\n";
    close($fh);
}

sub process_edge_redefinition {
    my $checkpoint = "edgeredef.ok";
    
    if (-f $checkpoint && -d 'edge_redef_res') {
        print "Edge redefinition already completed (checkpoint found), skipping...\n";
        # Still need to create final symlinks
        create_symlinks('edge_redef_res', undef);
        return;
    }
    
    print "Processing edge redefinition...\n";
    remove_tree('edge_redef_res') if -d 'edge_redef_res';
    make_path('edge_redef_res');
    
    create_symlinks('ele_redef_res', 'edge_redef_res');
    run_cmd("edgeredef seq_name.list > edgeredef.log 2>&1");
    move_gmon("edgeredef");
    remove_symlinks();
    
    # Create final symlinks
    create_symlinks('edge_redef_res', undef);
    
    # Create checkpoint
    open(my $fh, '>', $checkpoint) or die "Cannot create $checkpoint: $!\n";
    print $fh "edgeredef completed at " . localtime() . "\n";
    close($fh);
}

sub move_gmon {
    my $prefix = shift;
    if (-f "gmon.out") {
        rename("gmon.out", "${prefix}-gmon.out") 
            or die "Cannot rename gmon.out: $!\n";
    }
}

sub create_symlinks {
    my ($src, $dst) = @_;
    
    # Remove old symlinks/directories
    remove_tree("tmp") if -e "tmp";
    remove_tree("tmp2") if -e "tmp2";
    unlink("summary") if -l "summary";
    
    # Create tmp symlink
    symlink($src, 'tmp') or die "Cannot create symlink tmp -> $src: $!\n";
    
    # Create tmp2 symlink - use src as default if dst is not provided
    my $tmp2_target = $dst || $src;
    symlink($tmp2_target, 'tmp2') or die "Cannot create symlink tmp2 -> $tmp2_target: $!\n";
    
    # IMPORTANT: Also create/update summary symlink
    # When dst is undef (after edgeredef), summary should point to the final result dir
    if (!$dst) {
        # This is the final stage - create summary symlink pointing to edge_redef_res
        if (-l "summary") {
            unlink("summary");
        }
        symlink($src, 'summary') or die "Cannot create symlink summary -> $src: $!\n";
        print "Created final summary symlink -> $src\n";
    }
}

sub remove_symlinks {
    remove_tree("tmp") if -e "tmp";
    remove_tree("tmp2") if -e "tmp2";
    # Don't remove summary - it may be needed by later steps
}

################################################################################
# Advanced RECON Pipeline for TE Discovery with Refiner.py Integration
# Version: 3.4
# Description: Pipelined BLAST-RECON with TRF masking, checkpoints and enhanced consensus
# Features: 
#   - TRF masking before BLAST analysis for improved TE detection accuracy
#   - Pipelined BLAST-RECON execution (BLAST(t) + RECON(t-1) parallel)
#   - Comprehensive checkpoint system for resume capability
#   - BLAST completion checkpoints and RECON step-by-step checkpoints
#   - Phase 3 & 4 replaced with Refiner.py for superior consensus quality
#   - Optimized thread allocation (60% BLAST, 40% RECON)
#   - Multi-method TRF masking (RepeatMasker/TRF/dustmasker fallback)
################################################################################

use strict;
use warnings;
use 5.010;

# Suppress redefined subroutine warnings
no warnings 'redefine';

use Cwd qw(getcwd abs_path);
use FindBin;
use lib $FindBin::RealBin;
use lib qw(/home/shuoc/tool/Pan_TE/bin 
           /home/shuoc/tool/Pan_TE/share 
           /home/shuoc/tool/miniconda3/envs/PGTA/share/RepeatMasker 
           /home/shuoc/tool/miniconda3/envs/PGTA/lib/perl5/site_perl);

use File::Path qw(make_path remove_tree);
use File::Spec;
use File::Basename;
use File::Temp qw(tempfile tempdir);
use Getopt::Long;
use Pod::Usage;
use POSIX qw(:sys_wait_h ceil floor);
# List::Util already imported at the beginning of the script
use Log::Log4perl qw(:easy);
use Parallel::ForkManager;
# Removed unused thread imports - using Parallel::ForkManager throughout

# Load custom utility module
use RECONUtils;

# ===================================================================
# HELPER FUNCTIONS - DEFINED BEFORE MAIN EXECUTION
# ===================================================================

sub split_chimeric_family {
    my ($family) = @_;
    
    # Simple chimera splitting - return undef for now
    # In a full implementation, this would use alignment analysis
    return undef;
}

sub families_are_redundant {
    my ($family1, $family2) = @_;
    
    # Simple redundancy check based on consensus similarity
    return 0 unless ($family1->{consensus} && $family2->{consensus});
    
    my $similarity = calculate_sequence_similarity(
        $family1->{consensus}, 
        $family2->{consensus}
    );
    
    return $similarity > 0.95;  # 95% similarity threshold
}

sub calculate_sequence_similarity {
    my ($seq1, $seq2) = @_;
    
    my $len1 = length($seq1);
    my $len2 = length($seq2);
    my $min_len = min($len1, $len2);
    
    return 0 if $min_len == 0;
    
    my $matches = 0;
    for my $i (0..$min_len-1) {
        $matches++ if substr($seq1, $i, 1) eq substr($seq2, $i, 1);
    }
    
    return $matches / $min_len;
}

sub merge_family_info {
    my ($target_family, $source_family) = @_;
    
    # Merge additional information from source to target
    if ($source_family->{members} && $target_family->{members}) {
        push @{$target_family->{members}}, @{$source_family->{members}};
    }
    
    # Update copy count
    $target_family->{copy_count} = scalar(@{$target_family->{members}}) if $target_family->{members};
}

sub calculate_gc_content {
    my ($file) = @_;
    return 0.4;  # Placeholder
}

sub estimate_repeat_content {
    my ($file) = @_;
    return 0.3;  # Placeholder
}

sub calculate_kmer_complexity {
    my ($file, $k) = @_;
    return 0.5;  # Placeholder
}

sub generate_sampling_layer {
    my ($genome_file, $layer) = @_;
    
    print STDERR "[DEBUG] Generating sampling layer: $layer->{name} for genome: $genome_file\n";
    
    # Check if genome file exists
    if (!-f $genome_file) {
        print STDERR "[ERROR] Genome file does not exist: $genome_file\n";
        return;
    }
    
    my $genome_size = -s $genome_file;
    print STDERR "[DEBUG] Genome file size: $genome_size bytes\n";
    
    # Use random window sampling strategy for all layers
    my $window_size = $layer->{window_size};
    my $target_bp = $layer->{sample_size};
    my $num_windows_needed = int($target_bp / $window_size);
    
    print STDERR "[DEBUG] Random window sampling: window_size=$window_size, target_bp=$target_bp, num_windows_needed=$num_windows_needed\n";
    
    # Step 1: Generate non-overlapping windows covering entire genome
    my $step_size = $window_size;  # No overlap: step = window size
    my $temp_windows_file = "$layer->{file}.tmp_windows";
    my $cmd1 = "seqkit sliding -s $step_size -W $window_size $genome_file > $temp_windows_file";
    print STDERR "[DEBUG] Step 1 - Generate all windows: $cmd1\n";
    
    my $result1 = system($cmd1);
    if ($result1 != 0) {
        print STDERR "[ERROR] Window generation failed with exit code: $result1\n";
        return;
    }
    
    # Check how many windows we generated
    my $total_windows = `seqkit stats -T $temp_windows_file | tail -n 1 | cut -f 4`;
    chomp $total_windows;
    print STDERR "[DEBUG] Total windows generated: $total_windows\n";
    
    # Step 2: Randomly sample the required number of windows
    if ($num_windows_needed >= $total_windows) {
        # If we need all or more windows than available, use all
        print STDERR "[DEBUG] Using all available windows ($total_windows)\n";
        system("mv $temp_windows_file $layer->{file}");
    } else {
        # Random sampling of windows
        my $cmd2 = "seqkit sample -s 42 -n $num_windows_needed $temp_windows_file > $layer->{file}";
        print STDERR "[DEBUG] Step 2 - Random window sampling: $cmd2\n";
        
        my $result2 = system($cmd2);
        if ($result2 != 0) {
            print STDERR "[ERROR] Window sampling failed with exit code: $result2\n";
            unlink $temp_windows_file;
            return;
        }
        
        # Clean up temporary file
        unlink $temp_windows_file;
    }
    
    # Check if output file was created and has content
    my $output_size = -s $layer->{file} || 0;
    print STDERR "[DEBUG] Sampling completed. Output file: $layer->{file}, size: $output_size bytes\n";
    
    if ($output_size == 0) {
        print STDERR "[WARN] Warning: sampling produced empty output file\n";
    } else {
        # Calculate actual number of windows and estimated size
        my $actual_windows = `seqkit stats -T $layer->{file} | tail -n 1 | cut -f 4`;
        chomp $actual_windows;
        my $estimated_size = $actual_windows * $window_size;
        print STDERR "[DEBUG] Final result: $actual_windows windows, estimated $estimated_size bp\n";
    }
}

sub generate_coarse_sample {
    my ($genome_file, $genome_size) = @_;
    
    my $sample_file = "/tmp/coarse_sample.fa";
    my $sample_size = min(100_000_000, int($genome_size * 0.1));
    
    # Ensure we have a minimum sample size
    $sample_size = max(10_000_000, $sample_size);  # At least 10MB
    
    # Calculate proportion for coarse sampling
    my $proportion = ($genome_size > 0) ? min(1.0, $sample_size / $genome_size) : 0.1;
    system("seqkit sample -s 42 -p $proportion --two-pass $genome_file > $sample_file 2>/dev/null");
    
    return $sample_file;
}

sub identify_te_hotspots {
    my ($sample_file) = @_;
    
    # Placeholder: return mock hotspots
    return [
        { id => 'hotspot1', start => 1000, end => 5000 },
        { id => 'hotspot2', start => 10000, end => 15000 },
    ];
}

sub build_consensus_sequence {
    my ($members) = @_;
    
    return '' unless $members && @$members;
    return $members->[0]{sequence} if @$members == 1;
    
    # Simple consensus: return longest sequence
    my $longest = $members->[0]{sequence};
    foreach my $member (@$members) {
        if (length($member->{sequence}) > length($longest)) {
            $longest = $member->{sequence};
        }
    }
    
    return $longest;
}

sub check_for_tirs {
    my ($consensus) = @_;
    return { found => 0, length => 0, identity => 0 };
}

sub check_for_ltrs {
    my ($consensus) = @_;
    return { found => 0, length => 0, identity => 0 };
}

sub check_for_tsds {
    my ($family) = @_;
    return { found => 0, length => 0, sequence => '' };
}

sub assess_coding_potential {
    my ($consensus) = @_;
    return { has_orfs => 0, longest_orf => 0 };
}

sub analyze_alignment_edges {
    my ($msa_file) = @_;
    return { needs_trimming => 0, trim_start => 0, trim_end => 0 };
}

sub trim_consensus {
    my ($consensus, $start, $end) = @_;
    return $consensus;  # No trimming for now
}

sub calculate_sequence_distances {
    my ($members) = @_;
    return [];  # Empty distance matrix
}

sub hierarchical_clustering {
    my ($distance_matrix, $threshold) = @_;
    return [];  # No clusters
}

sub calculate_average_divergence {
    my ($cluster) = @_;
    return 0.1;  # Placeholder divergence
}

sub calculate_gc_content_string {
    my ($sequence) = @_;
    my $gc_count = ($sequence =~ tr/GCgc//);
    return length($sequence) ? $gc_count / length($sequence) : 0;
}

sub calculate_std_dev {
    my ($values) = @_;
    return 0 unless @$values;
    
    my $mean = sum(@$values) / @$values;
    my $sum_sq = sum(map { ($_ - $mean) ** 2 } @$values);
    return sqrt($sum_sq / @$values);
}

sub classify_ltr_family {
    my ($family) = @_;
    return 'Gypsy';  # Placeholder
}

sub classify_dna_family {
    my ($family) = @_;
    return 'hAT';  # Placeholder
}

sub generate_single_focused_sample {
    my ($genome_file, $hotspot) = @_;
    
    # Generate a single focused sample file based on hotspot
    my $sample_file = "/tmp/focused_$hotspot->{id}.fa";
    
    # Extract sequences from hotspot region
    system("seqkit subseq -r $hotspot->{start}:$hotspot->{end} $genome_file > $sample_file 2>/dev/null");
    
    return $sample_file;
}

sub perform_msa_threaded {
    my ($sequences, $msa_file, $threads) = @_;
    
    # Create temporary input file
    my $temp_input = "$msa_file.input";
    open(my $fh, '>', $temp_input);
    
    for my $i (0..$#$sequences) {
        print $fh ">seq_$i\n$sequences->[$i]\n";
    }
    close $fh;
    
    # Run MAFFT with threading if available
    my $mafft_cmd = "mafft";
    if ($threads > 1) {
        $mafft_cmd .= " --thread $threads";
    }
    $mafft_cmd .= " --auto $temp_input > $msa_file 2>/dev/null";
    
    system($mafft_cmd);
    
    # Clean up temp file
    unlink $temp_input;
}

sub calculate_family_statistics {
    my ($family) = @_;
    
    # Calculate basic statistics for the family
    $family->{copy_count} = scalar(@{$family->{members}}) if $family->{members};
    $family->{consensus_length} = length($family->{consensus} || '');
    
    # Calculate average divergence if members exist
    if ($family->{members} && scalar(@{$family->{members}}) > 1) {
        my $total_divergence = 0;
        my $comparisons = 0;
        
        for my $i (0..$#{$family->{members}}) {
            for my $j ($i+1..$#{$family->{members}}) {
                my $div = calculate_pairwise_divergence(
                    $family->{members}[$i]{sequence},
                    $family->{members}[$j]{sequence}
                );
                $total_divergence += $div;
                $comparisons++;
            }
        }
        
        if ($comparisons > 0) {
            $family->{divergence} = $total_divergence / $comparisons;
        }
    }
}

sub calculate_pairwise_divergence {
    my ($seq1, $seq2) = @_;
    
    # Simple divergence calculation
    my $min_len = min(length($seq1), length($seq2));
    return 1.0 if $min_len == 0;
    
    my $differences = 0;
    for my $i (0..$min_len-1) {
        $differences++ if substr($seq1, $i, 1) ne substr($seq2, $i, 1);
    }
    
    return $differences / $min_len;
}

# ===================================================================
# GLOBAL CONFIGURATION
# ===================================================================

# Initialize logging
Log::Log4perl->easy_init({
    level   => $INFO,
    layout  => '[%d{yyyy-MM-dd HH:mm:ss}] %p %m%n',
    file    => '>>' . getcwd() . '/recon_advanced.log'
});
my $logger = Log::Log4perl->get_logger();

our $VERSION = '3.0.0';

# Configuration
my %config = (
    genome_file     => undef,
    input_lib       => undef,  # Optional: existing TE library to extend
    output_dir      => undef,
    threads         => 4,
    batch_size      => 100000,
    min_family_size => 3,
    min_length      => 50,
    max_length      => 50000,
    force           => 0,
    debug           => 0,
    keep_temp       => 0,
);

# Genome size thresholds
my %GENOME_THRESHOLDS = (
    TINY   => 100_000_000,     # 100MB
    SMALL  => 500_000_000,     # 500MB
    MEDIUM => 1_000_000_000,   # 1GB
    LARGE  => 5_000_000_000,   # 5GB
    HUGE   => 10_000_000_000,  # 10GB
);

# ===================================================================
# MAIN EXECUTION
# ===================================================================

sub main {
    parse_arguments();
    setup_environment();
    
    $logger->info("=== Advanced RECON Pipeline v$VERSION Started ===");
    
    # Get genome information
    my $genome_size = -s $config{genome_file};
    my $genome_type = classify_genome_size($genome_size);
    
    $logger->info("Genome size: " . format_size($genome_size) . " (Type: $genome_type)");
    
    # Phase 1: Intelligent preprocessing
    $logger->info("Phase 1: Intelligent preprocessing...");
    my $prepared_input = prepare_recon_input($config{genome_file}, $genome_size, $genome_type);
    
    # Phase 2: Adaptive RECON execution
    $logger->info("Phase 2: Running adaptive RECON...");
    my $raw_families = run_adaptive_recon($prepared_input, $genome_type);
    
    # Phase 3: Biological optimization
    $logger->info("Phase 3: Biological feature optimization...");
    my $optimized_families = biological_optimization($raw_families);
    
    # Phase 4: Quality control
    $logger->info("Phase 4: Comprehensive quality control...");
    my $validated_families = comprehensive_quality_control($optimized_families);
    
    # Phase 5: Integration and finalization
    $logger->info("Phase 5: Integration and finalization...");
    my $final_library = integrate_and_finalize($validated_families);
    
    # Generate output
    write_output($final_library);
    generate_report($final_library);
    
    # Cleanup
    cleanup() unless $config{keep_temp};
    
    $logger->info("=== Pipeline completed successfully ===");
}

# ===================================================================
# PHASE 1: INTELLIGENT PREPROCESSING
# ===================================================================

sub prepare_recon_input {
    my ($genome_file, $genome_size, $genome_type) = @_;
    
    $logger->info("Preparing input based on genome type: $genome_type");
    
    # Assess genome complexity
    my $complexity = assess_genome_complexity($genome_file);
    $logger->info("Genome complexity score: $complexity");
    
    my $prepared_input;
    
    if ($genome_type eq 'TINY') {
        # Direct full genome analysis
        $prepared_input = prepare_full_genome_strategy($genome_file);
    }
    elsif ($genome_type eq 'SMALL') {
        # Partitioned parallel analysis
        $prepared_input = prepare_partitioned_strategy($genome_file, $genome_size);
    }
    elsif ($genome_type eq 'MEDIUM' || $genome_type eq 'LARGE') {
        # Multi-layer sampling
        $prepared_input = prepare_multilayer_sampling($genome_file, $genome_size, $complexity);
    }
    else {  # HUGE
        # Hierarchical adaptive sampling
        $prepared_input = prepare_hierarchical_sampling($genome_file, $genome_size, $complexity);
    }
    
    return $prepared_input;
}

sub assess_genome_complexity {
    my ($genome_file) = @_;
    
    $logger->info("Assessing genome complexity...");
    
    my $sample_size = 10_000_000;  # Sample 10MB for quick assessment
    my $sample_file = "$config{output_dir}/tmp/complexity_sample.fa";
    
    # Create sample - use proportion to approximate target size
    my $genome_file_size = -s $genome_file;
    my $proportion = ($genome_file_size > 0) ? min(1.0, $sample_size / $genome_file_size) : 0.1;
    system("seqkit sample -s 42 -p $proportion --two-pass $genome_file > $sample_file 2>/dev/null");
    
    # Calculate metrics
    my $gc_content = calculate_gc_content($sample_file);
    my $repeat_content = estimate_repeat_content($sample_file);
    my $kmer_complexity = calculate_kmer_complexity($sample_file, 15);
    
    # Complexity score (0-1)
    my $complexity = ($gc_content * 0.3) + ($repeat_content * 0.5) + ($kmer_complexity * 0.2);
    
    unlink $sample_file unless $config{keep_temp};
    
    return $complexity;
}

sub prepare_full_genome_strategy {
    my ($genome_file) = @_;
    
    $logger->info("Using full genome strategy");
    
    return {
        strategy => 'full_genome',
        files => [$genome_file],
        params => {
            batch_size => $config{batch_size},
            min_family_size => $config{min_family_size},
        }
    };
}

sub prepare_partitioned_strategy {
    my ($genome_file, $genome_size) = @_;
    
    $logger->info("Using partitioned parallel strategy");
    
    my $num_partitions = min($config{threads}, 8);
    my $partition_size = int($genome_size / $num_partitions);
    
    my @partition_files;
    
    # Create partitions
    for my $i (1..$num_partitions) {
        my $partition_file = "$config{output_dir}/tmp/partition_$i.fa";
        my $start_pos = ($i - 1) * $partition_size;
        my $end_pos = $i * $partition_size;
        
        system("seqkit subseq -r ${start_pos}:${end_pos} $genome_file > $partition_file 2>/dev/null");
        push @partition_files, $partition_file;
    }
    
    return {
        strategy => 'partitioned',
        files => \@partition_files,
        params => {
            batch_size => $config{batch_size},
            min_family_size => max(2, $config{min_family_size} - 1),
        }
    };
}

sub prepare_multilayer_sampling {
    my ($genome_file, $genome_size, $complexity) = @_;
    
    $logger->info("Using multi-layer sampling strategy");
    
    my @layers;
    
    # Layer 1: Dense sampling for high-copy TEs
    push @layers, {
        name => 'dense',
        sample_size => min(500_000_000, $genome_size * 0.3),
        window_size => 100_000,
        method => 'random_window',
        file => "$config{output_dir}/tmp/layer_dense.fa"
    };
    
    # Layer 2: Sparse sampling for medium-copy TEs
    push @layers, {
        name => 'sparse',
        sample_size => min(300_000_000, $genome_size * 0.2),
        window_size => 100_000,
        method => 'random_window',
        file => "$config{output_dir}/tmp/layer_sparse.fa"
    };
    
    # Layer 3: Random sampling for discovery
    push @layers, {
        name => 'random',
        sample_size => min(200_000_000, $genome_size * 0.1),
        window_size => 100_000,
        method => 'random_window',
        file => "$config{output_dir}/tmp/layer_random.fa"
    };
    
    # Generate layer files in parallel
    my $pm_sampling = Parallel::ForkManager->new(min($config{threads}, scalar(@layers)));
    
    foreach my $layer (@layers) {
        $pm_sampling->start and next;
        
        generate_sampling_layer($genome_file, $layer);
        
        $pm_sampling->finish;
    }
    
    $pm_sampling->wait_all_children;
    
    return {
        strategy => 'multilayer',
        files => [map { $_->{file} } @layers],
        layers => \@layers,
        params => {
            batch_size => $config{batch_size},
            min_family_size => $config{min_family_size},
        }
    };
}

sub prepare_hierarchical_sampling {
    my ($genome_file, $genome_size, $complexity) = @_;
    
    $logger->info("Using hierarchical adaptive sampling strategy");
    
    # Initial coarse sampling
    my $coarse_sample = generate_coarse_sample($genome_file, $genome_size);
    
    # Run preliminary analysis
    my $hotspots = identify_te_hotspots($coarse_sample);
    
    # Generate focused samples in parallel
    my $pm_focused = Parallel::ForkManager->new($config{threads});
    my @focused_results;
    
    $pm_focused->run_on_finish(sub {
        my ($pid, $exit_code, $ident, $exit_signal, $core_dump, $data) = @_;
        push @focused_results, $data if $data;
    });
    
    foreach my $hotspot (@$hotspots) {
        $pm_focused->start and next;
        
        my $focused_sample = generate_single_focused_sample($genome_file, $hotspot);
        
        $pm_focused->finish(0, $focused_sample);
    }
    
    $pm_focused->wait_all_children;
    
    # Combine samples
    my @all_samples = ($coarse_sample, @focused_results);
    
    return {
        strategy => 'hierarchical',
        files => \@all_samples,
        hotspots => $hotspots,
        params => {
            batch_size => $config{batch_size} * 2,
            min_family_size => $config{min_family_size},
            adaptive => 1,
        }
    };
}

# ===================================================================
# PHASE 2: ADAPTIVE RECON EXECUTION
# ===================================================================

sub run_adaptive_recon {
    my ($prepared_input, $genome_type) = @_;
    
    $logger->info("Running adaptive RECON with strategy: $prepared_input->{strategy}");
    
    my $all_families = {};
    
    # Define iteration rounds based on genome type
    my @rounds = get_recon_rounds($genome_type);
    
    foreach my $round (@rounds) {
        $logger->info("RECON round: $round->{name} (identity: $round->{identity}, coverage: $round->{coverage})");
        
        # Prepare round-specific parameters
        my $round_params = prepare_round_parameters($round, $prepared_input->{params});
        
        # Run RECON for this round
        my $round_families;
        
        if ($prepared_input->{strategy} eq 'partitioned') {
            $round_families = run_parallel_recon($prepared_input->{files}, $round_params);
        }
        else {
            $round_families = run_sequential_recon($prepared_input->{files}, $round_params);
        }
        
        # Quality filter for this round
        $round_families = filter_round_families($round_families, $round);
        
        # Merge into all families
        merge_family_results($all_families, $round_families);
        
        # Update input for next round (remove identified sequences)
        if ($round != $rounds[-1]) {
            $prepared_input = update_input_for_next_round($prepared_input, $round_families);
        }
    }
    
    return $all_families;
}

sub get_recon_rounds {
    my ($genome_type) = @_;
    
    my @rounds;
    
    if ($genome_type eq 'TINY' || $genome_type eq 'SMALL') {
        @rounds = (
            { name => 'high_similarity', identity => 0.90, coverage => 0.80 },
            { name => 'medium_similarity', identity => 0.70, coverage => 0.50 },
            { name => 'low_similarity', identity => 0.60, coverage => 0.30 },
        );
    }
    else {
        @rounds = (
            { name => 'strict', identity => 0.95, coverage => 0.90 },
            { name => 'high', identity => 0.85, coverage => 0.70 },
            { name => 'medium', identity => 0.70, coverage => 0.50 },
            { name => 'relaxed', identity => 0.60, coverage => 0.30 },
        );
    }
    
    return @rounds;
}

sub prepare_round_parameters {
    my ($round, $base_params) = @_;
    
    # Clone base parameters
    my %round_params = %$base_params;
    
    # Add round-specific parameters
    $round_params{identity} = $round->{identity};
    $round_params{coverage} = $round->{coverage};
    $round_params{round_name} = $round->{name};
    
    # Adjust batch size based on similarity level
    if ($round->{name} eq 'low_similarity' || $round->{name} eq 'relaxed') {
        # Use smaller batches for low similarity rounds
        $round_params{batch_size} = int($round_params{batch_size} * 0.5);
    }
    
    return \%round_params;
}

sub run_parallel_recon {
    my ($input_files, $params) = @_;
    
    $logger->info("Running pipelined BLAST-RECON on " . scalar(@$input_files) . " partitions");
    
    return run_pipelined_blast_recon($input_files, $params, 'SMALL');
}

sub run_sequential_recon {
    my ($input_files, $params) = @_;
    
    $logger->info("Running pipelined BLAST-RECON on " . scalar(@$input_files) . " files");
    
    return run_pipelined_blast_recon($input_files, $params, 'MEDIUM_LARGE');
}

# ===================================================================
# PIPELINED BLAST-RECON EXECUTION
# ===================================================================

sub run_pipelined_blast_recon {
    my ($input_files, $params, $strategy) = @_;
    
    $logger->info("Starting pipelined BLAST-RECON execution ($strategy strategy)");
    $logger->info("Processing " . scalar(@$input_files) . " input files");
    
    # Initialize checkpoint system
    my $checkpoint_manager = initialize_checkpoint_system($strategy);
    
    my $all_families = {};
    
    # Filter out completed batches for resume capability
    my @remaining_files = filter_completed_batches($checkpoint_manager, $input_files);
    my @pipeline_queue = @remaining_files;
    
    if (scalar(@remaining_files) < scalar(@$input_files)) {
        my $skipped = scalar(@$input_files) - scalar(@remaining_files);
        $logger->info("Resume mode: Processing $skipped remaining batches, $skipped already completed");
    }
    
    # Determine parallel thread allocation
    my $blast_threads = max(1, int($config{threads} * 0.6));  # 60% for BLAST
    my $recon_threads = max(1, int($config{threads} * 0.4));  # 40% for RECON
    
    $logger->info("Thread allocation: BLAST=$blast_threads, RECON=$recon_threads");
    
    # Pipeline state tracking
    my %pipeline_state = (
        blast_running => {},      # Currently running BLAST jobs
        blast_completed => {},    # Completed BLAST results waiting for RECON  
        recon_running => {},      # Currently running RECON jobs
        recon_completed => {},    # Completed RECON results
    );
    
    # Create pipeline management processes
    my $blast_pm = Parallel::ForkManager->new($blast_threads);
    my $recon_pm = Parallel::ForkManager->new($recon_threads);
    
    # Setup callback handlers with checkpoint integration
    $blast_pm->run_on_finish(\&handle_blast_completion, \%pipeline_state, $checkpoint_manager);
    $recon_pm->run_on_finish(\&handle_recon_completion, \%pipeline_state, $all_families, $checkpoint_manager);
    
    my $batch_counter = 0;
    my $total_batches = scalar(@pipeline_queue);
    
    # Main pipeline execution loop
    while (@pipeline_queue || keys %{$pipeline_state{blast_running}} || 
           keys %{$pipeline_state{blast_completed}} || keys %{$pipeline_state{recon_running}}) {
        
        # Start new BLAST jobs if threads available and files in queue
        while (@pipeline_queue && 
               scalar(keys %{$pipeline_state{blast_running}}) < $blast_threads) {
            
            my $input_file = shift @pipeline_queue;
            $batch_counter++;
            
            my $batch_id = "batch_${batch_counter}_" . basename($input_file);
            $logger->info("[$batch_counter/$total_batches] Starting BLAST for $batch_id");
            
            $pipeline_state{blast_running}->{$batch_id} = {
                input_file => $input_file,
                params => $params,
                start_time => time(),
            };
            
            $blast_pm->start($batch_id) and next;
            
            # BLAST execution in child process
            my $blast_result = run_blast_for_batch($input_file, $params, $batch_id, $checkpoint_manager);
            
            $blast_pm->finish(0, {
                batch_id => $batch_id,
                blast_result => $blast_result,
                input_file => $input_file,
                params => $params,
            });
        }
        
        # Start RECON jobs for completed BLAST results
        while (keys %{$pipeline_state{blast_completed}} && 
               scalar(keys %{$pipeline_state{recon_running}}) < $recon_threads) {
            
            my ($batch_id) = keys %{$pipeline_state{blast_completed}};
            my $blast_data = delete $pipeline_state{blast_completed}->{$batch_id};
            
            $logger->info("Starting RECON for $batch_id (BLAST completed)");
            
            $pipeline_state{recon_running}->{$batch_id} = {
                %$blast_data,
                recon_start_time => time(),
            };
            
            $recon_pm->start($batch_id) and next;
            
            # RECON execution in child process
            my $families = run_recon_for_batch($blast_data, $checkpoint_manager);
            
            $recon_pm->finish(0, {
                batch_id => $batch_id,
                families => $families,
            });
        }
        
        # Process completed children
        $blast_pm->reap_finished_children();
        $recon_pm->reap_finished_children();
        
        # Brief sleep to prevent busy waiting
        sleep(0.1);
    }
    
    # Wait for all remaining processes
    $blast_pm->wait_all_children();
    $recon_pm->wait_all_children();
    
    $logger->info("Pipelined BLAST-RECON execution completed");
    $logger->info("Total families discovered: " . scalar(keys %$all_families));
    
    return $all_families;
}

sub handle_blast_completion {
    my ($pipeline_state, $checkpoint_manager) = @_;
    
    return sub {
        my ($pid, $exit_code, $batch_id, $exit_signal, $core_dump, $data) = @_;
        
        if ($exit_code == 0 && $data) {
            my $batch_id = $data->{batch_id};
            delete $pipeline_state->{blast_running}->{$batch_id};
            $pipeline_state->{blast_completed}->{$batch_id} = $data;
            
            # Save BLAST checkpoint
            save_checkpoint($checkpoint_manager, 'blast', $batch_id, 'completed', {
                input_file => $data->{input_file},
                blast_result => $data->{blast_result}->{success} ? 'success' : 'partial'
            });
            
            my $elapsed = time() - ($pipeline_state->{blast_running}->{$batch_id}->{start_time} || time());
            $logger->info("BLAST completed for $batch_id (${elapsed}s) - checkpoint saved");
        } else {
            $logger->error("BLAST failed for batch (exit code: $exit_code)");
            if ($batch_id) {
                save_checkpoint($checkpoint_manager, 'blast', $batch_id, 'failed', {
                    exit_code => $exit_code,
                    error => "BLAST execution failed"
                });
            }
        }
    };
}

sub handle_recon_completion {
    my ($pipeline_state, $all_families, $checkpoint_manager) = @_;
    
    return sub {
        my ($pid, $exit_code, $batch_id, $exit_signal, $core_dump, $data) = @_;
        
        if ($exit_code == 0 && $data) {
            my $batch_id = $data->{batch_id};
            delete $pipeline_state->{recon_running}->{$batch_id};
            $pipeline_state->{recon_completed}->{$batch_id} = $data;
            
            # Merge families into results
            if ($data->{families}) {
                merge_family_results($all_families, $data->{families});
            }
            
            # Save RECON completion checkpoint
            my $family_count = scalar(keys %{$data->{families} || {}});
            save_checkpoint($checkpoint_manager, 'recon', $batch_id, 'completed', {
                families_found => $family_count,
                step => 'completed'
            });
            
            $logger->info("RECON completed for $batch_id ($family_count families) - checkpoint saved");
        } else {
            $logger->error("RECON failed for batch (exit code: $exit_code)");
            if ($batch_id) {
                save_checkpoint($checkpoint_manager, 'recon', $batch_id, 'failed', {
                    exit_code => $exit_code,
                    error => "RECON execution failed",
                    step => 'failed'
                });
            }
        }
    };
}

sub run_blast_for_batch {
    my ($input_file, $params, $batch_id, $checkpoint_manager) = @_;
    
    # Check if BLAST already completed
    if ($checkpoint_manager && is_checkpoint_completed($checkpoint_manager, 'blast', $batch_id)) {
        $logger->info("BLAST for $batch_id already completed, skipping");
        my $batch_dir = "$config{output_dir}/tmp/blast_$batch_id";
        return {
            batch_dir => $batch_dir,
            blast_files => { success => 1 },
            success => 1,
            resumed => 1,
        };
    }
    
    # Create batch-specific working directory
    my $batch_dir = "$config{output_dir}/tmp/blast_$batch_id";
    make_path($batch_dir);
    chdir($batch_dir);
    
    # Run BLAST with batch-specific parameters
    my $blast_result = run_blast_analysis($input_file, $params, $batch_id);
    
    return {
        batch_dir => $batch_dir,
        blast_files => $blast_result,
        success => ($blast_result && $blast_result->{success}),
    };
}

sub run_recon_for_batch {
    my ($blast_data, $checkpoint_manager) = @_;
    
    my $batch_id = $blast_data->{batch_id};
    
    # Check if RECON already completed
    if ($checkpoint_manager && is_checkpoint_completed($checkpoint_manager, 'recon', $batch_id)) {
        $logger->info("RECON for $batch_id already completed, skipping");
        # Load existing results if available
        return load_existing_recon_results($blast_data->{blast_result}->{batch_dir});
    }
    
    # Switch to batch directory  
    chdir($blast_data->{blast_result}->{batch_dir});
    
    # Run RECON pipeline steps with checkpoint support
    my $families = execute_recon_pipeline_with_checkpoints($blast_data->{params}, $batch_id, $checkpoint_manager);
    
    return $families;
}

# ===================================================================
# TRF MASKING FUNCTIONS  
# ===================================================================

sub apply_trf_masking {
    my ($input_file, $batch_id) = @_;
    
    $logger->debug("Applying TRF masking to $input_file for batch $batch_id");
    
    my $masked_output = "trf_masked_${batch_id}.fa";
    
    # Check if TRF masking already completed (checkpoint)
    if (-f $masked_output && -s $masked_output > 0) {
        $logger->debug("TRF masked file already exists: $masked_output");
        return $masked_output;
    }
    
    # Try different TRF masking approaches in priority order
    my $trf_result = try_trf_masking_methods($input_file, $masked_output, $batch_id);
    
    if ($trf_result) {
        $logger->debug("TRF masking completed successfully: $masked_output");
        return $masked_output;
    } else {
        $logger->warn("All TRF masking methods failed for batch $batch_id");
        return undef;
    }
}

sub try_trf_masking_methods {
    my ($input_file, $output_file, $batch_id) = @_;
    
    # Method 1: Try RepeatMasker with simple repeats
    if (try_repeatmasker_trf($input_file, $output_file)) {
        return 1;
    }
    
    # Method 2: Try standalone TRF
    if (try_standalone_trf($input_file, $output_file)) {
        return 1;
    }
    
    # Method 3: Try seqkit with tandem repeat detection
    if (try_seqkit_trf($input_file, $output_file)) {
        return 1;
    }
    
    # Method 4: Basic low complexity masking as fallback
    if (try_dustmasker_fallback($input_file, $output_file)) {
        return 1;
    }
    
    return 0;
}

sub try_repeatmasker_trf {
    my ($input_file, $output_file) = @_;
    
    # Check if RepeatMasker is available
    if (system("which RepeatMasker > /dev/null 2>&1") != 0) {
        return 0;
    }
    
    $logger->debug("Trying RepeatMasker for TRF masking");
    
    # Use RepeatMasker with simple repeats library
    my $rm_cmd = "RepeatMasker -species human -xsmall -no_is -norna -nolow -div 40 -q $input_file 2>/dev/null";
    
    if (system($rm_cmd) == 0) {
        my $rm_output = $input_file . ".masked";
        if (-f $rm_output && -s $rm_output > 0) {
            system("cp $rm_output $output_file");
            unlink($rm_output, $input_file . ".out", $input_file . ".tbl");
            return 1;
        }
    }
    
    return 0;
}

sub try_standalone_trf {
    my ($input_file, $output_file) = @_;
    
    # Check if TRF is available
    if (system("which trf > /dev/null 2>&1") != 0) {
        return 0;
    }
    
    $logger->debug("Trying standalone TRF");
    
    # Run TRF with standard parameters
    my $trf_cmd = "trf $input_file 2 7 7 80 10 50 500 -m -h 2>/dev/null";
    
    if (system($trf_cmd) == 0) {
        # TRF produces multiple output files, find the masked one
        my @trf_outputs = glob("${input_file}.*.mask");
        if (@trf_outputs && -f $trf_outputs[0] && -s $trf_outputs[0] > 0) {
            system("cp $trf_outputs[0] $output_file");
            unlink(@trf_outputs);
            return 1;
        }
    }
    
    return 0;
}

sub try_seqkit_trf {
    my ($input_file, $output_file) = @_;
    
    # Check if seqkit is available
    if (system("which seqkit > /dev/null 2>&1") != 0) {
        return 0;
    }
    
    $logger->debug("Trying seqkit for simple repeat masking");
    
    # Use seqkit to mask low complexity regions (basic approach)
    my $seqkit_cmd = "seqkit seq -w 0 $input_file | seqkit replace -p '([ATCG])\\1{4,}' -r 'NNNNN' > $output_file 2>/dev/null";
    
    if (system($seqkit_cmd) == 0 && -f $output_file && -s $output_file > 0) {
        return 1;
    }
    
    return 0;
}

sub try_dustmasker_fallback {
    my ($input_file, $output_file) = @_;
    
    # Check if dustmasker is available (comes with BLAST+)
    if (system("which dustmasker > /dev/null 2>&1") != 0) {
        return 0;
    }
    
    $logger->debug("Using dustmasker as TRF fallback");
    
    # Use dustmasker for low complexity masking
    my $dust_cmd = "dustmasker -in $input_file -infmt fasta -outfmt fasta -out $output_file 2>/dev/null";
    
    if (system($dust_cmd) == 0 && -f $output_file && -s $output_file > 0) {
        return 1;
    }
    
    return 0;
}

# ===================================================================
# CHECKPOINT MANAGEMENT SYSTEM
# ===================================================================

sub initialize_checkpoint_system {
    my ($strategy) = @_;
    
    my $checkpoint_dir = "$config{output_dir}/checkpoints";
    make_path($checkpoint_dir);
    
    my $checkpoint_manager = {
        strategy => $strategy,
        checkpoint_dir => $checkpoint_dir,
        blast_checkpoints => "$checkpoint_dir/blast_status.json",
        recon_checkpoints => "$checkpoint_dir/recon_status.json", 
        batch_registry => "$checkpoint_dir/batch_registry.json",
    };
    
    # Load existing checkpoints if resuming
    load_existing_checkpoints($checkpoint_manager);
    
    $logger->info("Checkpoint system initialized: $checkpoint_dir");
    return $checkpoint_manager;
}

sub load_existing_checkpoints {
    my ($checkpoint_manager) = @_;
    
    # Load BLAST checkpoint status
    if (-f $checkpoint_manager->{blast_checkpoints}) {
        $checkpoint_manager->{blast_status} = load_json_checkpoint($checkpoint_manager->{blast_checkpoints});
        $logger->info("Loaded existing BLAST checkpoints: " . scalar(keys %{$checkpoint_manager->{blast_status}}));
    } else {
        $checkpoint_manager->{blast_status} = {};
    }
    
    # Load RECON checkpoint status  
    if (-f $checkpoint_manager->{recon_checkpoints}) {
        $checkpoint_manager->{recon_status} = load_json_checkpoint($checkpoint_manager->{recon_checkpoints});
        $logger->info("Loaded existing RECON checkpoints: " . scalar(keys %{$checkpoint_manager->{recon_status}}));
    } else {
        $checkpoint_manager->{recon_status} = {};
    }
    
    # Load batch registry
    if (-f $checkpoint_manager->{batch_registry}) {
        $checkpoint_manager->{batches} = load_json_checkpoint($checkpoint_manager->{batch_registry});
    } else {
        $checkpoint_manager->{batches} = {};
    }
}

sub load_json_checkpoint {
    my ($file_path) = @_;
    
    open(my $fh, '<', $file_path) or return {};
    my $content = do { local $/; <$fh> };
    close($fh);
    
    # Simple JSON parsing for checkpoint data
    eval {
        my $data = parse_simple_json($content);
        return $data;
    };
    
    if ($@) {
        $logger->warn("Failed to parse checkpoint file $file_path: $@");
        return {};
    }
}

sub parse_simple_json {
    my ($json_str) = @_;
    
    # Simple JSON parser for checkpoint data (basic implementation)
    # This is a simplified version - for production use JSON::PP or similar
    my $data = {};
    
    if ($json_str =~ /^\s*\{(.*)\}\s*$/s) {
        my $content = $1;
        while ($content =~ /"([^"]+)"\s*:\s*"([^"]*)"/g) {
            $data->{$1} = $2;
        }
    }
    
    return $data;
}

sub save_checkpoint {
    my ($checkpoint_manager, $type, $batch_id, $status, $metadata) = @_;
    
    my $timestamp = time();
    my $checkpoint_data = {
        batch_id => $batch_id,
        status => $status,
        timestamp => $timestamp,
        metadata => $metadata || {},
    };
    
    if ($type eq 'blast') {
        $checkpoint_manager->{blast_status}->{$batch_id} = $checkpoint_data;
        save_json_checkpoint($checkpoint_manager->{blast_checkpoints}, $checkpoint_manager->{blast_status});
        
        # Create individual BLAST checkpoint file
        my $blast_checkpoint_file = "$checkpoint_manager->{checkpoint_dir}/${batch_id}.blast.ok";
        save_text_checkpoint($blast_checkpoint_file, $checkpoint_data);
        
    } elsif ($type eq 'recon') {
        $checkpoint_manager->{recon_status}->{$batch_id} = $checkpoint_data;
        save_json_checkpoint($checkpoint_manager->{recon_checkpoints}, $checkpoint_manager->{recon_status});
        
        # Create individual RECON step checkpoint
        my $step = $metadata->{step} || 'completed';
        my $recon_checkpoint_file = "$checkpoint_manager->{checkpoint_dir}/${batch_id}.recon.${step}.ok";
        save_text_checkpoint($recon_checkpoint_file, $checkpoint_data);
    }
    
    $logger->debug("Checkpoint saved: $type/$batch_id/$status");
}

sub save_json_checkpoint {
    my ($file_path, $data) = @_;
    
    open(my $fh, '>', $file_path) or do {
        $logger->error("Cannot save checkpoint: $file_path");
        return;
    };
    
    # Simple JSON serialization
    print $fh "{\n";
    my @entries;
    foreach my $key (sort keys %$data) {
        my $value = $data->{$key};
        if (ref $value eq 'HASH') {
            my $status = $value->{status} || 'unknown';
            my $timestamp = $value->{timestamp} || time();
            push @entries, qq("$key": "$status:$timestamp");
        }
    }
    print $fh join(",\n", @entries);
    print $fh "\n}\n";
    
    close($fh);
}

sub save_text_checkpoint {
    my ($file_path, $checkpoint_data) = @_;
    
    open(my $fh, '>', $file_path) or return;
    print $fh "# Checkpoint: $checkpoint_data->{batch_id}\n";
    print $fh "# Status: $checkpoint_data->{status}\n";
    print $fh "# Timestamp: " . localtime($checkpoint_data->{timestamp}) . "\n";
    if ($checkpoint_data->{metadata}) {
        foreach my $key (keys %{$checkpoint_data->{metadata}}) {
            print $fh "# $key: $checkpoint_data->{metadata}->{$key}\n";
        }
    }
    close($fh);
}

sub is_checkpoint_completed {
    my ($checkpoint_manager, $type, $batch_id, $step) = @_;
    
    if ($type eq 'blast') {
        my $checkpoint_file = "$checkpoint_manager->{checkpoint_dir}/${batch_id}.blast.ok";
        return -f $checkpoint_file;
    } elsif ($type eq 'recon') {
        if ($step) {
            my $checkpoint_file = "$checkpoint_manager->{checkpoint_dir}/${batch_id}.recon.${step}.ok";
            return -f $checkpoint_file;
        } else {
            # Check if all RECON steps are completed
            my @steps = qw(imagespread eledef eleredef edgeredef famdef);
            foreach my $s (@steps) {
                my $checkpoint_file = "$checkpoint_manager->{checkpoint_dir}/${batch_id}.recon.${s}.ok";
                return 0 unless -f $checkpoint_file;
            }
            return 1;
        }
    }
    
    return 0;
}

sub filter_completed_batches {
    my ($checkpoint_manager, $input_files) = @_;
    
    my @remaining_files;
    my $completed_count = 0;
    
    foreach my $input_file (@$input_files) {
        my $batch_id = "batch_" . basename($input_file);
        
        # Check if both BLAST and RECON are completed for this batch
        if (is_checkpoint_completed($checkpoint_manager, 'blast', $batch_id) &&
            is_checkpoint_completed($checkpoint_manager, 'recon', $batch_id)) {
            $completed_count++;
            $logger->info("Skipping completed batch: $batch_id");
        } else {
            push @remaining_files, $input_file;
        }
    }
    
    if ($completed_count > 0) {
        $logger->info("Resume detected: $completed_count batches already completed");
    }
    
    return @remaining_files;
}

# ===================================================================
# BLAST AND RECON EXECUTION FUNCTIONS
# ===================================================================

sub run_blast_analysis {
    my ($input_file, $params, $batch_id) = @_;
    
    $logger->debug("Running BLAST analysis for $batch_id with input: $input_file");
    
    # Convert to absolute path
    my $abs_input = File::Spec->rel2abs($input_file);
    if (!-f $abs_input) {
        $logger->error("Input file not found: $abs_input");
        return undef;
    }
    
    my $input_size = -s $abs_input;
    $logger->debug("Input file size: $input_size bytes");
    
    # Apply TRF masking before BLAST analysis
    my $masked_input = apply_trf_masking($abs_input, $batch_id);
    if (!$masked_input) {
        $logger->warn("TRF masking failed for $batch_id, using original input");
        $masked_input = $abs_input;
    }
    
    # Create BLAST database using masked input
    my $db_path = "recon_db_$batch_id";
    $logger->debug("Creating BLAST database: $db_path with masked input");
    
    my $makedb_result = system("makeblastdb -in $masked_input -dbtype nucl -out $db_path 2>/dev/null");
    if ($makedb_result != 0) {
        $logger->error("makeblastdb failed for $batch_id (exit code: $makedb_result)");
        return undef;
    }
    
    # Verify database creation
    my @db_files = glob("${db_path}.*");
    if (@db_files == 0) {
        $logger->error("BLAST database files not created for $batch_id");
        return undef;
    }
    
    # Run BLAST search  
    my $blast_threads = max(1, int($config{threads} * 0.6));  # Simple thread allocation
    my $blast_cmd = "blastn " .
                   "-task dc-megablast " .
                   "-db $db_path " .
                   "-query $masked_input " .
                   "-outfmt 6 " .
                   "-dust yes " .
                   "-evalue 1e-10 " .
                   "-max_target_seqs 100000000 " .
                   "-num_threads $blast_threads " .
                   "> recon.sample.blast6 2>/dev/null";
    
    $logger->debug("BLAST command: $blast_cmd");
    
    my $blast_result = system($blast_cmd);
    if ($blast_result != 0) {
        $logger->error("BLAST failed for $batch_id (exit code: $blast_result)");
        return undef;
    }
    
    # Convert BLAST to MSP format
    convert_blast_to_msp('recon.sample.blast6', 'msp.out', $params);
    
    # Prepare sequence name list using masked input
    prepare_recon_input_files($masked_input, $params);
    
    return {
        db_files => \@db_files,
        blast_file => 'recon.sample.blast6',
        msp_file => 'msp.out',
        success => 1,
    };
}

sub execute_recon_pipeline {
    my ($params) = @_;
    
    $logger->debug("Executing RECON pipeline steps (legacy mode)");
    
    # Step 1: imagespread - spread images
    make_path('images');
    system("imagespread seq_name.list msp.out > imagespread.log 2>&1");
    system("sort -k 3,3 -k 4n,4n -k 5nr,5nr images/spread1 > images/images_sorted 2>/dev/null");
    unlink glob "images/spread*";
    
    # Step 2: eledef - define elements
    make_path('ele_def_res');
    my $eledef_result = system("eledef seq_name.list msp.out single > eledef.log 2>&1");
    if ($eledef_result != 0) {
        $logger->warn("eledef step failed, continuing with partial results");
    }
    
    # Step 3: eleredef - redefine elements
    make_path('ele_redef_res');
    create_recon_symlinks('ele_def_res', 'ele_redef_res');
    system("eleredef seq_name.list > eleredef.log 2>&1");
    remove_recon_symlinks();
    
    # Step 4: edgeredef - redefine edges
    make_path('edge_redef_res');
    create_recon_symlinks('ele_redef_res', 'edge_redef_res');
    system("edgeredef seq_name.list > edgeredef.log 2>&1");
    remove_recon_symlinks();
    
    # Step 5: famdef - define families
    create_recon_symlinks('edge_redef_res', undef);
    system("famdef seq_name.list > famdef.log 2>&1");
    remove_recon_symlinks();
    
    # Parse results
    my $families = parse_recon_families($params);
    
    $logger->debug("RECON pipeline completed. Families found: " . scalar(keys %$families));
    
    return $families;
}

sub execute_recon_pipeline_with_checkpoints {
    my ($params, $batch_id, $checkpoint_manager) = @_;
    
    $logger->debug("Executing RECON pipeline with checkpoints for $batch_id");
    
    my @recon_steps = (
        { name => 'imagespread', desc => 'Image spreading', cmd => \&run_imagespread_step },
        { name => 'eledef', desc => 'Element definition', cmd => \&run_eledef_step },
        { name => 'eleredef', desc => 'Element redefinition', cmd => \&run_eleredef_step },
        { name => 'edgeredef', desc => 'Edge redefinition', cmd => \&run_edgeredef_step },
        { name => 'famdef', desc => 'Family definition', cmd => \&run_famdef_step },
    );
    
    foreach my $step (@recon_steps) {
        # Check if step already completed
        if (is_checkpoint_completed($checkpoint_manager, 'recon', $batch_id, $step->{name})) {
            $logger->debug("RECON step $step->{name} for $batch_id already completed, skipping");
            next;
        }
        
        $logger->info("Running RECON step: $step->{desc} for $batch_id");
        
        my $step_result = $step->{cmd}->($params);
        if ($step_result) {
            # Save step checkpoint
            save_checkpoint($checkpoint_manager, 'recon', $batch_id, 'step_completed', {
                step => $step->{name},
                description => $step->{desc}
            });
            $logger->debug("Checkpoint saved for RECON step $step->{name}/$batch_id");
        } else {
            $logger->warn("RECON step $step->{name} failed for $batch_id, continuing");
        }
    }
    
    # Parse final results
    my $families = parse_recon_families($params);
    
    $logger->debug("RECON pipeline with checkpoints completed for $batch_id. Families found: " . scalar(keys %$families));
    
    return $families;
}

sub run_imagespread_step {
    my ($params) = @_;
    
    make_path('images');
    my $result = system("imagespread seq_name.list msp.out > imagespread.log 2>&1");
    if ($result == 0) {
        system("sort -k 3,3 -k 4n,4n -k 5nr,5nr images/spread1 > images/images_sorted 2>/dev/null");
        unlink glob "images/spread*";
        return 1;
    }
    return 0;
}

sub run_eledef_step {
    my ($params) = @_;
    
    make_path('ele_def_res');
    my $result = system("eledef seq_name.list msp.out single > eledef.log 2>&1");
    return ($result == 0);
}

sub run_eleredef_step {
    my ($params) = @_;
    
    make_path('ele_redef_res');
    create_recon_symlinks('ele_def_res', 'ele_redef_res');
    my $result = system("eleredef seq_name.list > eleredef.log 2>&1");
    remove_recon_symlinks();
    return ($result == 0);
}

sub run_edgeredef_step {
    my ($params) = @_;
    
    make_path('edge_redef_res');
    create_recon_symlinks('ele_redef_res', 'edge_redef_res');
    my $result = system("edgeredef seq_name.list > edgeredef.log 2>&1");
    remove_recon_symlinks();
    return ($result == 0);
}

sub run_famdef_step {
    my ($params) = @_;
    
    create_recon_symlinks('edge_redef_res', undef);
    my $result = system("famdef seq_name.list > famdef.log 2>&1");
    remove_recon_symlinks();
    return ($result == 0);
}

sub load_existing_recon_results {
    my ($batch_dir) = @_;
    
    my $families_file = "$batch_dir/family_summary";
    if (-f $families_file) {
        # Parse existing family results
        my $families = {};
        open(my $fh, '<', $families_file);
        while (my $line = <$fh>) {
            chomp $line;
            next if $line =~ /^#/ || $line =~ /^\s*$/;
            
            my @members = split(/\s+/, $line);
            next unless @members >= 2;
            
            my $family_id = "family_" . scalar(keys %$families);
            $families->{$family_id} = {
                members => [map { { id => $_, sequence => '' } } @members],
                copy_count => scalar(@members),
            };
        }
        close($fh);
        
        $logger->debug("Loaded existing RECON results: " . scalar(keys %$families) . " families");
        return $families;
    }
    
    return {};
}

sub parse_recon_families {
    my ($params) = @_;
    
    my $families = {};
    
    # Read family summary if available
    if (-f 'family_summary') {
        open(my $sum_fh, '<', 'family_summary');
        while (my $line = <$sum_fh>) {
            chomp $line;
            next if $line =~ /^#/ || $line =~ /^\s*$/;
            
            my @members = split(/\s+/, $line);
            next unless @members >= $config{min_family_size};
            
            my $family_id = "family_" . scalar(keys %$families);
            
            $families->{$family_id} = {
                members => [],
                copy_count => 0,
            };
            
            foreach my $member (@members) {
                push @{$families->{$family_id}{members}}, {
                    id => $member,
                    sequence => '',  # Will be filled later if needed
                };
            }
            
            $families->{$family_id}{copy_count} = scalar(@{$families->{$family_id}{members}});
            
            # Build consensus if enough members
            if (scalar(@{$families->{$family_id}{members}}) >= 2) {
                $families->{$family_id}{consensus} = 
                    build_consensus_sequence($families->{$family_id}{members});
            }
        }
        close($sum_fh);
    }
    
    return $families;
}

sub filter_round_families {
    my ($families, $round) = @_;
    
    my $filtered = {};
    
    foreach my $family_id (keys %$families) {
        my $family = $families->{$family_id};
        
        # Apply round-specific filtering
        my $min_copies = ($round->{name} =~ /strict|high/) ? 5 : 3;
        my $min_length = ($round->{name} =~ /strict/) ? 100 : 50;
        
        if ($family->{copy_count} >= $min_copies && 
            $family->{consensus_length} >= $min_length) {
            $filtered->{$family_id} = $family;
        }
    }
    
    $logger->info("Filtered families: " . scalar(keys %$filtered) . " passed round $round->{name} criteria");
    
    return $filtered;
}

sub merge_family_results {
    my ($all_families, $new_families) = @_;
    
    foreach my $family_id (keys %$new_families) {
        if (exists $all_families->{$family_id}) {
            # Merge with existing family
            if ($new_families->{$family_id}{copy_count} > $all_families->{$family_id}{copy_count}) {
                $all_families->{$family_id} = $new_families->{$family_id};
            }
        }
        else {
            # Add new family
            $all_families->{$family_id} = $new_families->{$family_id};
        }
    }
}

sub update_input_for_next_round {
    my ($prepared_input, $identified_families) = @_;
    
    # Create a new prepared_input structure with sequences that weren't identified
    my $updated_input = {
        strategy => $prepared_input->{strategy},
        params => $prepared_input->{params},
        files => []
    };
    
    # For now, just return the original input
    # In a full implementation, we would mask or remove identified sequences
    $updated_input->{files} = $prepared_input->{files};
    
    return $updated_input;
}

sub merge_partition_results {
    my ($partition_results) = @_;
    
    my $merged = {};
    
    foreach my $partition (@$partition_results) {
        next unless $partition;
        merge_family_results($merged, $partition);
    }
    
    return $merged;
}

sub run_recon_on_file {
    my ($input_file, $params) = @_;
    
    my $basename = basename($input_file, '.fa');
    my $work_dir = "$config{output_dir}/tmp/recon_$basename";
    make_path($work_dir);
    
    # Change to working directory for RECON
    my $current_dir = getcwd();
    chdir $work_dir;
    
    # Prepare input files
    prepare_recon_input_files($input_file, $params);
    
    # Run RECON workflow steps
    my $families = run_recon_workflow($input_file, $params);
    
    # Change back to original directory
    chdir $current_dir;
    
    return $families;
}

sub prepare_recon_input_files {
    my ($input_file, $params) = @_;
    
    # Create sequence name list
    open(my $name_fh, '>', 'seq_name.list');
    open(my $seq_fh, '<', $input_file);
    
    while (my $line = <$seq_fh>) {
        if ($line =~ /^>(\S+)/) {
            print $name_fh "$1\n";
        }
    }
    close $seq_fh;
    close $name_fh;
    
    # Run MSPcrunch or prepare BLAST output in MSP format
    prepare_msp_output($input_file, $params);
}

sub prepare_msp_output {
    my ($input_file, $params) = @_;
    
    # Get current working directory and create absolute path for database
    my $cwd = getcwd();
    my $abs_input = abs_path($input_file);
    my $db_path = "$cwd/blastdb";
    
    # Check if input file exists and has content
    if (!-f $abs_input) {
        $logger->error("Input file does not exist: $abs_input");
        die "Input file not found: $abs_input";
    }
    
    my $input_size = -s $abs_input;
    if ($input_size == 0) {
        $logger->error("Input file is empty: $abs_input (size: $input_size bytes)");
        die "Input file is empty, cannot create BLAST database";
    }
    
    $logger->info("Input file check passed: $abs_input (size: $input_size bytes)");
    
    # Create BLAST database with absolute path - remove error suppression to see issues
    $logger->info("Creating BLAST database: makeblastdb -in $abs_input -dbtype nucl -out $db_path");
    my $makedb_result = system("makeblastdb -in $abs_input -dbtype nucl -out $db_path");
    if ($makedb_result != 0) {
        $logger->error("makeblastdb failed with exit code: $makedb_result");
        die "Failed to create BLAST database";
    }
    
    # Verify database files were created
    my @db_files = glob("${db_path}.*");
    if (@db_files == 0) {
        $logger->error("No database files found with pattern: ${db_path}.*");
        die "BLAST database files not created";
    }
    $logger->info("BLAST database created successfully. Files: " . join(", ", @db_files));
    
    # Use optimized BLAST parameters for RECON
    my $blast_threads = $config{threads};
    
    # Option 1: Original format 0 as requested (for traditional RECON compatibility)
    my $blast_cmd_fmt0 = "blastn " .
                        "-task dc-megablast " .
                        "-db $db_path " .
                        "-query $abs_input " .
                        "-outfmt 0 " .
                        "-dust yes " .
                        "-evalue 1e-10 " .
                        "-max_target_seqs 100000000 " .
                        "-num_threads $blast_threads " .
                        "> recon.sample.blastn";
    
    # Option 2: Tabular format for MSP conversion (for this pipeline)
    my $blast_cmd = "blastn " .
                   "-task dc-megablast " .
                   "-db $db_path " .
                   "-query $abs_input " .
                   "-outfmt 6 " .
                   "-dust yes " .
                   "-evalue 1e-10 " .
                   "-max_target_seqs 100000000 " .
                   "-num_threads $blast_threads " .
                   "> recon.sample.blast6";
    
    # Use tabular format for processing but also generate format 0 if needed
    $logger->info("Running BLAST command: $blast_cmd");
    my $blast_result = system($blast_cmd);
    if ($blast_result != 0) {
        $logger->error("BLAST failed with exit code: $blast_result");
        die "BLAST search failed";
    }
    $logger->info("BLAST search completed successfully");
    # Uncomment the next line if you need format 0 output as well
    # system($blast_cmd_fmt0);
    
    # Convert BLAST to MSP format - use the tabular output file
    convert_blast_to_msp('recon.sample.blast6', 'msp.out', $params);
}

sub convert_blast_to_msp {
    my ($blast_file, $msp_file, $params) = @_;
    
    open(my $blast_fh, '<', $blast_file);
    open(my $msp_fh, '>', $msp_file);
    
    while (my $line = <$blast_fh>) {
        chomp $line;
        my @fields = split(/\t/, $line);
        
        next unless @fields >= 12;
        
        my ($qid, $sid, $pident, $length, $mismatch, $gaps,
            $qstart, $qend, $sstart, $send, $evalue, $score) = @fields;
        
        # Skip self-hits
        next if $qid eq $sid;
        
        # Apply filters
        next if $pident < ($params->{min_identity} || 60);
        next if $length < ($params->{min_length} || 50);
        
        # MSP format: score seq1_start seq1_end seq1_name seq2_start seq2_end seq2_name
        print $msp_fh "$score $qstart $qend $qid $sstart $send $sid\n";
    }
    
    close $blast_fh;
    close $msp_fh;
}

sub run_recon_workflow {
    my ($input_file, $params) = @_;
    
    # Step 1: imagespread - spreads alignments
    make_path('images');
    system("imagespread seq_name.list msp.out > imagespread.log 2>&1");
    
    # Sort images
    system("sort -k 3,3 -k 4n,4n -k 5nr,5nr images/spread1 > images/images_sorted 2>/dev/null");
    unlink glob "images/spread*";
    
    # Step 2: eledef - define elements
    make_path('ele_def_res');
    system("eledef seq_name.list msp.out single > eledef.log 2>&1");
    
    # Step 3: eleredef - redefine elements
    make_path('ele_redef_res');
    create_recon_symlinks('ele_def_res', 'ele_redef_res');
    system("eleredef seq_name.list > eleredef.log 2>&1");
    remove_recon_symlinks();
    
    # Step 4: edgeredef - redefine edges
    make_path('edge_redef_res');
    create_recon_symlinks('ele_redef_res', 'edge_redef_res');
    system("edgeredef seq_name.list > edgeredef.log 2>&1");
    remove_recon_symlinks();
    
    # Step 5: famdef - define families
    create_recon_symlinks('edge_redef_res', undef);
    system("famdef seq_name.list > famdef.log 2>&1");
    remove_recon_symlinks();
    
    # Extract families from results
    my $families = extract_recon_families_from_output($input_file);
    
    return $families;
}

sub create_recon_symlinks {
    my ($from_dir, $to_dir) = @_;
    
    if ($from_dir && $to_dir) {
        # Link from one directory to another
        symlink("../$from_dir/asn1", "$to_dir/asn1");
        symlink("../$from_dir/eles", "$to_dir/eles");
    }
    elsif ($from_dir) {
        # Link from directory to current directory
        symlink("$from_dir/asn1", "asn1");
        symlink("$from_dir/eles", "eles");
    }
}

sub remove_recon_symlinks {
    unlink("asn1", "eles");
}

sub extract_recon_families_from_output {
    my ($input_fasta) = @_;
    
    my %families;
    
    # Read family assignments from famdef output
    if (-f 'families') {
        open(my $fam_fh, '<', 'families');
        while (my $line = <$fam_fh>) {
            chomp $line;
            my @members = split(/\s+/, $line);
            next unless @members >= $config{min_family_size};
            
            my $family_id = "family_" . (scalar(keys %families) + 1);
            $families{$family_id} = {
                id => $family_id,
                members => [],
            };
            
            foreach my $member (@members) {
                push @{$families{$family_id}{members}}, {
                    id => $member,
                    sequence => '',
                };
            }
        }
        close $fam_fh;
    }
    
    # Read sequences and assign to families
    my %sequences = read_fasta_sequences($input_fasta);
    
    foreach my $family_id (keys %families) {
        foreach my $member (@{$families{$family_id}{members}}) {
            $member->{sequence} = $sequences{$member->{id}} || '';
        }
        
        # Build consensus
        if (scalar(@{$families{$family_id}{members}}) >= 2) {
            $families{$family_id}{consensus} = 
                build_consensus_sequence($families{$family_id}{members});
        }
    }
    
    return \%families;
}

sub read_fasta_sequences {
    my ($file) = @_;
    
    my %sequences;
    my $current_id = '';
    my $current_seq = '';
    
    open(my $fh, '<', $file);
    while (my $line = <$fh>) {
        chomp $line;
        if ($line =~ /^>(\S+)/) {
            if ($current_id && $current_seq) {
                $sequences{$current_id} = $current_seq;
            }
            $current_id = $1;
            $current_seq = '';
        }
        else {
            $current_seq .= $line;
        }
    }
    # Don't forget last sequence
    if ($current_id && $current_seq) {
        $sequences{$current_id} = $current_seq;
    }
    close $fh;
    
    return %sequences;
}

# ===================================================================
# PHASE 3: ENHANCED CONSENSUS BUILDING WITH REFINER.PY
# ===================================================================

sub biological_optimization {
    my ($families) = @_;
    
    $logger->info("Enhanced consensus building for " . scalar(keys %$families) . " families using Refiner.py");
    
    my $optimized = {};
    my @family_ids = keys %$families;
    
    # Create temp directory for family processing
    my $temp_dir = create_temp_dir("refiner_families");
    
    # Use parallel processing for family optimization
    my $pm = Parallel::ForkManager->new($config{threads});
    my @optimization_results;
    
    $pm->run_on_finish(sub {
        my ($pid, $exit_code, $ident, $exit_signal, $core_dump, $data) = @_;
        push @optimization_results, $data if $data;
    });
    
    $logger->info("Processing " . scalar(@family_ids) . " families using $config{threads} threads");
    
    foreach my $family_id (@family_ids) {
        $pm->start and next;
        
        my $family = $families->{$family_id};
        
        # Skip very small families
        if (scalar(@{$family->{members}}) < 2) {
            $pm->finish(0, undef);
            next;
        }
        
        # Export family to FASTA and process with Refiner.py
        my $enhanced_family = process_family_with_refiner($family, $family_id, $temp_dir);
        
        if ($enhanced_family) {
            $pm->finish(0, { $family_id => $enhanced_family });
        } else {
            $logger->warn("Failed to enhance family $family_id, keeping original");
            $pm->finish(0, { $family_id => $family });
        }
    }
    
    $pm->wait_all_children;
    
    # Merge results from parallel processing
    foreach my $result (@optimization_results) {
        next unless $result;
        foreach my $family_id (keys %$result) {
            $optimized->{$family_id} = $result->{$family_id};
        }
    }
    
    # Cleanup temp directory
    cleanup_temp_dir($temp_dir) unless $config{keep_temp};
    
    $logger->info("Enhanced consensus building completed for " . scalar(keys %$optimized) . " families");
    return $optimized;
}

sub detect_structural_features {
    my ($family) = @_;
    
    my $consensus = $family->{consensus};
    return unless $consensus;
    
    # Check for Terminal Inverted Repeats (TIRs)
    my $tir_result = check_for_tirs($consensus);
    if ($tir_result->{found}) {
        $family->{structure_type} = 'TIR';
        $family->{tir_length} = $tir_result->{length};
        $family->{tir_identity} = $tir_result->{identity};
    }
    
    # Check for Long Terminal Repeats (LTRs)
    my $ltr_result = check_for_ltrs($consensus);
    if ($ltr_result->{found}) {
        $family->{structure_type} = 'LTR';
        $family->{ltr_length} = $ltr_result->{length};
        $family->{ltr_identity} = $ltr_result->{identity};
    }
    
    # Check for Target Site Duplications (TSDs)
    my $tsd_result = check_for_tsds($family);
    if ($tsd_result->{found}) {
        $family->{tsd_length} = $tsd_result->{length};
        $family->{tsd_sequence} = $tsd_result->{sequence};
    }
    
    # Check for coding potential
    my $coding_potential = assess_coding_potential($consensus);
    $family->{has_coding} = $coding_potential->{has_orfs};
    $family->{longest_orf} = $coding_potential->{longest_orf};
}

sub optimize_family_boundaries {
    my ($family) = @_;
    
    return unless scalar(@{$family->{members}}) >= 3;
    
    # Get member sequences
    my @sequences = map { $_->{sequence} } @{$family->{members}};
    
    # Perform multiple sequence alignment with threading
    my $msa_file = "$config{output_dir}/tmp/family_$family->{id}.aln";
    perform_msa_threaded(\@sequences, $msa_file, $config{threads});
    
    # Analyze alignment edges
    my $edge_analysis = analyze_alignment_edges($msa_file);
    
    # Trim based on conservation
    if ($edge_analysis->{needs_trimming}) {
        my $trimmed_consensus = trim_consensus(
            $family->{consensus},
            $edge_analysis->{trim_start},
            $edge_analysis->{trim_end}
        );
        
        $family->{consensus} = $trimmed_consensus;
        $family->{boundary_refined} = 1;
    }
    
    unlink $msa_file unless $config{keep_temp};
}

sub detect_subfamilies {
    my ($family) = @_;
    
    # Calculate pairwise distances
    my $distance_matrix = calculate_sequence_distances($family->{members});
    
    # Perform hierarchical clustering
    my $clusters = hierarchical_clustering($distance_matrix, 0.15);  # 15% divergence threshold
    
    if (scalar(@$clusters) > 1) {
        $family->{has_subfamilies} = 1;
        $family->{num_subfamilies} = scalar(@$clusters);
        
        my @subfamilies;
        foreach my $cluster (@$clusters) {
            if (scalar(@$cluster) >= 3) {
                my $subfamily = {
                    members => $cluster,
                    consensus => build_consensus_sequence($cluster),
                    divergence => calculate_average_divergence($cluster),
                };
                push @subfamilies, $subfamily;
            }
        }
        
        $family->{subfamilies} = \@subfamilies;
    }
}

# ===================================================================
# PHASE 4: REFINER.PY ENHANCED QUALITY VALIDATION
# ===================================================================

sub comprehensive_quality_control {
    my ($families) = @_;
    
    $logger->info("Refiner.py enhanced quality validation for " . scalar(keys %$families) . " families");
    
    # Since families are already enhanced by Refiner.py in Phase 3,
    # we only need basic filtering and validation here
    my $validated = {};
    my $stats = {
        total => scalar(keys %$families),
        passed => 0,
        filtered => 0,
        low_quality => 0,
    };
    
    foreach my $family_id (keys %$families) {
        my $family = $families->{$family_id};
        
        # Basic size filtering
        if (scalar(@{$family->{members} || []}) < $config{min_family_size}) {
            $stats->{filtered}++;
            next;
        }
        
        # Basic length filtering
        my $length = length($family->{consensus} || '');
        if ($length < $config{min_length} || $length > $config{max_length}) {
            $stats->{filtered}++;
            next;
        }
        
        # Check Refiner.py quality indicators
        # Refiner.py already handles quality assessment and consensus building
        my $refiner_quality = $family->{refiner_quality} || 0.8;  # Default high quality for Refiner.py output
        
        if ($refiner_quality < 0.3) {
            $stats->{low_quality}++;
            next;
        }
        
        # Family passed validation
        $family->{quality_score} = $refiner_quality;
        $validated->{$family_id} = $family;
        $stats->{passed}++;
    }
    
    # Log statistics
    $logger->info("Enhanced quality validation statistics:");
    $logger->info("  Total families: $stats->{total}");
    $logger->info("  Passed: $stats->{passed}");
    $logger->info("  Filtered (size/length): $stats->{filtered}");
    $logger->info("  Low quality: $stats->{low_quality}");
    
    return $validated;
}

# ===================================================================
# REFINER.PY INTEGRATION FUNCTIONS
# ===================================================================

sub process_family_with_refiner {
    my ($family, $family_id, $temp_dir) = @_;
    
    # Export family members to FASTA file
    my $input_file = export_family_to_fasta($family, $family_id, $temp_dir);
    return undef unless $input_file;
    
    # Run Refiner.py on the family
    my $output_file = "$temp_dir/${family_id}_refined.fa";
    my $refiner_path = find_refiner_py();
    my $refiner_cmd = "python3 '$refiner_path' '$input_file' '$output_file' --threads 1 --min-score 150";
    
    $logger->debug("Running Refiner.py: $refiner_cmd");
    
    my $exit_code = system($refiner_cmd);
    if ($exit_code != 0) {
        $logger->warn("Refiner.py failed for family $family_id (exit code: $exit_code)");
        return undef;
    }
    
    # Import enhanced consensus result
    my $enhanced_family = import_refiner_consensus($output_file, $family, $family_id);
    
    # Clean up temp files
    unlink($input_file, $output_file) unless $config{keep_temp};
    
    return $enhanced_family;
}

sub export_family_to_fasta {
    my ($family, $family_id, $temp_dir) = @_;
    
    my $output_file = "$temp_dir/${family_id}_members.fa";
    
    open(my $fh, '>', $output_file) or do {
        $logger->error("Cannot create family FASTA file: $output_file");
        return undef;
    };
    
    # Export all family members
    my $member_count = 0;
    if ($family->{members}) {
        foreach my $member (@{$family->{members}}) {
            $member_count++;
            my $seq_id = $member->{id} || "${family_id}_member_${member_count}";
            my $sequence = $member->{sequence} || '';
            
            print $fh ">$seq_id\n";
            print $fh "$sequence\n";
        }
    }
    
    close($fh);
    
    if ($member_count == 0) {
        $logger->warn("No members found for family $family_id");
        unlink($output_file);
        return undef;
    }
    
    $logger->debug("Exported $member_count members for family $family_id to $output_file");
    return $output_file;
}

sub import_refiner_consensus {
    my ($consensus_file, $original_family, $family_id) = @_;
    
    return undef unless -f $consensus_file;
    
    # Read the enhanced consensus from Refiner.py output
    open(my $fh, '<', $consensus_file) or do {
        $logger->error("Cannot read Refiner.py output: $consensus_file");
        return undef;
    };
    
    my $consensus_seq = '';
    my $header = '';
    my $reading_seq = 0;
    
    while (my $line = <$fh>) {
        chomp $line;
        if ($line =~ /^>(.+)/) {
            $header = $1;
            $reading_seq = 1;
        }
        elsif ($reading_seq && $line) {
            $consensus_seq .= $line;
        }
    }
    close($fh);
    
    if (!$consensus_seq) {
        $logger->warn("No consensus sequence found in Refiner.py output for family $family_id");
        return $original_family;  # Return original family
    }
    
    # Create enhanced family structure
    my $enhanced_family = {
        %$original_family,  # Copy original family data
        consensus => $consensus_seq,
        consensus_length => length($consensus_seq),
        refiner_enhanced => 1,
        refiner_quality => extract_quality_from_header($header),
        enhancement_method => 'refiner_py',
    };
    
    $logger->debug("Enhanced family $family_id: consensus length = " . length($consensus_seq));
    return $enhanced_family;
}

sub extract_quality_from_header {
    my ($header) = @_;
    
    # Extract quality information from Refiner.py header
    # Default to high quality since Refiner.py produces reliable results
    return 0.85;
}

sub find_refiner_py {
    # Try to locate Refiner.py in standard locations
    my @search_paths = (
        'bin/Refiner.py',
        '../bin/Refiner.py',
        'Refiner.py',
        '/home/shuoc/tool/Pan_TE/bin/Refiner.py'
    );
    
    foreach my $path (@search_paths) {
        if (-f $path && -x $path) {
            return $path;
        }
    }
    
    # Default fallback
    return 'bin/Refiner.py';
}

sub create_temp_dir {
    my ($prefix) = @_;
    
    my $temp_dir = "$config{tmp_dir}/${prefix}_$$";
    mkdir($temp_dir) or die "Cannot create temp directory: $temp_dir";
    
    return $temp_dir;
}

sub cleanup_temp_dir {
    my ($temp_dir) = @_;
    
    if (-d $temp_dir) {
        system("rm -rf '$temp_dir'");
    }
}

sub is_chimeric_family {
    my ($family) = @_;
    
    return 0 unless $family->{consensus};
    
    my $seq_length = length($family->{consensus});
    return 0 if $seq_length < 1000;  # Too short to be chimeric
    
    # Check for multiple distinct regions
    my $window_size = 500;
    my $step = 250;
    my @gc_values;
    
    for (my $i = 0; $i <= $seq_length - $window_size; $i += $step) {
        my $window = substr($family->{consensus}, $i, $window_size);
        push @gc_values, calculate_gc_content_string($window);
    }
    
    # Check for significant GC variation
    my $gc_std = calculate_std_dev(\@gc_values);
    
    return 1 if $gc_std > 0.15;  # High GC variation suggests chimera
    
    return 0;
}

sub assess_family_quality {
    my ($family) = @_;
    
    my $score = 1.0;
    
    # Penalize low copy number
    my $copy_count = scalar(@{$family->{members}});
    if ($copy_count < 5) {
        $score *= 0.8;
    }
    elsif ($copy_count < 3) {
        $score *= 0.6;
    }
    
    # Reward structural features
    if ($family->{structure_type}) {
        $score *= 1.2;
    }
    
    # Reward refined boundaries
    if ($family->{boundary_refined}) {
        $score *= 1.1;
    }
    
    # Penalize high divergence
    if ($family->{divergence} && $family->{divergence} > 0.3) {
        $score *= 0.9;
    }
    
    # Normalize score
    $score = min(1.0, max(0.0, $score));
    
    return $score;
}

# ===================================================================
# PHASE 5: INTEGRATION AND FINALIZATION  
# ===================================================================

sub integrate_and_finalize {
    my ($validated_families) = @_;
    
    $logger->info("Integrating and finalizing " . scalar(keys %$validated_families) . " families");
    
    # Remove redundancy
    my $nr_families = remove_redundancy($validated_families);
    
    # Classify families
    classify_te_families($nr_families);
    
    # Sort by priority
    my $prioritized = prioritize_families($nr_families);
    
    return $prioritized;
}

sub remove_redundancy {
    my ($families) = @_;
    
    $logger->info("Removing redundant families...");
    
    my $nr_families = {};
    my @sorted_families = sort { 
        scalar(@{$families->{$b}{members}}) <=> scalar(@{$families->{$a}{members}})
    } keys %$families;
    
    foreach my $family_id (@sorted_families) {
        my $family = $families->{$family_id};
        my $is_redundant = 0;
        
        # Check against already selected families
        foreach my $nr_id (keys %$nr_families) {
            my $nr_family = $nr_families->{$nr_id};
            
            if (families_are_redundant($family, $nr_family)) {
                $is_redundant = 1;
                
                # Merge information if beneficial
                if (scalar(@{$family->{members}}) > scalar(@{$nr_family->{members}}) * 0.5) {
                    merge_family_info($nr_family, $family);
                }
                
                last;
            }
        }
        
        unless ($is_redundant) {
            $nr_families->{$family_id} = $family;
        }
    }
    
    $logger->info("Reduced from " . scalar(@sorted_families) . " to " . 
                  scalar(keys %$nr_families) . " non-redundant families");
    
    return $nr_families;
}

sub classify_te_families {
    my ($families) = @_;
    
    foreach my $family_id (keys %$families) {
        my $family = $families->{$family_id};
        
        # Classification based on structural features
        if ($family->{structure_type}) {
            if ($family->{structure_type} eq 'LTR') {
                $family->{class} = 'LTR';
                $family->{superfamily} = classify_ltr_family($family);
            }
            elsif ($family->{structure_type} eq 'TIR') {
                $family->{class} = 'DNA';
                $family->{superfamily} = classify_dna_family($family);
            }
        }
        else {
            # Try to classify based on sequence features
            $family->{class} = 'Unknown';
            
            if ($family->{has_coding} && $family->{longest_orf} > 300) {
                $family->{class} = 'LINE';  # Potentially LINE element
            }
            elsif (length($family->{consensus} || '') < 500) {
                $family->{class} = 'SINE';  # Potentially SINE element
            }
        }
    }
}

sub prioritize_families {
    my ($families) = @_;
    
    # Calculate priority score for each family
    foreach my $family_id (keys %$families) {
        my $family = $families->{$family_id};
        
        my $priority_score = 0;
        
        # Factor 1: Copy number (40% weight)
        my $copy_score = min(100, scalar(@{$family->{members}})) / 100;
        $priority_score += $copy_score * 0.4;
        
        # Factor 2: Consensus quality (30% weight)
        my $quality_score = $family->{quality_score} || 0.5;
        $priority_score += $quality_score * 0.3;
        
        # Factor 3: Structural features (20% weight)
        my $structure_score = $family->{structure_type} ? 1.0 : 0.5;
        $priority_score += $structure_score * 0.2;
        
        # Factor 4: Classification confidence (10% weight)
        my $class_score = ($family->{class} && $family->{class} ne 'Unknown') ? 1.0 : 0.5;
        $priority_score += $class_score * 0.1;
        
        $family->{priority_score} = $priority_score;
    }
    
    return $families;
}

# ===================================================================
# OUTPUT AND REPORTING
# ===================================================================

sub write_output {
    my ($families) = @_;
    
    my $output_file = "$config{output_dir}/consensi.fa";
    open(my $out_fh, '>', $output_file) or die "Cannot open $output_file: $!";
    
    # Sort by priority score
    my @sorted_ids = sort { 
        $families->{$b}{priority_score} <=> $families->{$a}{priority_score}
    } keys %$families;
    
    foreach my $family_id (@sorted_ids) {
        my $family = $families->{$family_id};
        
        # Build header with metadata
        my $header = build_family_header($family_id, $family);
        
        print $out_fh ">$header\n";
        print $out_fh format_sequence($family->{consensus}), "\n";
        
        # Also output subfamilies if present
        if ($family->{subfamilies}) {
            foreach my $i (0..$#{$family->{subfamilies}}) {
                my $subfamily = $family->{subfamilies}[$i];
                print $out_fh ">${family_id}_sub${i} copies=" . scalar(@{$subfamily->{members}}) . "\n";
                print $out_fh format_sequence($subfamily->{consensus}), "\n";
            }
        }
    }
    
    close $out_fh;
    
    $logger->info("Wrote " . scalar(@sorted_ids) . " families to $output_file");
}

sub generate_report {
    my ($families) = @_;
    
    my $report_file = "$config{output_dir}/recon_report.txt";
    open(my $rpt_fh, '>', $report_file) or die "Cannot open $report_file: $!";
    
    print $rpt_fh "=" x 80, "\n";
    print $rpt_fh "Advanced RECON Pipeline Report\n";
    print $rpt_fh "=" x 80, "\n\n";
    
    # Summary statistics
    print $rpt_fh "SUMMARY STATISTICS\n";
    print $rpt_fh "-" x 40, "\n";
    print $rpt_fh "Total families identified: ", scalar(keys %$families), "\n";
    
    # Classification breakdown
    my %class_counts;
    foreach my $family (values %$families) {
        $class_counts{$family->{class} || 'Unknown'}++;
    }
    
    print $rpt_fh "\nClassification breakdown:\n";
    foreach my $class (sort keys %class_counts) {
        print $rpt_fh "  $class: $class_counts{$class}\n";
    }
    
    # Detailed family information
    print $rpt_fh "\n", "=" x 80, "\n";
    print $rpt_fh "DETAILED FAMILY INFORMATION\n";
    print $rpt_fh "=" x 80, "\n\n";
    
    my @sorted_ids = sort { 
        $families->{$b}{priority_score} <=> $families->{$a}{priority_score}
    } keys %$families;
    
    foreach my $family_id (@sorted_ids) {
        my $family = $families->{$family_id};
        
        print $rpt_fh "Family: $family_id\n";
        print $rpt_fh "  Class: ", $family->{class} || 'Unknown', "\n";
        print $rpt_fh "  Length: ", length($family->{consensus}), " bp\n";
        print $rpt_fh "  Copies: ", scalar(@{$family->{members}}), "\n";
        print $rpt_fh "  Priority score: ", sprintf("%.3f", $family->{priority_score}), "\n";
        
        if ($family->{structure_type}) {
            print $rpt_fh "  Structure: $family->{structure_type}\n";
        }
        
        if ($family->{has_subfamilies}) {
            print $rpt_fh "  Subfamilies: $family->{num_subfamilies}\n";
        }
        
        print $rpt_fh "\n";
    }
    
    close $rpt_fh;
    
    $logger->info("Generated report: $report_file");
}

# ===================================================================
# UTILITY FUNCTIONS
# ===================================================================

sub parse_arguments {
    GetOptions(
        'genome=s'      => \$config{genome_file},
        'input=s'       => \$config{input_lib},
        'output=s'      => \$config{output_dir},
        'threads=i'     => \$config{threads},
        'batch=i'       => \$config{batch_size},
        'min-family=i'  => \$config{min_family_size},
        'min-length=i'  => \$config{min_length},
        'max-length=i'  => \$config{max_length},
        'force'         => \$config{force},
        'debug'         => \$config{debug},
        'keep-temp'     => \$config{keep_temp},
        'help'          => sub { pod2usage(1) },
        'version'       => sub { print "Version $VERSION\n"; exit; },
    ) or pod2usage(2);
    
    # Validate required arguments
    unless ($config{genome_file} && $config{output_dir}) {
        pod2usage("Error: --genome and --output are required");
    }
    
    # Validate files
    unless (-f $config{genome_file}) {
        die "Genome file not found: $config{genome_file}\n";
    }
    
    # Make absolute paths
    $config{genome_file} = abs_path($config{genome_file});
    $config{output_dir} = abs_path($config{output_dir});
}

sub setup_environment {
    # Create output directory
    make_path($config{output_dir}) unless -d $config{output_dir};
    
    # Create temp directory
    make_path("$config{output_dir}/tmp");
    
    # Check for required tools
    # RECON workflow tools
    my @recon_tools = qw(imagespread eledef eleredef edgeredef famdef);
    # Other required tools
    my @other_tools = qw(blastn makeblastdb seqkit mafft);
    # Optional TRF masking tools (at least one should be available)
    my @trf_tools = qw(RepeatMasker trf dustmasker);
    
    my @all_tools = (@recon_tools, @other_tools);
    my @missing_tools;
    
    foreach my $tool (@all_tools) {
        unless (system("which $tool > /dev/null 2>&1") == 0) {
            push @missing_tools, $tool;
        }
    }
    
    # Check TRF masking tools (at least one should be available)
    my @available_trf_tools;
    foreach my $trf_tool (@trf_tools) {
        if (system("which $trf_tool > /dev/null 2>&1") == 0) {
            push @available_trf_tools, $trf_tool;
        }
    }
    
    if (@missing_tools) {
        $logger->error("Missing required tools: " . join(", ", @missing_tools));
        die "Required tools not found. Please ensure RECON is installed and in PATH.\n";
    }
    
    if (@available_trf_tools) {
        $logger->info("TRF masking tools available: " . join(", ", @available_trf_tools));
    } else {
        $logger->warn("No TRF masking tools found. Tandem repeat masking will be skipped.");
        $logger->warn("For better results, install: RepeatMasker, TRF, or ensure dustmasker is available");
    }
    
    $logger->info("All required tools found");
}

sub classify_genome_size {
    my ($size) = @_;
    
    return 'TINY' if $size < $GENOME_THRESHOLDS{TINY};
    return 'SMALL' if $size < $GENOME_THRESHOLDS{SMALL};
    return 'MEDIUM' if $size < $GENOME_THRESHOLDS{MEDIUM};
    return 'LARGE' if $size < $GENOME_THRESHOLDS{LARGE};
    return 'HUGE';
}

sub format_size {
    my ($bytes) = @_;
    
    my @units = ('B', 'KB', 'MB', 'GB', 'TB');
    my $unit = 0;
    my $size = $bytes;
    
    while ($size >= 1024 && $unit < $#units) {
        $size /= 1024;
        $unit++;
    }
    
    return sprintf("%.2f %s", $size, $units[$unit]);
}

sub format_sequence {
    my ($seq, $width) = @_;
    $width ||= 80;
    
    my $formatted = '';
    for (my $i = 0; $i < length($seq); $i += $width) {
        $formatted .= substr($seq, $i, $width) . "\n";
    }
    
    return $formatted;
}

sub build_family_header {
    my ($family_id, $family) = @_;
    
    my @header_parts = ($family_id);
    
    push @header_parts, "class=$family->{class}" if $family->{class};
    push @header_parts, "copies=" . scalar(@{$family->{members}});
    push @header_parts, "length=" . length($family->{consensus});
    push @header_parts, "score=" . sprintf("%.3f", $family->{priority_score});
    
    if ($family->{structure_type}) {
        push @header_parts, "structure=$family->{structure_type}";
    }
    
    return join(" ", @header_parts);
}

sub cleanup {
    $logger->info("Cleaning up temporary files...");
    remove_tree("$config{output_dir}/tmp");
}

# ===================================================================
# HELPER FUNCTIONS FOR PARALLEL PROCESSING
# ===================================================================

# Duplicate function definitions removed - all functions now defined at the top of the script

# Run main
main() unless caller;

__END__

=head1 NAME

run_RECON_advanced - Advanced RECON Pipeline for TE Discovery

=head1 SYNOPSIS

run_RECON_advanced --genome GENOME.fa --output OUTPUT_DIR [OPTIONS]

=head1 DESCRIPTION

Biologically-informed RECON implementation with adaptive optimization
for transposable element discovery.

=head1 OPTIONS

  --genome FILE       Input genome file (required)
  --output DIR        Output directory (required)
  --input FILE        Optional existing TE library to extend
  --threads INT       Number of threads (default: 4)
  --batch INT         Batch size for processing (default: 100000)
  --min-family INT    Minimum family size (default: 3)
  --min-length INT    Minimum element length (default: 50)
  --max-length INT    Maximum element length (default: 50000)
  --force             Force overwrite existing output
  --debug             Enable debug mode
  --keep-temp         Keep temporary files
  --help              Show this help message
  --version           Show version

=head1 AUTHOR

Pan_TE Development Team

=cut

